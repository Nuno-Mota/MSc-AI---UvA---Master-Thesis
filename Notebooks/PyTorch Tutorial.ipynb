{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from   torch.utils.data import Dataset, DataLoader\n",
    "from   torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence, PackedSequence\n",
    "from   torch.distributions.categorical import Categorical\n",
    "from   torch.distributions.dirichlet import Dirichlet\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") # So it's possible to retrieve packages at a higher top level. (than the directory where the notebook is running)\n",
    "\n",
    "\n",
    "\n",
    "# ******************** #\n",
    "# Import own functions #\n",
    "# ******************** #\n",
    "\n",
    "# from helpers import classes_instantiator as classes\n",
    "# from datasets.data_loaders.from_storage import load_MNIST_data_numpy\n",
    "# from datasets.data_loaders.into_trainer import UW_RE_UVA\n",
    "# from datasets.data_loaders.into_trainer import PadCollateMetaDataloader\n",
    "# from helpers.functions import masked_softmax\n",
    "# from models.re_bow import RE_BOW\n",
    "# from models.mlp import MLP\n",
    "# import helpers.trainer.helpers as trainer_helpers\n",
    "# from helpers.trainer.metrics_manager import MetricsManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2140, 0.1517, 0.1174, 0.2420],\n",
      "        [0.3145, 0.0296, 0.8579, 0.6520],\n",
      "        [0.5831, 0.7071, 0.8518, 0.3934]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3,4)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3600ae1fa1f4898b19fa10aae2c0014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=184, description='var', max=369), Output()), _dom_classes=('widget-interâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "with open(\"../correlation_matrices_cpu_esim_enc_long.crm\", 'rb') as f:\n",
    "    correlation_matrices = pickle.load(f)\n",
    "\n",
    "# # Normalise correlation matrices?\n",
    "# for epoch in correlation_matrices:\n",
    "#     correlation_matrices[epoch] = correlation_matrices[epoch]/correlation_matrices[epoch].sum(dim=1)\n",
    "\n",
    "sn.set(font_scale=1)#for label size\n",
    "\n",
    "def f(var):\n",
    "    fig = plt.figure(figsize = (10,7))\n",
    "    plt.xlabel('test')\n",
    "    fig.suptitle('Confusion Matrix (Epoch: ' + str(var) + ')', fontsize=20, fontweight='bold')\n",
    "    sn.heatmap(correlation_matrices[str(var)], annot=False, cmap='coolwarm', robust=False, square=True)\n",
    "    plt.xlabel('Predicted Classes', fontsize=25)\n",
    "    plt.ylabel('True Classes', fontsize=25)\n",
    "#     plt.show()\n",
    "\n",
    "interact(f, var=(0, len(correlation_matrices) - 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate Memory Usage in GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder persistent Memory: 0.162495 GB\n",
      "\n",
      "\n",
      "######################################\n",
      "# 1st Phase - Input Encoding BiLSTMs #\n",
      "######################################\n",
      "\n",
      "Input x data size: 0.001860 GB.\n",
      "\n",
      "Input y data size: 0.084972 GB.\n",
      "\n",
      "Total taken memory: Total: 0.509824 GB ---> (Accumulated: 0.162495 GB ||| Temporary Variables: 0.347328 GB).\n",
      "\n",
      "\n",
      "# *** Start Pass through first LSTM stage *** #\n",
      "\n",
      "Total taken memory: Total: 0.671277 GB ---> (Accumulated: 0.614746 GB ||| Temporary Variables: 0.056531 GB).\n",
      "\n",
      "\n",
      "# *** Start Attention stage *** #\n",
      "\n",
      "Total taken memory: Total: 0.815115 GB ---> (Accumulated: 0.614746 GB ||| Temporary Variables: 0.200369 GB).\n",
      "\n",
      "\n",
      "# *** Have both attention_weights and its mask at this point. Will now compute their element wise product and delete the mask *** #\n",
      "\n",
      "Total taken memory: Total: 0.743198 GB ---> (Accumulated: 0.614746 GB ||| Temporary Variables: 0.128452 GB).\n",
      "\n",
      "\n",
      "# *** Now compute the summation over the attention, so we can compute a tilde *** #\n",
      "\n",
      "Total taken memory: Total: 0.745861 GB ---> (Accumulated: 0.614746 GB ||| Temporary Variables: 0.131116 GB).\n",
      "\n",
      "\n",
      "# *** Compute Normalised a tilde Attention Weights *** #\n",
      "\n",
      "Total taken memory: Total: 0.817778 GB ---> (Accumulated: 0.614746 GB ||| Temporary Variables: 0.203032 GB).\n",
      "\n",
      "\n",
      "# *** Delete the summed a tilde attention values *** #\n",
      "\n",
      "Total taken memory: Total: 0.815115 GB ---> (Accumulated: 0.614746 GB ||| Temporary Variables: 0.200369 GB).\n",
      "\n",
      "\n",
      "# *** Compute the a tilde *** #\n",
      "\n",
      "Total taken memory: Total: 2.146906 GB ---> (Accumulated: 0.614746 GB ||| Temporary Variables: 1.532160 GB).\n",
      "\n",
      "\n",
      "# *** Delete the Normalised a tilde Attention Weights *** #\n",
      "\n",
      "Total taken memory: Total: 2.074989 GB ---> (Accumulated: 0.614746 GB ||| Temporary Variables: 1.460243 GB).\n",
      "\n",
      "\n",
      "# *** Compute the m_a vectors *** #\n",
      "\n",
      "Total taken memory: Total: 7.402154 GB ---> (Accumulated: 0.614746 GB ||| Temporary Variables: 6.787409 GB).\n",
      "\n",
      "\n",
      "# *** Delete a tilde *** #\n",
      "\n",
      "Total taken memory: Total: 6.070363 GB ---> (Accumulated: 0.614746 GB ||| Temporary Variables: 5.455617 GB).\n",
      "\n",
      "\n",
      "# *** Compress m_a *** #\n",
      "\n",
      "Total taken memory: Total: 6.070363 GB ---> (Accumulated: 3.278328 GB ||| Temporary Variables: 2.792035 GB).\n",
      "\n",
      "\n",
      "# *** Now, for b tilde, compute the summation over the attention *** #\n",
      "\n",
      "Total taken memory: Total: 6.071469 GB ---> (Accumulated: 3.278328 GB ||| Temporary Variables: 2.793141 GB).\n",
      "\n",
      "\n",
      "# *** Compute Normalised b tilde Attention Weights *** #\n",
      "\n",
      "Total taken memory: Total: 6.143386 GB ---> (Accumulated: 3.278328 GB ||| Temporary Variables: 2.865058 GB).\n",
      "\n",
      "\n",
      "# *** Delete the summed b tilde attention values *** #\n",
      "\n",
      "Total taken memory: Total: 6.142280 GB ---> (Accumulated: 3.278328 GB ||| Temporary Variables: 2.863951 GB).\n",
      "\n",
      "\n",
      "# *** Compute the b tilde *** #\n",
      "\n",
      "Total taken memory: Total: 6.695485 GB ---> (Accumulated: 3.278328 GB ||| Temporary Variables: 3.417157 GB).\n",
      "\n",
      "\n",
      "# *** Delete the Normalised b tilde Attention Weights *** #\n",
      "\n",
      "Total taken memory: Total: 6.623569 GB ---> (Accumulated: 3.278328 GB ||| Temporary Variables: 3.345240 GB).\n",
      "\n",
      "\n",
      "# *** Compute the m_b vectors *** #\n",
      "\n",
      "Total taken memory: Total: 8.836391 GB ---> (Accumulated: 3.278328 GB ||| Temporary Variables: 5.558063 GB).\n",
      "\n",
      "\n",
      "# *** Delete b tilde *** #\n",
      "\n",
      "Total taken memory: Total: 8.283185 GB ---> (Accumulated: 3.278328 GB ||| Temporary Variables: 5.004857 GB).\n",
      "\n",
      "\n",
      "# *** Compress m_b *** #\n",
      "\n",
      "Total taken memory: Total: 8.283185 GB ---> (Accumulated: 4.384739 GB ||| Temporary Variables: 3.898446 GB).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def humanbytes(B):\n",
    "    \"\"\"Return the given bytes as a human friendly KB, MB, GB, or TB string\"\"\"\n",
    "    B = float(B)\n",
    "    KB = float(1024)\n",
    "    MB = float(KB ** 2) # 1,048,576\n",
    "    GB = float(KB ** 3) # 1,073,741,824\n",
    "    TB = float(KB ** 4) # 1,099,511,627,776\n",
    "\n",
    "    return '{0:.6f} GB'.format(B/GB)\n",
    "#     if B < KB:\n",
    "#         return '{0} {1}'.format(B,'Bytes' if 0 == B > 1 else 'Byte')\n",
    "#     elif KB <= B < MB:\n",
    "#         return '{0:.2f} KB'.format(B/KB)\n",
    "#     elif MB <= B < GB:\n",
    "#         return '{0:.2f} MB'.format(B/MB)\n",
    "#     elif GB <= B < TB:\n",
    "#         return '{0:.2f} GB'.format(B/GB)\n",
    "#     elif TB <= B:\n",
    "#         return '{0:.2f} TB'.format(B/TB)\n",
    "\n",
    "def lstm_persist_mem(input_size, hidden_size, bidirectional, num_lstm_structs, dtype_size):\n",
    "    weights_mem = 4 * (hidden_size * (input_size + hidden_size))\n",
    "    biases_mem = 8 * (hidden_size)\n",
    "    return num_lstm_structs * (2 if bidirectional else 1) * (weights_mem + biases_mem) * dtype_size\n",
    "\n",
    "def lstm_upper_bound_kept_activs_mem(input_shape, hidden_size, bidirectional, dtype_size):\n",
    "    # 4 activation functions * Num_sentences * max_seq_length * hidden_size\n",
    "    activs_mem = 4 * input_shape[0] * input_shape[1] * hidden_size\n",
    "    return (2 if bidirectional else 1) * activs_mem * dtype_size\n",
    "\n",
    "\n",
    "def mlp_persist_mem(layers_sizes, dtype_size):\n",
    "    weights_mem = sum(pair[0]*pair[1] for pair in zip(layers_sizes, layers_sizes[1:]))\n",
    "    biases_mem = sum(layers_sizes[1:])\n",
    "    return (weights_mem + biases_mem) * dtype_size\n",
    "\n",
    "def mlp_upper_bound_kept_activs_mem(input_shape, layers_sizes, dtype_size):\n",
    "    num_instances = np.prod(np.array(input_shape[:-1], dtype='int64'))\n",
    "    activs_mem = np.sum(np.array([num_instances * layer_size for layer_size in layers_sizes[1:]]))\n",
    "    return activs_mem * dtype_size\n",
    "\n",
    "\n",
    "def memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size):\n",
    "    aggregated = np.sum(np.array(aggregated_memory_at_phase, dtype='int64'))\n",
    "    temp = np.sum(np.array([current_variables_memory[key] for key in current_variables_memory] ,dtype='int64'))*dtype_size\n",
    "    return \"Total: \" + humanbytes(aggregated + temp) + \" ---> (Accumulated: \" + humanbytes(aggregated) + \" ||| Temporary Variables: \" + humanbytes(temp)\n",
    "\n",
    "###################################################\n",
    "\n",
    "dtype_size = 4 #Bytes\n",
    "\n",
    "train = False\n",
    "\n",
    "if (train):\n",
    "    num_observations = 100\n",
    "    num_descriptions = 120\n",
    "else:\n",
    "    num_observations = 10\n",
    "    num_descriptions = 1100\n",
    "\n",
    "max_observation_len =  65\n",
    "max_desc_length = 27\n",
    "\n",
    "embedding_size = 3*1024\n",
    "\n",
    "\n",
    "biLSTM = True\n",
    "one_for_both_x_and_y = True\n",
    "num_lstm_structs = 2 if one_for_both_x_and_y else 1\n",
    "\n",
    "\n",
    "first_lstm_hidden_size = 500\n",
    "\n",
    "first_MLP_layer_sizes = [4 * first_lstm_hidden_size, 1000]\n",
    "\n",
    "second_lstm_hidden_size = 500\n",
    "\n",
    "last_MLP_layer_sizes = [4 * second_lstm_hidden_size, 500, 20, 1]\n",
    "\n",
    "\n",
    "##############################################\n",
    "\n",
    "aggregated_memory_at_phase = []\n",
    "current_variables_memory = {}\n",
    "\n",
    "##############################################\n",
    "# CONSTANT MEMORY - MODEL WEIGHTS AND BIASES #\n",
    "##############################################\n",
    "\n",
    "first_biLSTMS_persistent_memory = lstm_persist_mem(embedding_size, first_lstm_hidden_size, biLSTM, num_lstm_structs, dtype_size)\n",
    "# print(humanbytes(first_biLSTMS_persistent_memory))\n",
    "\n",
    "first_mlp_persistent_memory = mlp_persist_mem(first_MLP_layer_sizes, dtype_size)\n",
    "# print(humanbytes(first_mlp_persistent_memory))\n",
    "\n",
    "second_biLSTMS_persistent_memory = lstm_persist_mem(first_MLP_layer_sizes[1], second_lstm_hidden_size, biLSTM, num_lstm_structs, dtype_size)\n",
    "# print(humanbytes(second_biLSTMS_persistent_memory))\n",
    "\n",
    "second_MLP_b_persistent_mem = mlp_persist_mem(last_MLP_layer_sizes, dtype_size)\n",
    "# print(humanbytes(second_MLP_b_persistent_mem))\n",
    "\n",
    "encoder_persistent_mem = first_biLSTMS_persistent_memory + first_mlp_persistent_memory + second_biLSTMS_persistent_memory + second_MLP_b_persistent_mem\n",
    "aggregated_memory_at_phase.append(encoder_persistent_mem)\n",
    "\n",
    "print(\"Encoder persistent Memory: \" + humanbytes(encoder_persistent_mem) + \"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "######################################\n",
    "# 1st Phase - Input Encoding BiLSTMs #\n",
    "######################################\n",
    "print(\"######################################\\n# 1st Phase - Input Encoding BiLSTMs #\\n######################################\\n\")\n",
    "\n",
    "input_x_data_size = num_observations * max_observation_len * embedding_size\n",
    "current_variables_memory['input_x_data_size'] = input_x_data_size\n",
    "print(\"Input x data size: \" + humanbytes(input_x_data_size) + \".\\n\")\n",
    "\n",
    "input_y_data_size = num_descriptions * max_desc_length * embedding_size\n",
    "current_variables_memory['input_y_data_size'] = input_y_data_size\n",
    "print(\"Input y data size: \" + humanbytes(input_y_data_size) + \".\\n\")\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "print(\"\\n# *** Start Pass through first LSTM stage *** #\\n\")\n",
    "# x sentences\n",
    "input_shape = [num_observations, max_observation_len, embedding_size]\n",
    "aggregated_memory_at_phase.append(lstm_upper_bound_kept_activs_mem(input_shape, first_lstm_hidden_size, biLSTM, dtype_size))\n",
    "\n",
    "# y sentences\n",
    "input_shape = [num_descriptions, max_desc_length, embedding_size]\n",
    "aggregated_memory_at_phase.append(lstm_upper_bound_kept_activs_mem(input_shape, first_lstm_hidden_size, biLSTM, dtype_size))\n",
    "\n",
    "del current_variables_memory['input_x_data_size']\n",
    "del current_variables_memory['input_y_data_size']\n",
    "\n",
    "current_variables_memory['x_data_size'] = num_observations * max_observation_len * first_lstm_hidden_size\n",
    "current_variables_memory['y_data_size'] = num_descriptions * max_desc_length * first_lstm_hidden_size\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Start Attention stage *** #\\n\")\n",
    "\n",
    "current_variables_memory['x_lengths'] = num_observations\n",
    "current_variables_memory['y_lengths'] = num_descriptions\n",
    "\n",
    "# Attention Elements\n",
    "current_variables_memory['attention_elements'] = num_observations * num_descriptions * max_observation_len * max_desc_length\n",
    "\n",
    "# Masks\n",
    "current_variables_memory['mask_x'] = num_observations * max_observation_len\n",
    "current_variables_memory['mask_y'] = num_descriptions * max_desc_length\n",
    "current_variables_memory['attention_values_mask'] = num_observations * num_descriptions * max_observation_len * max_desc_length\n",
    "\n",
    "del current_variables_memory['mask_x']\n",
    "del current_variables_memory['mask_y']\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Have both attention_weights and its mask at this point. Will now compute their element wise product and delete the mask *** #\\n\")\n",
    "\n",
    "del current_variables_memory['attention_values_mask']\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Now compute the summation over the attention, so we can compute a tilde *** #\\n\")\n",
    "\n",
    "current_variables_memory['summed_a_tilde'] = num_observations * num_descriptions * max_observation_len\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Compute Normalised a tilde Attention Weights *** #\\n\")\n",
    "\n",
    "current_variables_memory['normalised_attention_weights_a_tilde'] = num_observations * num_descriptions * max_observation_len * max_desc_length\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Delete the summed a tilde attention values *** #\\n\")\n",
    "\n",
    "del current_variables_memory['summed_a_tilde']\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Compute the a tilde *** #\\n\")\n",
    "\n",
    "current_variables_memory['a_tilde'] = num_observations * num_descriptions * max_observation_len * first_lstm_hidden_size\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Delete the Normalised a tilde Attention Weights *** #\\n\")\n",
    "\n",
    "del current_variables_memory['normalised_attention_weights_a_tilde']\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Compute the m_a vectors *** #\\n\")\n",
    "\n",
    "current_variables_memory['m_a'] = num_observations * num_descriptions * max_observation_len * (4 * first_lstm_hidden_size)\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Delete a tilde *** #\\n\")\n",
    "\n",
    "del current_variables_memory['a_tilde']\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Compress m_a *** #\\n\")\n",
    "\n",
    "m_a_shape = [num_observations, num_descriptions, max_observation_len, 4 * first_lstm_hidden_size]\n",
    "aggregated_memory_at_phase.append(mlp_upper_bound_kept_activs_mem(m_a_shape, first_MLP_layer_sizes, dtype_size))\n",
    "\n",
    "current_variables_memory['m_a'] = num_observations * num_descriptions * max_observation_len * first_MLP_layer_sizes[-1]\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n# *** Now, for b tilde, compute the summation over the attention *** #\\n\")\n",
    "\n",
    "current_variables_memory['summed_b_tilde'] = num_observations * num_descriptions * max_desc_length\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Compute Normalised b tilde Attention Weights *** #\\n\")\n",
    "\n",
    "current_variables_memory['normalised_attention_weights_b_tilde'] = num_observations * num_descriptions * max_observation_len * max_desc_length\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Delete the summed b tilde attention values *** #\\n\")\n",
    "\n",
    "del current_variables_memory['summed_b_tilde']\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Compute the b tilde *** #\\n\")\n",
    "\n",
    "current_variables_memory['b_tilde'] = num_observations * num_descriptions * max_desc_length * first_lstm_hidden_size\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Delete the Normalised b tilde Attention Weights *** #\\n\")\n",
    "\n",
    "del current_variables_memory['normalised_attention_weights_b_tilde']\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Compute the m_b vectors *** #\\n\")\n",
    "\n",
    "current_variables_memory['m_b'] = num_observations * num_descriptions * max_desc_length * (4 * first_lstm_hidden_size)\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Delete b tilde *** #\\n\")\n",
    "\n",
    "del current_variables_memory['b_tilde']\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n# *** Compress m_b *** #\\n\")\n",
    "\n",
    "m_b_shape = [num_observations, num_descriptions, max_desc_length, 4 * first_lstm_hidden_size]\n",
    "aggregated_memory_at_phase.append(mlp_upper_bound_kept_activs_mem(m_b_shape, first_MLP_layer_sizes, dtype_size))\n",
    "\n",
    "current_variables_memory['m_b'] = num_observations * num_descriptions * max_desc_length * first_MLP_layer_sizes[-1]\n",
    "\n",
    "print(\"Total taken memory: \" + memory_at_stage(aggregated_memory_at_phase, current_variables_memory, dtype_size) + \").\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
