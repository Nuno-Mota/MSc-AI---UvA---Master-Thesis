{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/12/2019 14:28:50 - INFO - allennlp.commands.elmo -   Initializing ELMo.\n"
     ]
    }
   ],
   "source": [
    "elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Academic & Professional\\Academic\\Master Thesis AI\\Master-Thesis\\Code\\Notebooks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_MLI_data = \"../datasets/Data/MultiNLI/\"\n",
    "\n",
    "train          = path_to_MLI_data + \"multinli_1.0_train.txt\"\n",
    "dev_matched    = path_to_MLI_data + \"multinli_1.0_dev_matched.txt\"\n",
    "dev_mismatched = path_to_MLI_data + \"multinli_1.0_dev_mismatched.txt\"\n",
    "\n",
    "path_to_test = \"test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(1).astype(np.float32)\n",
    "dtype = a.dtype\n",
    "print(dtype)\n",
    "np.dtype(eval('np.' + str(dtype))).itemsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable_array_storage(object):\n",
    "\n",
    "    def __init__(self, storage_file_path, num_data_elements_per_line, write_mode, dtype):\n",
    "\n",
    "        self._storage_file_path          = open(storage_file_path + '.mem', write_mode)\n",
    "        self._cursor_positions_file_path = storage_file_path + '.idx'\n",
    "\n",
    "        self._num_data_elements_per_line = num_data_elements_per_line # Load into memory\n",
    "\n",
    "        self._write_mode = write_mode\n",
    "\n",
    "        \n",
    "        if (write_mode == 'rb'):\n",
    "            with open(self._cursor_positions_file_path, 'rb') as f:\n",
    "                self._instances_positions = pickle.load(f)\n",
    "            self._data_num_bytes = np.dtype(eval('np.' + str(self._instances_positions[0]))).itemsize\n",
    "        else:\n",
    "            self._instances_positions = [dtype]\n",
    "            self._data_num_bytes = np.dtype(eval('np.' + str(dtype))).itemsize\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def save_line_sequential(self, sentences_encodings):\n",
    "        if (self._num_data_elements_per_line > 1):\n",
    "            for instance_num in range(self._num_data_elements_per_line):\n",
    "                self._save_instance_sequential(sentences_encodings[instance_num])\n",
    "        else:\n",
    "            self._save_instance_sequential(sentences_encodings)\n",
    "\n",
    "\n",
    "\n",
    "    def _save_instance_sequential(self, sentence_encoding):\n",
    "        self._instances_positions.append(self._storage_file_path.tell())\n",
    "        for seq_ele in range(sentence_encoding.shape[1]):\n",
    "            for encoding_type in range(self._num_encodings_types):# TODO: This variable can be moved to parent class\n",
    "                for value_idx in range(self._size_encodings):\n",
    "                    self._storage_file_path.write(sentence_encoding[encoding_type][seq_ele][value_idx])\n",
    "\n",
    "\n",
    "\n",
    "    def save_instances_positions(self):\n",
    "        with open(self._cursor_positions_file_path, 'wb+') as f:\n",
    "            pickle.dump(self._instances_positions, f)\n",
    "\n",
    "\n",
    "\n",
    "    def close_file(self):\n",
    "        self._storage_file_path.close()\n",
    "\n",
    "\n",
    "\n",
    "    def load_line_at_index(self, idx):\n",
    "        raise NotImplemented\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "class Variable_ELMO_array_storage(Variable_array_storage):\n",
    "\n",
    "    def __init__(self, storage_file_path, num_data_elements_per_line, write_mode, dtype_size_bytes=None, num_encodings_types=3, size_encodings=1024):\n",
    "        super(Variable_ELMO_array_storage, self).__init__(storage_file_path, num_data_elements_per_line, write_mode, dtype_size_bytes)\n",
    "\n",
    "        # ELMO embeddings characteristics\n",
    "        self._num_encodings_types = num_encodings_types\n",
    "        self._size_encodings      = size_encodings\n",
    "\n",
    "\n",
    "\n",
    "    def load_line_at_index(self, line_idx):\n",
    "        dtype = eval('np.' + str(self._instances_positions[0]))\n",
    "        line_idx = 1 + line_idx*self._num_data_elements_per_line\n",
    "        self._storage_file_path.seek(self._instances_positions[line_idx])\n",
    "        line = []\n",
    "        for instance_num in range(self._num_data_elements_per_line):\n",
    "            if (line_idx + instance_num < len(self._instances_positions)):\n",
    "                if (line_idx + instance_num + 1 < len(self._instances_positions)):\n",
    "                    seq_length = self._instances_positions[line_idx + instance_num + 1] - self._instances_positions[line_idx + instance_num]\n",
    "                else:\n",
    "                    self._storage_file_path.seek(-1,2)\n",
    "                    last_position = self._storage_file_path.tell() + 1 # +1 to account for the actual position the next instance would be at\n",
    "                    self._storage_file_path.seek(self._instances_positions[line_idx + instance_num]) # Restore cursor position to the beginning of the instance\n",
    "                    seq_length = last_position - self._instances_positions[line_idx + instance_num]\n",
    "                seq_length = int(seq_length/(self._num_encodings_types * self._size_encodings * self._data_num_bytes))\n",
    "            else:\n",
    "                raise IndexError(\"There is no such line number (\" + str(line_idx) + \") in file \" + self._storage_file_path.name + \".\")\n",
    "\n",
    "            data_instance = np.empty([self._num_encodings_types, seq_length, self._size_encodings], dtype=dtype)\n",
    "\n",
    "            for seq_ele in range(seq_length):\n",
    "                for encoding_type in range(self._num_encodings_types):\n",
    "                    for value_idx in range(self._size_encodings):\n",
    "                        data_instance[encoding_type][seq_ele][value_idx] = np.frombuffer(self._storage_file_path.read(self._data_num_bytes), dtype=dtype)\n",
    "\n",
    "            line.append(data_instance)\n",
    "\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "[[array([[[0.00108573, 0.29183447],\n",
      "        [0.77035459, 0.90411301]],\n",
      "\n",
      "       [[0.26867345, 0.12984343],\n",
      "        [0.04426825, 0.89142792]],\n",
      "\n",
      "       [[0.91958899, 0.29741063],\n",
      "        [0.72152843, 0.54500064]]]), array([[[0.60545101, 0.77043845],\n",
      "        [0.11846807, 0.32999093],\n",
      "        [0.98382971, 0.36558403],\n",
      "        [0.10523205, 0.46761935]],\n",
      "\n",
      "       [[0.03855612, 0.31502763],\n",
      "        [0.24207041, 0.73431365],\n",
      "        [0.10535907, 0.39135685],\n",
      "        [0.2826739 , 0.91535291]],\n",
      "\n",
      "       [[0.00169617, 0.97615194],\n",
      "        [0.96185165, 0.2999511 ],\n",
      "        [0.9812867 , 0.10195023],\n",
      "        [0.67164552, 0.01241636]]])], [array([[[0.41212383, 0.96545007],\n",
      "        [0.0546776 , 0.7107712 ],\n",
      "        [0.5505881 , 0.556125  ]],\n",
      "\n",
      "       [[0.19634092, 0.6540999 ],\n",
      "        [0.17051387, 0.61365517],\n",
      "        [0.24236891, 0.17776813]],\n",
      "\n",
      "       [[0.15770505, 0.56735433],\n",
      "        [0.19640796, 0.84603931],\n",
      "        [0.66754472, 0.68189391]]]), array([[[0.52480888, 0.25467228],\n",
      "        [0.18592735, 0.42112653],\n",
      "        [0.66452649, 0.74938758],\n",
      "        [0.66649422, 0.87228193],\n",
      "        [0.07284315, 0.57713035]],\n",
      "\n",
      "       [[0.2587362 , 0.07700945],\n",
      "        [0.01027407, 0.3965815 ],\n",
      "        [0.22326667, 0.89547983],\n",
      "        [0.34793616, 0.39370783],\n",
      "        [0.62556668, 0.82839823]],\n",
      "\n",
      "       [[0.03790781, 0.9923825 ],\n",
      "        [0.67404895, 0.81326395],\n",
      "        [0.85375239, 0.10947917],\n",
      "        [0.85764065, 0.98802673],\n",
      "        [0.15409629, 0.86763861]]])]]\n"
     ]
    }
   ],
   "source": [
    "test_file_name = \"test\"\n",
    "a = [[np.random.rand(3, 2, 2), np.random.rand(3, 4, 2)], [np.random.rand(3, 3, 2), np.random.rand(3, 5, 2)]]\n",
    "print(a[0][0].dtype)\n",
    "print(a[0][1].dtype)\n",
    "print(a[1][0].dtype)\n",
    "print(a[1][1].dtype)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "memmap = Variable_ELMO_array_storage(test_file_name, 2, \"wb+\", dtype_size_bytes=a[0][0].dtype, num_encodings_types=3, size_encodings=2)\n",
    "\n",
    "for line in a:\n",
    "    memmap.save_line_sequential(line)\n",
    "memmap.save_instances_positions()\n",
    "memmap.close_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "memmap = Variable_ELMO_array_storage(test_file_name, 2, \"rb\", dtype_size_bytes=a[0][0].dtype, num_encodings_types=3, size_encodings=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "[array([[[0.41212383, 0.96545007],\n",
      "        [0.0546776 , 0.7107712 ],\n",
      "        [0.5505881 , 0.556125  ]],\n",
      "\n",
      "       [[0.19634092, 0.6540999 ],\n",
      "        [0.17051387, 0.61365517],\n",
      "        [0.24236891, 0.17776813]],\n",
      "\n",
      "       [[0.15770505, 0.56735433],\n",
      "        [0.19640796, 0.84603931],\n",
      "        [0.66754472, 0.68189391]]]), array([[[0.52480888, 0.25467228],\n",
      "        [0.18592735, 0.42112653],\n",
      "        [0.66452649, 0.74938758],\n",
      "        [0.66649422, 0.87228193],\n",
      "        [0.07284315, 0.57713035]],\n",
      "\n",
      "       [[0.2587362 , 0.07700945],\n",
      "        [0.01027407, 0.3965815 ],\n",
      "        [0.22326667, 0.89547983],\n",
      "        [0.34793616, 0.39370783],\n",
      "        [0.62556668, 0.82839823]],\n",
      "\n",
      "       [[0.03790781, 0.9923825 ],\n",
      "        [0.67404895, 0.81326395],\n",
      "        [0.85375239, 0.10947917],\n",
      "        [0.85764065, 0.98802673],\n",
      "        [0.15409629, 0.86763861]]])]\n"
     ]
    }
   ],
   "source": [
    "print(memmap.load_line_at_index(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "memmap.close_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname, \"r\", encoding='utf-8') as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_string(s):\n",
    "    tokenized_string = []\n",
    "    split_element = s.split(' ')\n",
    "    for k, word in enumerate(split_element):\n",
    "        tokenized_string += nltk.tokenize.word_tokenize(word)\n",
    "    return tokenized_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392703\n"
     ]
    }
   ],
   "source": [
    "split = train\n",
    "\n",
    "mnli_split_len = file_len(split)\n",
    "print(mnli_split_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral', '( ( Conceptually ( cream skimming ) ) ( ( has ( ( ( two ( basic dimensions ) ) - ) ( ( product and ) geography ) ) ) . ) )', '( ( ( Product and ) geography ) ( ( are ( what ( make ( cream ( skimming work ) ) ) ) ) . ) )', '(ROOT (S (NP (JJ Conceptually) (NN cream) (NN skimming)) (VP (VBZ has) (NP (NP (CD two) (JJ basic) (NNS dimensions)) (: -) (NP (NN product) (CC and) (NN geography)))) (. .)))', '(ROOT (S (NP (NN Product) (CC and) (NN geography)) (VP (VBP are) (SBAR (WHNP (WP what)) (S (VP (VBP make) (NP (NP (NN cream)) (VP (VBG skimming) (NP (NN work)))))))) (. .)))', 'Conceptually cream skimming has two basic dimensions - product and geography.', 'Product and geography are what make cream skimming work. ', '31193', '31193n', 'government', 'neutral', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "with open(split, \"r\", encoding='utf-8') as f:\n",
    "    for l_num, line in enumerate(f):\n",
    "        if (l_num > 0):\n",
    "            if (l_num == 1):\n",
    "                print(line.rstrip('\\r\\n').split('\\t'))\n",
    "                break\n",
    "#             if (l_num % min(int(mnli_split_len/3), 250) == 0 or l_num + 1 == mnli_split_len):\n",
    "#                     print('\\r' + str(l_num + 1) + '/' + str(mnli_split_len), end=\"\", flush=True)\n",
    "\n",
    "#             line_temp = line.rstrip('\\r\\n').split('\\t')\n",
    "#             if (line_temp[0] != 'contradiction' and line_temp[0] != 'neutral'  and line_temp[0] != 'entailment'):\n",
    "#                 counter += 1\n",
    "                \n",
    "# print(\"\\n\\n\" + str(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355501/392703"
     ]
    }
   ],
   "source": [
    "p_data = [] # Propositions\n",
    "h_data = [] # Hypothesis\n",
    "labels = [] # Labels\n",
    "\n",
    "with open(split, \"r\", encoding='utf-8') as f:\n",
    "    for l_num, line in enumerate(f):\n",
    "        \n",
    "        line_temp = line.rstrip('\\r\\n').split('\\t')\n",
    "        \n",
    "        if (l_num > 0 and (line_temp[0] == 'contradiction' or line_temp[0] == 'neutral' or line_temp[0] == 'entailment')):\n",
    "            if (l_num % min(int(mnli_split_len/3), 250) == 0 or l_num + 1 == mnli_split_len):\n",
    "                    print('\\r' + str(l_num + 1) + '/' + str(mnli_split_len), end=\"\", flush=True)\n",
    "\n",
    "            labels.append(line_temp[0])\n",
    "            p_data.append(tokenize_string(line_temp[5]))\n",
    "            h_data.append(tokenize_string(line_temp[6]))\n",
    "\n",
    "\n",
    "p_embeddings = np.array(elmo.batch_to_embeddings(p_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Maps testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(100000000, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18684146 0.48278754 0.41862899]\n",
      " [0.94090055 0.23783645 0.43103484]\n",
      " [0.53371374 0.16162614 0.63580051]]\n",
      "[[0.31235169 0.24899142 0.3077785 ]\n",
      " [0.25172037 0.09333479 0.07135843]\n",
      " [0.71446911 0.29686622 0.11627533]]\n"
     ]
    }
   ],
   "source": [
    "print(a[1])\n",
    "print(a[1327847])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.memmap(path_to_test, dtype='float64', mode='w+', shape=a.shape)\n",
    "\n",
    "b[:] = a[:]\n",
    "del b\n",
    "del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "newb = np.memmap(path_to_test, dtype='float64', mode='r', shape=(100000000, 3, 3))#, offset=9*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000000, 3, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.18684146 0.48278754 0.41862899]\n",
      "  [0.94090055 0.23783645 0.43103484]\n",
      "  [0.53371374 0.16162614 0.63580051]]\n",
      "\n",
      " [[0.94837605 0.02247012 0.27250388]\n",
      "  [0.79709503 0.99217476 0.87264656]\n",
      "  [0.04750585 0.63639476 0.75983353]]\n",
      "\n",
      " [[0.47735076 0.60599367 0.49551718]\n",
      "  [0.26734756 0.07261834 0.96984787]\n",
      "  [0.6244175  0.11239189 0.24127904]]]\n"
     ]
    }
   ],
   "source": [
    "test = np.array(newb[1:4])\n",
    "print (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.from_numpy(np.array(newb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1868, 0.4828, 0.4186],\n",
      "        [0.9409, 0.2378, 0.4310],\n",
      "        [0.5337, 0.1616, 0.6358]], dtype=torch.float64)\n",
      "tensor([[0.3124, 0.2490, 0.3078],\n",
      "        [0.2517, 0.0933, 0.0714],\n",
      "        [0.7145, 0.2969, 0.1163]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(t[1])\n",
    "print(t[1327847])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "del t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del newb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "experiment_types = ['normal', 'zero_shot', 'few_shot']\n",
    "dataset_types = ['train', 'val', 'test']\n",
    "new_data_path = \"../datasets/Data/Ours/mini_test/\"\n",
    "\n",
    "type_determiner_tokens = [\"SUBJECT_ENTITY\", \"ate\", \"an\", \"OBJECT_ENTITY\", \"for\", \"breakfast\"]\n",
    "type_determiner_embeddings = elmo.embed_sentence(type_determiner_tokens)\n",
    "print(type_determiner_embeddings.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600\n",
      "\n",
      "Average time encoding: 0.496s.\n",
      "Average time writing: 0.103s.\n",
      "\n",
      "\n",
      "1200/1200\n",
      "\n",
      "Average time encoding: 0.484s.\n",
      "Average time writing: 0.104s.\n",
      "\n",
      "\n",
      "1200/1200\n",
      "\n",
      "Average time encoding: 0.477s.\n",
      "Average time writing: 0.103s.\n",
      "\n",
      "\n",
      "100/100\n",
      "\n",
      "Average time encoding: 0.475s.\n",
      "Average time writing: 0.102s.\n",
      "\n",
      "\n",
      "200/200\n",
      "\n",
      "Average time encoding: 0.476s.\n",
      "Average time writing: 0.102s.\n",
      "\n",
      "\n",
      "1240/1240\n",
      "\n",
      "Average time encoding: 0.471s.\n",
      "Average time writing: 0.100s.\n",
      "\n",
      "\n",
      "100/100\n",
      "\n",
      "Average time encoding: 0.472s.\n",
      "Average time writing: 0.101s.\n",
      "\n",
      "\n",
      "200/200\n",
      "\n",
      "Average time encoding: 0.473s.\n",
      "Average time writing: 0.101s.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_encodings = []\n",
    "time_writing = []\n",
    "for e_t_num, experiment_type in enumerate(experiment_types):\n",
    "    for d_t_num, dataset_type in enumerate(dataset_types):\n",
    "\n",
    "        if (experiment_type == 'normal' and dataset_type == 'train'):\n",
    "            continue\n",
    "\n",
    "        file_path = new_data_path + experiment_type + '/' + dataset_type\n",
    "        f_len = file_len(file_path + '.txt')\n",
    "\n",
    "        memmap = Variable_ELMO_array_storage(file_path, 1, \"wb+\", dtype_size_bytes=type_determiner_embeddings.dtype, num_encodings_types=3, size_encodings=1024)\n",
    "\n",
    "        id_labels_map = []\n",
    "        labels_id_map = {}\n",
    "        labels = []\n",
    "\n",
    "        with open(file_path + '.txt', \"r\", encoding='utf-8') as f:\n",
    "            for l_num, line in enumerate(f):\n",
    "                if (l_num % min(1, 250) == 0 or l_num + 1 == f_len):#int(f_len/3)\n",
    "                    print('\\r' + str(l_num + 1) + '/' + str(f_len), end=\"\", flush=True)\n",
    "\n",
    "                line_temp = line.rstrip('\\r\\n').split('\\t')\n",
    "\n",
    "                if (line_temp[0] not in labels_id_map):\n",
    "                    id_labels_map.append(line_temp[0])\n",
    "                    labels_id_map[line_temp[0]] = len(id_labels_map) - 1\n",
    "\n",
    "                labels.append(labels_id_map[line_temp[0]])\n",
    "                start = time.time()\n",
    "                encodings = elmo.embed_sentence(tokenize_string(line_temp[1]))\n",
    "                time_encodings.append(time.time() - start)\n",
    "                start = time.time()\n",
    "                memmap.save_line_sequential(encodings)\n",
    "                time_writing.append(time.time() - start)\n",
    "\n",
    "        memmap.save_instances_positions()\n",
    "        memmap.close_file()\n",
    "\n",
    "        with open(file_path + '.lbs', \"wb+\") as f:\n",
    "            pickle.dump(id_labels_map, f)\n",
    "            pickle.dump(labels_id_map, f)\n",
    "            pickle.dump(labels, f)\n",
    "\n",
    "        print(\"\\n\\nAverage time encoding: \" + '{:.3f}'.format(np.average(np.array(time_encodings))) + \"s.\")\n",
    "        print(\"Average time writing: \" + '{:.3f}'.format(np.average(np.array(time_writing))) + \"s.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path + '.lbs', \"wb+\") as f:\n",
    "            pickle.dump(id_labels_map, f)\n",
    "            pickle.dump(labels_id_map, f)\n",
    "            pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path + '.lbs', \"rb\") as f:\n",
    "            id_labels_map = pickle.load(f)\n",
    "            labels_id_map = pickle.load(f)\n",
    "            labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n",
      "IUCN conservation status 0\n",
      "1800\n"
     ]
    }
   ],
   "source": [
    "print(len(id_labels_map))\n",
    "print(len(labels_id_map))\n",
    "print(id_labels_map[0], labels_id_map[id_labels_map[0]])\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
