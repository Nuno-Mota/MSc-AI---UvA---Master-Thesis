{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscellaneous (track program progress and so on)\n",
    "import os\n",
    "import time\n",
    "from   datetime import timedelta\n",
    "import sys\n",
    "sys.path.append(\"..\") # So it's possible to retrieve packages at a higher top level. (than the directory where the notebook is running)\n",
    "\n",
    "# Math imports\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib as plt\n",
    "from   ipywidgets import interact, fixed\n",
    "from   IPython.display import clear_output\n",
    "\n",
    "# Saving/loading, memory optim imports\n",
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "# NLP imports\n",
    "from   allennlp.data.tokenizers.word_tokenizer import WordTokenizer\n",
    "# from   allennlp.predictors.predictor import Predictor\n",
    "# predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/ner-model-2018.12.18.tar.gz\")\n",
    "# predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/fine-grained-ner-model-elmo-2018.12.21.tar.gz\")\n",
    "import re\n",
    "\n",
    "# Own imports\n",
    "from helpers.general_helpers import pretty_dict_json_dump, pretty_dict_json_load, join_path, get_path_components\n",
    "from datasets.preprocessing.uw_re_uva import create_splits_for_masking_types, create_all_capped_masked_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_datasets = join_path([\"..\", \"datasets\" , \"Data\"])\n",
    "path_levy     = join_path([path_datasets, \"Levy\"])\n",
    "path_ours     = join_path([path_datasets, \"Ours\"])\n",
    "\n",
    "levy_full_dataset_path = join_path([path_levy, 'positive_examples'])\n",
    "\n",
    "path_relation_names = join_path([path_ours, \"relation_names.txt\"])\n",
    "path_relation_descriptions = join_path([path_ours, \"relations_descriptions\"])# + each relation's name\n",
    "path_proposed_splits = join_path([path_ours, \"proposed_splits\"])\n",
    "path_proposed_splits_new = join_path([path_ours, \"proposed_splits_new\"])\n",
    "path_proposed_splits_masking = join_path([path_ours, \"proposed_splits_masking\"])\n",
    "path_proposed_splits_original = join_path([path_proposed_splits, \"original\"])\n",
    "path_proposed_splits_original_new = join_path([path_proposed_splits_new, \"original\"])\n",
    "path_proposed_splits_original_masking = join_path([path_proposed_splits_masking, \"original\"])\n",
    "\n",
    "# Load relation names from file.\n",
    "relation_list = []\n",
    "\n",
    "with open(path_relation_names, \"r\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        relation_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows to get the number of lines (and, hence, instances) in a file.\n",
    "def file_len(fname):\n",
    "    with open(fname, \"r\", encoding='utf-8') as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "## ELMO computes embeddings on pre-tokenized, space separated sentences! ##\n",
    "###########################################################################\n",
    "tokenizer = WordTokenizer()\n",
    "\n",
    "# Tokenize with a space between tokens.\n",
    "def space_word_tokenize_string(s, tokenizer):\n",
    "    tokenized_string = [token.text for token in tokenizer.tokenize(s)]\n",
    "    sentence = []\n",
    "    for k, token in enumerate(tokenized_string):\n",
    "        sentence += [token] + ([' '] if k + 1 < len(tokenized_string) else [])\n",
    "    return sentence\n",
    "\n",
    "# Tokenize in a way that matches AllenNLP's NER predictor.\n",
    "def NER_space_word_tokenize_string(s, tokenizer):\n",
    "    split_s = s.split()\n",
    "    sentence = []\n",
    "    for k, split_s_ele in enumerate(split_s):\n",
    "        sentence += [token.text for token in tokenizer.tokenize(split_s_ele)]\n",
    "        if (k + 1 < len(split_s)):\n",
    "            sentence += [' ']\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data from Levy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For printing purposes, we also get the number of lines present in Levy's positive examples.\n",
    "NUM_LINES_POSITIVE = file_len(levy_full_dataset_path)\n",
    "\n",
    "# We define a maximum (tokenized) sentence length.\n",
    "max_sentence_length = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Damage Assessement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start by aligning all sentences used to create the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "996361\n"
     ]
    }
   ],
   "source": [
    "masking_types = ['original', 'sub_obj_masking', 'NER_masking']\n",
    "\n",
    "paired_final_sentences = [[], [], []]\n",
    "\n",
    "for relation in relation_list:\n",
    "    for m_num, masking_type in enumerate(masking_types):\n",
    "        with open(join_path([path_ours, \"Levy_by_relation\", \"max_tokenized_sentence_len_60_exclusive_and_unique_relation_masking\",\n",
    "                             masking_type, re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt']), 'r', encoding=\"utf-8\") as f:\n",
    "            for l_num, line in enumerate(f):\n",
    "                line_temp = line.strip()\n",
    "                paired_final_sentences[m_num].append(line_temp.split(\"\\t\")[0] if masking_type == 'original' else line_temp)\n",
    "\n",
    "paired_final_sentences = list(zip(paired_final_sentences[0], paired_final_sentences[1], paired_final_sentences[2]))\n",
    "print(len(paired_final_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%\n",
      "\n",
      "1478736 589248 39.84808647385334\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1944829 29597 1.5218304539884997\n"
     ]
    }
   ],
   "source": [
    "sentences_to_relations = {}\n",
    "annotated_sentences_to_relations = {}\n",
    "\n",
    "l_num = 0\n",
    "for relation in relation_list:\n",
    "    with open(join_path([path_ours, \"Levy_by_relation\", \"max_tokenized_sentence_len_60\",\n",
    "                         re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt']), 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if (l_num % 500 == 0 or l_num + 1 == lines_before):\n",
    "                print('\\r' + '{:6.2f}%'.format(100*(l_num)/lines_before), end=\"\", flush=True)\n",
    "            l_num += 1\n",
    "\n",
    "            line_temp = line.strip().split(\"\\t\")\n",
    "            original_sentence = line_temp[0]\n",
    "            \n",
    "            if original_sentence not in sentences_to_relations:\n",
    "                sentences_to_relations[original_sentence] = set()\n",
    "            sentences_to_relations[original_sentence].add(relation)\n",
    "\n",
    "            # We assume the first entity is the subject entity and that the second one is the object entity.\n",
    "            # Any other entities are disregarded.\n",
    "            tokenized_sentence = space_word_tokenize_string(line_temp[0], tokenizer)\n",
    "            tokenized_subject_entity = space_word_tokenize_string(line_temp[1], tokenizer)\n",
    "            tokenized_object_entity = space_word_tokenize_string(line_temp[2], tokenizer)\n",
    "\n",
    "            masked_sentence = []\n",
    "            word_pos = 0\n",
    "            while (word_pos < len(tokenized_sentence)):\n",
    "                current_word = tokenized_sentence[word_pos]\n",
    "\n",
    "                match = False\n",
    "                if (current_word == tokenized_subject_entity[0] or\n",
    "                        current_word == tokenized_object_entity[0]):\n",
    "                    # We assume that the subject entity is different from the object entity.\n",
    "                    # First we start by testing the subject entity.\n",
    "                    if (word_pos + len(tokenized_subject_entity) <= len(tokenized_sentence)):\n",
    "                        match = True\n",
    "                        for subj_word_num, subj_word in enumerate(tokenized_subject_entity):\n",
    "                            if (subj_word != tokenized_sentence[word_pos + subj_word_num]):\n",
    "                                match = False\n",
    "                                break\n",
    "                        if (match):\n",
    "                            masked_sentence.append('SUBJECT_ENTITY')\n",
    "                            word_pos += len(tokenized_subject_entity)\n",
    "\n",
    "                    # Now we test the object entity.\n",
    "                    if (not match and word_pos + len(tokenized_object_entity) <= len(tokenized_sentence)):\n",
    "                        match = True\n",
    "                        for obj_word_num, obj_word in enumerate(tokenized_object_entity):\n",
    "                            if (obj_word != tokenized_sentence[word_pos + obj_word_num]):\n",
    "                                match = False\n",
    "                                break\n",
    "                        if (match):\n",
    "                            masked_sentence.append('OBJECT_ENTITY')\n",
    "                            word_pos += len(tokenized_object_entity)\n",
    "\n",
    "                if (not match):\n",
    "                    masked_sentence.append(tokenized_sentence[word_pos])\n",
    "                    word_pos += 1\n",
    "\n",
    "            masked_sentence = ''.join(masked_sentence)\n",
    "            if masked_sentence not in annotated_sentences_to_relations:\n",
    "                annotated_sentences_to_relations[masked_sentence] = set()\n",
    "            annotated_sentences_to_relations[masked_sentence].add(relation)\n",
    "\n",
    "num_unique_sentences = len(sentences_to_relations)\n",
    "num_sentences_with_more_relations = sum(len(sentences_to_relations[sentence]) > 1 for sentence in sentences_to_relations)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(num_unique_sentences, num_sentences_with_more_relations, 100*num_sentences_with_more_relations/num_unique_sentences)\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "num_unique_annotated_sentences = len(annotated_sentences_to_relations)\n",
    "num_annotated_sentences_with_more_relations = sum(len(annotated_sentences_to_relations[masked_sentence]) > 1 for masked_sentence in annotated_sentences_to_relations)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(num_unique_annotated_sentences, num_annotated_sentences_with_more_relations, 100*num_annotated_sentences_with_more_relations/num_unique_annotated_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "996361 404580 40.60576437656633\n",
      "\n",
      "\n",
      "996361 23367 2.345234307645522\n"
     ]
    }
   ],
   "source": [
    "num_final_sentences = len(paired_final_sentences)\n",
    "num_sentences_with_more_relations = sum(len(sentences_to_relations[sentence]) > 1 for sentence, _, _ in paired_final_sentences)\n",
    "print(num_final_sentences, num_sentences_with_more_relations, 100*num_sentences_with_more_relations/num_final_sentences)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "num_annotated_sentences_with_more_relations = sum(len(annotated_sentences_to_relations[sub_obj_sent]) > 1 for _, sub_obj_sent, _ in paired_final_sentences)\n",
    "\n",
    "print(num_final_sentences, num_annotated_sentences_with_more_relations, 100*num_annotated_sentences_with_more_relations/num_final_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2314975 1472278\n"
     ]
    }
   ],
   "source": [
    "lines_before = sum(file_len(join_path([path_ours, \"Levy_by_relation\", \"max_tokenized_sentence_len_60\",\n",
    "                                       re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt'])) for relation in relation_list)\n",
    "\n",
    "lines_after = sum(file_len(join_path([path_ours, \"Levy_by_relation\", \"max_tokenized_sentence_len_60_exclusive_and_unique_relation\",\n",
    "                                      \"NER_masking\", re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt'])) for relation in relation_list)\n",
    "\n",
    "print(lines_before, lines_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%1944829 29597 1.5218304539884997\n"
     ]
    }
   ],
   "source": [
    "annotated_sentences_to_relations = {}\n",
    "\n",
    "l_num = 0\n",
    "for relation in relation_list:\n",
    "    with open(join_path([path_ours, \"Levy_by_relation\", \"max_tokenized_sentence_len_60\",\n",
    "                         re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt']), 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if (l_num % 500 == 0 or l_num + 1 == lines_before):\n",
    "                print('\\r' + '{:6.2f}%'.format(100*(l_num)/lines_before), end=\"\", flush=True)\n",
    "            l_num += 1\n",
    "\n",
    "            line_temp = line.strip().split(\"\\t\")\n",
    "\n",
    "            # We assume the first entity is the subject entity and that the second one is the object entity.\n",
    "            # Any other entities are disregarded.\n",
    "            tokenized_sentence       = space_word_tokenize_string(line_temp[0], tokenizer)\n",
    "            tokenized_subject_entity = space_word_tokenize_string(line_temp[1], tokenizer)\n",
    "            tokenized_object_entity  = space_word_tokenize_string(line_temp[2], tokenizer)\n",
    "\n",
    "            masked_sentence = []\n",
    "            word_pos = 0\n",
    "            while (word_pos < len(tokenized_sentence)):\n",
    "                current_word = tokenized_sentence[word_pos]\n",
    "\n",
    "                match = False\n",
    "                if (current_word == tokenized_subject_entity[0] or\n",
    "                        current_word == tokenized_object_entity[0]):\n",
    "                    # We assume that the subject entity is different from the object entity.\n",
    "                    # First we start by testing the subject entity.\n",
    "                    if (word_pos + len(tokenized_subject_entity) <= len(tokenized_sentence)):\n",
    "                        match = True\n",
    "                        for subj_word_num, subj_word in enumerate(tokenized_subject_entity):\n",
    "                            if (subj_word != tokenized_sentence[word_pos + subj_word_num]):\n",
    "                                match = False\n",
    "                                break\n",
    "                        if (match):\n",
    "                            masked_sentence.append('SUBJECT_ENTITY')\n",
    "                            word_pos += len(tokenized_subject_entity)\n",
    "\n",
    "                    # Now we test the object entity.\n",
    "                    if (not match and word_pos + len(tokenized_object_entity) <= len(tokenized_sentence)):\n",
    "                        match = True\n",
    "                        for obj_word_num, obj_word in enumerate(tokenized_object_entity):\n",
    "                            if (obj_word != tokenized_sentence[word_pos + obj_word_num]):\n",
    "                                match = False\n",
    "                                break\n",
    "                        if (match):\n",
    "                            masked_sentence.append('OBJECT_ENTITY')\n",
    "                            word_pos += len(tokenized_object_entity)\n",
    "\n",
    "                if (not match):\n",
    "                    masked_sentence.append(tokenized_sentence[word_pos])\n",
    "                    word_pos += 1\n",
    "\n",
    "            masked_sentence = ''.join(masked_sentence)\n",
    "            if masked_sentence not in annotated_sentences_to_relations:\n",
    "                annotated_sentences_to_relations[masked_sentence] = set()\n",
    "            annotated_sentences_to_relations[masked_sentence].add(relation)\n",
    "\n",
    "num_unique_sentences = len(annotated_sentences_to_relations)\n",
    "num_sentences_with_more_relations = sum(len(annotated_sentences_to_relations[masked_sentence]) > 1 for masked_sentence in annotated_sentences_to_relations)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(num_unique_sentences, num_sentences_with_more_relations, 100*num_sentences_with_more_relations/num_unique_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1210977 544 0.04492240562785255\n"
     ]
    }
   ],
   "source": [
    "sentences_to_relations = {}\n",
    "\n",
    "for relation in relation_list:\n",
    "    with open(join_path([path_ours, \"Levy_by_relation\", \"max_tokenized_sentence_len_60_exclusive_and_unique_relation\",\n",
    "                         \"sub_obj_masking\", re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt']), 'r', encoding=\"utf-8\") as f:\n",
    "#     with open(join_path([path_ours, \"Levy_by_relation\", \"max_tokenized_sentence_len_60\",\n",
    "#                      re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt']), 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            sentence = line.strip()\n",
    "#             line_temp = line.strip().split(\"\\t\")\n",
    "#             sentence = line_temp[0]\n",
    "#             entity_1 = line_temp[1]\n",
    "#             entity_2 = line_temp[2]\n",
    "    \n",
    "            if sentence not in sentences_to_relations:\n",
    "                sentences_to_relations[sentence] = set()\n",
    "            sentences_to_relations[sentence].add(relation)\n",
    "#             if (sentence, entity_1, entity_2) not in sentences_to_relations:\n",
    "#                 sentences_to_relations[(sentence, entity_1, entity_2)] = set()\n",
    "#             sentences_to_relations[(sentence, entity_1, entity_2)].add(relation)\n",
    "\n",
    "num_unique_sentences = len(sentences_to_relations)\n",
    "num_sentences_with_more_relations = sum(len(sentences_to_relations[mention]) > 1 for mention in sentences_to_relations)\n",
    "\n",
    "print(num_unique_sentences, num_sentences_with_more_relations, 100*num_sentences_with_more_relations/num_unique_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We start by counting the total number of unique sentences and unique relation mentions in Levy's dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%\n",
      "Number of unique sentences: 1499932\n",
      "Number of unique relation mentions: 2416256\n"
     ]
    }
   ],
   "source": [
    "unique_sentences = set()\n",
    "unique_relation_mentions = set()\n",
    "\n",
    "with open(levy_full_dataset_path, \"r\", encoding='utf-8') as f:\n",
    "\n",
    "    for l_num, line in enumerate(f):\n",
    "        if (l_num % 500 == 0 or l_num + 1 == NUM_LINES_POSITIVE):\n",
    "            print('\\r' + '{:6.2f}%'.format(100*(l_num+1)/NUM_LINES_POSITIVE), end=\"\", flush=True)\n",
    "\n",
    "        line_temp = line.strip().split(\"\\t\")\n",
    "\n",
    "        relation       = line_temp[0]\n",
    "        subject_entity = line_temp[2]\n",
    "        sentence       = line_temp[3]\n",
    "        unique_sentences.add(sentence)\n",
    "        for object_entity in line_temp[4:]:\n",
    "            unique_relation_mentions.add((sentence, relation, subject_entity, object_entity))\n",
    "\n",
    "print()\n",
    "print(\"Number of unique sentences:\", len(unique_sentences))\n",
    "print(\"Number of unique relation mentions:\", len(unique_relation_mentions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we count the same statistics, but for a maximum tokenized sentence length of max_sentence_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%\n",
      "Number of unique sentences: 1478736\n",
      "Number of unique relation mentions: 2376807\n"
     ]
    }
   ],
   "source": [
    "unique_sentences_max_length = set()\n",
    "unique_relation_mentions_max_length = set()\n",
    "\n",
    "with open(levy_full_dataset_path, \"r\", encoding='utf-8') as f:\n",
    "\n",
    "    for l_num, line in enumerate(f):\n",
    "        if (l_num % 500 == 0 or l_num + 1 == NUM_LINES_POSITIVE):\n",
    "            print('\\r' + '{:6.2f}%'.format(100*(l_num+1)/NUM_LINES_POSITIVE), end=\"\", flush=True)\n",
    "\n",
    "        line_temp = line.strip().split(\"\\t\")\n",
    "\n",
    "        if (len(tokenizer.tokenize(line_temp[3])) <= max_sentence_length):\n",
    "            relation       = line_temp[0]\n",
    "            subject_entity = line_temp[2]\n",
    "            sentence       = line_temp[3]\n",
    "            unique_sentences_max_length.add(sentence)\n",
    "            for object_entity in line_temp[4:]:\n",
    "                unique_relation_mentions_max_length.add((sentence, relation, subject_entity, object_entity))\n",
    "\n",
    "print()\n",
    "print(\"Number of unique sentences:\", len(unique_sentences_max_length))\n",
    "print(\"Number of unique relation mentions:\", len(unique_relation_mentions_max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We evaluate how many sentences have more than 1 relation mention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1478736\n",
      "889488\n",
      "889488\n",
      "589248\n",
      "1425423\n"
     ]
    }
   ],
   "source": [
    "sentence_multiple_mentions = {}\n",
    "sentence_different_relation_mentions = {}\n",
    "\n",
    "for sentence, relation, entity_1, entity_2 in unique_relation_mentions_max_length:\n",
    "    if (sentence) not in sentence_multiple_mentions:\n",
    "        sentence_multiple_mentions[sentence] = 1\n",
    "    else:\n",
    "        sentence_multiple_mentions[sentence] += 1\n",
    "\n",
    "    if (sentence not in sentence_different_relation_mentions):\n",
    "        sentence_different_relation_mentions[sentence] = set()\n",
    "    sentence_different_relation_mentions[sentence].add(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct sentences: 1478736 True\n",
      "\n",
      "Number of sentences with one single relation mention: 875245\n",
      "Number of sentences that express one single relation: 889488\n",
      "Number of sentences that express one single relation, but have multiple mentions: 14243\n",
      "\n",
      "Number of sentences with multiple relation mentions: 1\n",
      "Number of sentences that express multiple relations: 589248\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of distinct sentences:\", len(sentence_multiple_mentions), len(sentence_multiple_mentions) == len(sentence_different_relation_mentions))\n",
    "print()\n",
    "\n",
    "n_sentences_single_mention = sum(sentence_multiple_mentions[sentence] == 1 for sentence in sentence_multiple_mentions)\n",
    "print(\"Number of sentences with one single relation mention:\", n_sentences_single_mention)\n",
    "n_sentences_express_single_relation = sum(len(sentence_different_relation_mentions[sentence]) == 1 for sentence in sentence_different_relation_mentions)\n",
    "print(\"Number of sentences that express one single relation:\", n_sentences_express_single_relation)\n",
    "n_sentences_single_rel_multiple_mentions = n_sentences_express_single_relation - n_sentences_single_mention\n",
    "print(\"Number of sentences that express one single relation, but have multiple mentions:\", n_sentences_single_rel_multiple_mentions)\n",
    "print()\n",
    "\n",
    "n_sentences_multiple_mentions = sum(sentence_multiple_mentions[sentence] > 20 for sentence in sentence_multiple_mentions)\n",
    "print(\"Number of sentences with multiple relation mentions:\", n_sentences_multiple_mentions)\n",
    "n_sentences_express_multiple_relations = sum(len(sentence_different_relation_mentions[sentence]) > 1 for sentence in sentence_different_relation_mentions)\n",
    "print(\"Number of sentences that express multiple relations:\", n_sentences_express_multiple_relations)\n",
    "# n_sentences_multiple_rels_multiple_mentions = \n",
    "# print(\"Number of sentences that express multiple relations and have multiple mentions:\", n_sentences_express_multiple_relations - n_sentences_multiple_mentions)\n",
    "\n",
    "# print(len(sentence_different_relation_mentions))\n",
    "# print(sum(len(sentence_different_relation_mentions[sentence]) == 1 for sentence in sentence_different_relation_mentions))\n",
    "# print(sum(len(sentence_different_relation_mentions[sentence]) for sentence in sentence_different_relation_mentions\n",
    "#                                                                   if len(sentence_different_relation_mentions[sentence]) == 1))\n",
    "# print(sum(len(sentence_different_relation_mentions[sentence]) > 1 for sentence in sentence_different_relation_mentions))\n",
    "# print(sum(len(sentence_different_relation_mentions[sentence]) for sentence in sentence_different_relation_mentions\n",
    "#                                                                   if len(sentence_different_relation_mentions[sentence]) > 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we extract all unique tuples (relation, sentence, entities) that match the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%\n",
      "\n",
      "Distinct number of instances: 2314975\n",
      "Number of instances without at least 2 entities: 0\n"
     ]
    }
   ],
   "source": [
    "# Make sure the \"positive_'str(max_sentence_length)'/\" Directory exists.\n",
    "directory = os.path.dirname(path_ours + 'Levy_by_relation/max_tokenized_sentence_len_' + str(max_sentence_length) + '/')\n",
    "if (not os.path.exists(directory)):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Open the file pointers for each relation.\n",
    "relation_filepointers_capped = {}\n",
    "for relation in relation_list:\n",
    "    file_name_positive = path_ours + 'Levy_by_relation/max_tokenized_sentence_len_' + str(max_sentence_length) + '/'\n",
    "    file_name_positive += re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt'\n",
    "    relation_filepointers_capped[relation] = open(file_name_positive, 'w', encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "num_sentences_without_at_least_two_entities = 0 # Used for statistics.\n",
    "num_distinct_instances = 0 # Used for statistics.\n",
    "\n",
    "# Used to make sure that we do not have repeated training instances. Due to multiple questions (the questions in Levy's framework) we\n",
    "# expect some tuple repetitions.\n",
    "sentences_hash_dict = {}\n",
    "\n",
    "with open(levy_full_dataset_path, \"r\", encoding='utf-8') as f:\n",
    "\n",
    "    # Used for printing purposes.\n",
    "    l_num = 0\n",
    "\n",
    "    # Used to check for repetition.\n",
    "    current_cursor_value = f.tell()\n",
    "    line = f.readline()\n",
    "    next_cursor_position = f.tell()\n",
    "\n",
    "    while line:\n",
    "        if (l_num % 500 == 0 or l_num + 1 == NUM_LINES_POSITIVE):\n",
    "            print('\\r' + '{:6.2f}%'.format(100*(l_num+1)/NUM_LINES_POSITIVE), end=\"\", flush=True)\n",
    "        \n",
    "        line_temp = line.strip().split(\"\\t\")\n",
    "\n",
    "        # We only want sentences that have at least two entities present, since we are modelling binary relations. From inspection, sentences\n",
    "        # with more than 2 entities just have that the subject entity relates to every other wntity in the same way, i.e., it has the same relation.\n",
    "        if (len(line_temp) < 5):\n",
    "            num_sentences_without_at_least_two_entities += 1\n",
    "\n",
    "        else:\n",
    "            # We cap the tokenized sentence length.\n",
    "            if (len(tokenizer.tokenize(line_temp[3])) <= max_sentence_length):\n",
    "\n",
    "                relation = line_temp[0]\n",
    "                sentence = line_temp[3]\n",
    "                entities = [line_temp[2]] + [line_temp[i] for i in range(4, len(line_temp))]\n",
    "\n",
    "                # We compute the hash so that the 'sentences_hash_dict' doesn't have to keep the entire sentence around.\n",
    "                sentence_hash = hashlib.md5(line_temp[3].encode('utf-8')).hexdigest()\n",
    "\n",
    "                # Check if this is a new training instance.\n",
    "                new_instance = True\n",
    "                if (sentence_hash not in sentences_hash_dict):\n",
    "                    sentences_hash_dict[sentence_hash] = [current_cursor_value]\n",
    "                else:\n",
    "                    for previous_sentence_cursor_position in sentences_hash_dict[sentence_hash]:\n",
    "                        f.seek(previous_sentence_cursor_position)\n",
    "                        previous_sentence = f.readline()\n",
    "                        previous_sentence_temp = previous_sentence.strip().split(\"\\t\")\n",
    "\n",
    "                        old_relation = previous_sentence_temp[0]\n",
    "                        old_sentence = previous_sentence_temp[3]\n",
    "                        old_entities = [previous_sentence_temp[2]] + [previous_sentence_temp[i] for i in range(4, len(previous_sentence_temp))]\n",
    "\n",
    "                        # Same sentence?\n",
    "                        if (sentence == old_sentence):\n",
    "                            # Same relation?\n",
    "                            if (relation == old_relation):\n",
    "                                # Same entities?\n",
    "                                if (len(entities) == len(old_entities) and all(entities[i] == old_entities[i] for i in range(len(entities)))):\n",
    "                                    new_instance = False\n",
    "                                    break\n",
    "\n",
    "                f.seek(next_cursor_position)\n",
    "                if (new_instance):\n",
    "                    num_distinct_instances += 1\n",
    "\n",
    "                    sentences_hash_dict[sentence_hash].append(current_cursor_value)\n",
    "\n",
    "                    new_instance = [sentence] + entities\n",
    "                    for i, data_element in enumerate(new_instance):\n",
    "                        relation_filepointers_capped[relation].write(data_element + ('\\t' if i + 1 < len(new_instance) else '\\n'))                                \n",
    "\n",
    "\n",
    "        l_num += 1\n",
    "        current_cursor_value = f.tell()\n",
    "        line = f.readline()\n",
    "        next_cursor_position = f.tell()\n",
    "\n",
    "\n",
    "# Close the file pointers.\n",
    "for relation in relation_filepointers_capped:\n",
    "    relation_filepointers_capped[relation].close()\n",
    "\n",
    "print(\"\\n\\nDistinct number of instances:\", num_distinct_instances)\n",
    "print(\"Number of instances without at least 2 entities:\", num_sentences_without_at_least_two_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we get some statistics on those tuples, such as how many instances of the sentences appear in more than one relation and how many sentences appear more than once within the same relation, due to different entities. We also count the number of sentences assigned to each relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%\n",
      "\n",
      "Number of repetitions, across relation (the same sentence might be repeated more than once): 836239\n",
      "Number of sentences that appear more than once in the same relation: 64\n"
     ]
    }
   ],
   "source": [
    "seen_xs = {}\n",
    "\n",
    "num_instances = sum(file_len(path_ours + 'Levy_by_relation/max_tokenized_sentence_len_' + str(max_sentence_length) + '/' +\n",
    "                             re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt') for relation in relation_list)\n",
    "\n",
    "repeated_xs = 0\n",
    "repeated_xs_same_relation = 0\n",
    "\n",
    "l_num = 1\n",
    "for relation in relation_list:\n",
    "    file_name_positive = path_ours + 'Levy_by_relation/max_tokenized_sentence_len_' + str(max_sentence_length) + '/'\n",
    "    file_name_positive += re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt'\n",
    "\n",
    "    with open(file_name_positive, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if (l_num % 500 == 0 or l_num + 1 == num_instances):\n",
    "                print('\\r' + '{:6.2f}%'.format(100*(l_num)/num_instances), end=\"\", flush=True)\n",
    "            l_num += 1\n",
    "\n",
    "            x = line.strip().split('\\t')[0]\n",
    "            if (x in seen_xs):\n",
    "                repeated_xs += 1\n",
    "                if (relation in seen_xs[x]):\n",
    "                    repeated_xs_same_relation += 1\n",
    "                    seen_xs[x][relation] += 1\n",
    "                else:\n",
    "                    seen_xs[x][relation] = 1\n",
    "            else:\n",
    "                seen_xs[x] = {relation: 1}\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Number of repetitions, across relation (the same sentence might be repeated more than once):\", repeated_xs)\n",
    "print(\"Number of sentences that appear more than once in the same relation:\", repeated_xs_same_relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We get the data distribution per relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2314975\n",
      "\n",
      "[['medical condition', 105], ['architectural style', 110], ['editor', 113], ['nominated for', 115], ['head of government', 126], ['date of official opening', 127], ['military rank', 127], ['time of discovery', 128], ['home venue', 139], ['replaced by', 143], ['instrumentation', 146], ['chairperson', 178], ['operating system', 182], ['product', 183], ['chromosome', 190], ['conferred by', 206], ['sister', 213], ['service entry', 218], ['religious order', 219], ['site of astronomical discovery', 219], ['manner of death', 222], ['convicted of', 228], ['license', 242], ['noble title', 251], ['standards body', 254], ['located next to body of water', 257], ['airline hub', 293], ['film editor', 308], ['illustrator', 326], ['time of spacecraft launch', 344], ['characters', 354], ['canonization status', 368], ['noble family', 392], ['location of formation', 404], ['collection', 414], ['IUCN conservation status', 420], ['drafted by', 440], ['industry', 468], ['based on', 502], ['located on astronomical body', 514], ['place of burial', 519], ['league', 540], ['stock exchange', 563], ['found in taxon', 568], ['architect', 571], ['occupant', 575], ['designer', 588], ['crosses', 592], ['parent company', 597], ['from fictional universe', 615], ['programming language', 636], ['discoverer or inventor', 652], ['start time', 657], ['residence', 673], ['distributor', 710], ['material used', 724], ['vessel class', 737], ['language of work or name', 935], ['production company', 956], ['point in time', 960], ['brother', 1015], ['cause of death', 1024], ['narrative location', 1037], ['lyrics by', 1044], ['end time', 1063], ['connecting line', 1097], ['mother', 1322], ['instrument', 1341], ['child', 1354], ['mouth of the watercourse', 1492], ['manufacturer', 1497], ['named after', 1638], ['founder', 1647], ['dissolved or abolished', 1652], ['present in work', 1746], ['spouse', 2059], ['licensed to broadcast to', 2518], ['voice type', 2723], ['developer', 3133], ['constellation', 3237], ['conflict', 3331], ['military branch', 3361], ['creator', 3689], ['father', 3733], ['award received', 4751], ['record label', 4789], ['original network', 4802], ['publisher', 4860], ['country of origin', 4935], ['employer', 4982], ['continent', 5019], ['educated at', 5497], ['series', 5990], ['screenwriter', 7236], ['native language', 7525], ['position held', 7645], ['member of political party', 7978], ['sex or gender', 9078], ['participant of', 10703], ['headquarters location', 15467], ['member of sports team', 16846], ['cast member', 17678], ['author', 22731], ['position played on team / speciality', 26043], ['place of death', 26875], ['director', 32312], ['inception', 33890], ['sport', 35659], ['languages spoken or written', 37244], ['performer', 45022], ['parent taxon', 46266], ['publication date', 54550], ['country of citizenship', 75310], ['place of birth', 105320], ['date of death', 151914], ['taxon rank', 194060], ['located in the administrative territorial entity', 258696], ['occupation', 285772], ['date of birth', 325449], ['country', 340772]]\n"
     ]
    }
   ],
   "source": [
    "# Create a map from index to a tuple that indicates the relation and corresponding number of instances in that relation.\n",
    "idx_to_relation_and_len = []\n",
    "\n",
    "\n",
    "# Go over each relation and get the number of instances.\n",
    "for r_num, relation in enumerate(relation_list):\n",
    "    file_name_positive = path_ours + 'Levy_by_relation/max_tokenized_sentence_len_' + str(max_sentence_length) + '/'\n",
    "    file_name_positive += re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt'\n",
    "    idx_to_relation_and_len.append([relation, file_len(file_name_positive)])\n",
    "\n",
    "# Sort the relations by number of instances, in increasing order.\n",
    "idx_to_relation_and_len.sort(key=lambda lst: lst[1])\n",
    "print(sum(lst[1] for lst in idx_to_relation_and_len))\n",
    "print()\n",
    "print(idx_to_relation_and_len)\n",
    "\n",
    "# Create a map from relation to (sorted) index.\n",
    "relation_to_idx = {relation: idx for idx, (relation, _) in enumerate(idx_to_relation_and_len)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we could allow for the same sentence, x, to be present in multiple relations (probably with different entities involved) or even for the same x to be present more than once for the same relation, for different entities. However, since at test time we do not really know how to mask (as that would mean knowing beforehand which entities are in the relation, which could happen, but is unlikely), we avoid having repeated sentences over the dataset. So, we assign those repeated sentences to the relation with the fewest instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\datasets\\\\Data\\\\OursLevy_by_relation/max_tokenized_sentence_len_60/IUCN_conservation_status.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-75723573f216>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelation_path_capped_exclusive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf_capped_exclusive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelation_path_capped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf_capped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf_capped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ml_num\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0ml_num\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnum_instances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\datasets\\\\Data\\\\OursLevy_by_relation/max_tokenized_sentence_len_60/IUCN_conservation_status.txt'"
     ]
    }
   ],
   "source": [
    "# Make sure the \"positive_'str(max_sentence_length)'_exclusive_and_unique_relation/\" Directory exists.\n",
    "path_dir_capped = path_ours + 'Levy_by_relation/max_tokenized_sentence_len_' + str(max_sentence_length) + '/'\n",
    "path_dir_capped_exclusive = path_ours + 'Levy_by_relation/max_tokenized_sentence_len_' + str(max_sentence_length) + '_exclusive_and_unique_relation/'\n",
    "directory = os.path.dirname(path_dir_capped_exclusive)\n",
    "if (not os.path.exists(directory)):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "\n",
    "relation_filepointers_capped_exclusive = {}\n",
    "\n",
    "l_num = 1\n",
    "\n",
    "\n",
    "for relation in relation_list:\n",
    "\n",
    "    relation_path_capped = path_dir_capped + re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt'\n",
    "    relation_path_capped_exclusive = path_dir_capped_exclusive + re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt'\n",
    "\n",
    "    with open(relation_path_capped_exclusive, 'w', encoding='utf-8') as f_capped_exclusive:\n",
    "        with open(relation_path_capped, 'r', encoding='utf-8') as f_capped:\n",
    "            for line in f_capped:\n",
    "                if (l_num % 500 == 0 or l_num + 1 == num_instances):\n",
    "                    print('\\r' + '{:6.2f}%'.format(100*(l_num)/num_instances), end=\"\", flush=True)\n",
    "                l_num += 1\n",
    "\n",
    "                x = line.strip().split('\\t')[0]\n",
    "\n",
    "                try:\n",
    "                    if (seen_xs[x] != 'DONE'):\n",
    "                        if (len(seen_xs[x]) == 1):\n",
    "                            f_capped_exclusive.write(line)\n",
    "                            idx_to_relation_and_len[relation_to_idx[relation]][1] -= seen_xs[x][relation] - 1\n",
    "                            seen_xs[x] = 'DONE'\n",
    "\n",
    "                        else:\n",
    "                            min_num_insts = np.inf\n",
    "                            for repeated_inst_rel in seen_xs[x]:\n",
    "                                if (idx_to_relation_and_len[relation_to_idx[repeated_inst_rel]][1] < min_num_insts):\n",
    "                                    min_num_insts = idx_to_relation_and_len[relation_to_idx[repeated_inst_rel]][1]\n",
    "                                    rel_with_fewest_num_insts = repeated_inst_rel\n",
    "\n",
    "                            if (relation == rel_with_fewest_num_insts):\n",
    "                                f_capped_exclusive.write(line)\n",
    "\n",
    "                                for repeated_inst_rel in seen_xs[x]:\n",
    "                                    if (relation == repeated_inst_rel):\n",
    "                                        idx_to_relation_and_len[relation_to_idx[repeated_inst_rel]][1] -= seen_xs[x][repeated_inst_rel] - 1\n",
    "                                    else:\n",
    "                                        idx_to_relation_and_len[relation_to_idx[repeated_inst_rel]][1] -= seen_xs[x][repeated_inst_rel]\n",
    "                                seen_xs[x] = 'DONE'\n",
    "                            else:\n",
    "                                seen_xs[x][relation] -= 1\n",
    "                                idx_to_relation_and_len[relation_to_idx[relation]][1] -= 1\n",
    "\n",
    "                except Exception as err:\n",
    "                    # Here in case something needs to be printed.\n",
    "                    raise err\n",
    "\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "idx_to_relation_and_len.sort(key=lambda lst: lst[1])\n",
    "print(\"Total number of unique and exclusive sentences:\", sum(lst[1] for lst in idx_to_relation_and_len))\n",
    "print()\n",
    "print(idx_to_relation_and_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also know we'll be running experiments with masking, so we have to guarantee that the masking does not break the uniqueness of the sentences, or else we might break our disjointness assumptions. Here we determine which sentences end up reducing to the same one, after having been masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique and exclusive sentences: 1472278\n",
      "\n",
      "[['medical condition', 105], ['architectural style', 110], ['editor', 113], ['nominated for', 115], ['replaced by', 125], ['head of government', 126], ['date of official opening', 127], ['military rank', 127], ['time of discovery', 128], ['home venue', 139], ['instrumentation', 146], ['chairperson', 177], ['operating system', 182], ['product', 183], ['chromosome', 190], ['site of astronomical discovery', 195], ['conferred by', 206], ['sister', 212], ['service entry', 218], ['religious order', 219], ['manner of death', 222], ['convicted of', 228], ['license', 238], ['noble title', 251], ['standards body', 254], ['located next to body of water', 257], ['airline hub', 293], ['film editor', 308], ['illustrator', 325], ['time of spacecraft launch', 344], ['characters', 353], ['canonization status', 364], ['noble family', 392], ['location of formation', 404], ['collection', 414], ['IUCN conservation status', 420], ['drafted by', 440], ['industry', 467], ['discoverer or inventor', 473], ['based on', 496], ['located on astronomical body', 514], ['place of burial', 516], ['league', 538], ['found in taxon', 548], ['architect', 560], ['stock exchange', 562], ['designer', 570], ['occupant', 571], ['parent company', 586], ['crosses', 592], ['from fictional universe', 613], ['programming language', 622], ['start time', 634], ['residence', 667], ['material used', 684], ['end time', 697], ['distributor', 704], ['vessel class', 737], ['production company', 855], ['language of work or name', 920], ['point in time', 934], ['cause of death', 958], ['brother', 986], ['narrative location', 1017], ['lyrics by', 1037], ['connecting line', 1096], ['mother', 1234], ['child', 1300], ['instrument', 1335], ['manufacturer', 1377], ['mouth of the watercourse', 1492], ['named after', 1498], ['founder', 1559], ['dissolved or abolished', 1616], ['present in work', 1709], ['spouse', 1908], ['licensed to broadcast to', 2517], ['father', 2686], ['voice type', 2718], ['developer', 2942], ['military branch', 3020], ['conflict', 3053], ['creator', 3127], ['constellation', 3232], ['publisher', 3512], ['award received', 3806], ['original network', 4322], ['country of origin', 4546], ['record label', 4755], ['employer', 4873], ['educated at', 4910], ['continent', 4999], ['series', 5678], ['screenwriter', 6629], ['position held', 7254], ['native language', 7307], ['member of political party', 7425], ['sex or gender', 8946], ['participant of', 10287], ['cast member', 14424], ['headquarters location', 14965], ['member of sports team', 16395], ['director', 16990], ['publication date', 18215], ['author', 20104], ['position played on team / speciality', 20781], ['place of death', 23218], ['languages spoken or written', 25105], ['sport', 27706], ['inception', 29735], ['performer', 40137], ['parent taxon', 46210], ['place of birth', 51100], ['country of citizenship', 54830], ['date of death', 82778], ['occupation', 98026], ['date of birth', 121276], ['taxon rank', 151963], ['located in the administrative territorial entity', 218744], ['country', 223200]]\n",
      "\n",
      "\n",
      "\n",
      "{'medical condition': 0, 'architectural style': 1, 'editor': 2, 'nominated for': 3, 'replaced by': 4, 'head of government': 5, 'date of official opening': 6, 'military rank': 7, 'time of discovery': 8, 'home venue': 9, 'instrumentation': 10, 'chairperson': 11, 'operating system': 12, 'product': 13, 'chromosome': 14, 'site of astronomical discovery': 15, 'conferred by': 16, 'sister': 17, 'service entry': 18, 'religious order': 19, 'manner of death': 20, 'convicted of': 21, 'license': 22, 'noble title': 23, 'standards body': 24, 'located next to body of water': 25, 'airline hub': 26, 'film editor': 27, 'illustrator': 28, 'time of spacecraft launch': 29, 'characters': 30, 'canonization status': 31, 'noble family': 32, 'location of formation': 33, 'collection': 34, 'IUCN conservation status': 35, 'drafted by': 36, 'industry': 37, 'discoverer or inventor': 38, 'based on': 39, 'located on astronomical body': 40, 'place of burial': 41, 'league': 42, 'found in taxon': 43, 'architect': 44, 'stock exchange': 45, 'designer': 46, 'occupant': 47, 'parent company': 48, 'crosses': 49, 'from fictional universe': 50, 'programming language': 51, 'start time': 52, 'residence': 53, 'material used': 54, 'end time': 55, 'distributor': 56, 'vessel class': 57, 'production company': 58, 'language of work or name': 59, 'point in time': 60, 'cause of death': 61, 'brother': 62, 'narrative location': 63, 'lyrics by': 64, 'connecting line': 65, 'mother': 66, 'child': 67, 'instrument': 68, 'manufacturer': 69, 'mouth of the watercourse': 70, 'named after': 71, 'founder': 72, 'dissolved or abolished': 73, 'present in work': 74, 'spouse': 75, 'licensed to broadcast to': 76, 'father': 77, 'voice type': 78, 'developer': 79, 'military branch': 80, 'conflict': 81, 'creator': 82, 'constellation': 83, 'publisher': 84, 'award received': 85, 'original network': 86, 'country of origin': 87, 'record label': 88, 'employer': 89, 'educated at': 90, 'continent': 91, 'series': 92, 'screenwriter': 93, 'position held': 94, 'native language': 95, 'member of political party': 96, 'sex or gender': 97, 'participant of': 98, 'cast member': 99, 'headquarters location': 100, 'member of sports team': 101, 'director': 102, 'publication date': 103, 'author': 104, 'position played on team / speciality': 105, 'place of death': 106, 'languages spoken or written': 107, 'sport': 108, 'inception': 109, 'performer': 110, 'parent taxon': 111, 'place of birth': 112, 'country of citizenship': 113, 'date of death': 114, 'occupation': 115, 'date of birth': 116, 'taxon rank': 117, 'located in the administrative territorial entity': 118, 'country': 119}\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length = 60\n",
    "path_dir_capped_exclusive = join_path([path_ours, 'Levy_by_relation', 'max_tokenized_sentence_len_' + str(max_sentence_length) + '_exclusive_and_unique_relation'])\n",
    "\n",
    "\n",
    "idx_to_relation_and_len = []\n",
    "for relation in relation_list:\n",
    "    idx_to_relation_and_len.append([relation, file_len(join_path([path_dir_capped_exclusive, re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt']))])\n",
    "\n",
    "idx_to_relation_and_len.sort(key=lambda lst: lst[1])\n",
    "relation_to_idx_map = {relation: r_num for r_num, (relation, _) in enumerate(idx_to_relation_and_len)}\n",
    "print(\"Total number of unique and exclusive sentences:\", sum(lst[1] for lst in idx_to_relation_and_len))\n",
    "print()\n",
    "print(idx_to_relation_and_len)\n",
    "print('\\n\\n')\n",
    "print(relation_to_idx_map)\n",
    "\n",
    "\n",
    "masking_types = ['original', 'sub_obj_masking', 'NER_masking']\n",
    "\n",
    "original_data = {}\n",
    "repeated = {masking_type: {} for masking_type in masking_types}\n",
    "sentence_id = {masking_type: {} for masking_type in masking_types}\n",
    "\n",
    "for relation in relation_list:\n",
    "    for masking_type in masking_types:\n",
    "        if (masking_type == 'original'):\n",
    "            path = join_path([path_dir_capped_exclusive, re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt'])\n",
    "        else:\n",
    "            path = join_path([path_dir_capped_exclusive, masking_type, re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt'])\n",
    "\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for l_num, line in enumerate(f):\n",
    "                sentence = line.strip().split('\\t')[0] if masking_type == 'original' else line.strip()\n",
    "                sentence_id[masking_type][(relation, l_num)] = sentence\n",
    "                if (sentence not in repeated[masking_type]):\n",
    "                    repeated[masking_type][sentence] = [(relation, l_num)]\n",
    "                else:\n",
    "                    repeated[masking_type][sentence].append((relation, l_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of included sentences: 996361\n",
      "Number of excluded sentences: 475917\n",
      "[['medical condition', 103], ['architectural style', 110], ['editor', 96], ['nominated for', 111], ['replaced by', 106], ['head of government', 117], ['date of official opening', 124], ['military rank', 123], ['time of discovery', 128], ['home venue', 96], ['instrumentation', 143], ['chairperson', 177], ['operating system', 182], ['product', 167], ['chromosome', 190], ['site of astronomical discovery', 183], ['conferred by', 205], ['sister', 211], ['service entry', 216], ['religious order', 219], ['manner of death', 217], ['convicted of', 227], ['license', 214], ['noble title', 229], ['standards body', 254], ['located next to body of water', 257], ['airline hub', 286], ['film editor', 302], ['illustrator', 284], ['time of spacecraft launch', 210], ['characters', 282], ['canonization status', 362], ['noble family', 385], ['location of formation', 397], ['collection', 398], ['IUCN conservation status', 405], ['drafted by', 381], ['industry', 464], ['discoverer or inventor', 405], ['based on', 492], ['located on astronomical body', 439], ['place of burial', 458], ['league', 463], ['found in taxon', 547], ['architect', 556], ['stock exchange', 535], ['designer', 551], ['occupant', 562], ['parent company', 574], ['crosses', 583], ['from fictional universe', 498], ['programming language', 613], ['start time', 518], ['residence', 604], ['material used', 675], ['end time', 660], ['distributor', 684], ['vessel class', 434], ['production company', 833], ['language of work or name', 905], ['point in time', 857], ['cause of death', 924], ['brother', 980], ['narrative location', 1006], ['lyrics by', 933], ['connecting line', 651], ['mother', 1216], ['child', 1292], ['instrument', 1327], ['manufacturer', 1316], ['mouth of the watercourse', 1296], ['named after', 1488], ['founder', 1524], ['dissolved or abolished', 1564], ['present in work', 1207], ['spouse', 1880], ['licensed to broadcast to', 1921], ['father', 2628], ['voice type', 2365], ['developer', 2849], ['military branch', 2522], ['conflict', 2436], ['creator', 2918], ['constellation', 2704], ['publisher', 2996], ['award received', 2586], ['original network', 4129], ['country of origin', 4406], ['record label', 4459], ['employer', 4742], ['educated at', 4731], ['continent', 4961], ['series', 3315], ['screenwriter', 5849], ['position held', 5182], ['native language', 5724], ['member of political party', 4842], ['sex or gender', 5841], ['participant of', 5138], ['cast member', 11962], ['headquarters location', 14389], ['member of sports team', 11675], ['director', 11859], ['publication date', 13464], ['author', 15812], ['position played on team / speciality', 12932], ['place of death', 21272], ['languages spoken or written', 21881], ['sport', 16045], ['inception', 27742], ['performer', 30379], ['parent taxon', 20535], ['place of birth', 36940], ['country of citizenship', 44739], ['date of death', 67378], ['occupation', 70495], ['date of birth', 80006], ['taxon rank', 64870], ['located in the administrative territorial entity', 142330], ['country', 129801]]\n",
      "\n",
      "\n",
      "\n",
      "Total number of unique and exclusive sentences: 996361\n",
      "Unseen: 20896 ; Seen: 975465\n",
      "\n",
      "[['editor', 96], ['home venue', 96], ['medical condition', 103], ['replaced by', 106], ['architectural style', 110], ['nominated for', 111], ['head of government', 117], ['military rank', 123], ['date of official opening', 124], ['time of discovery', 128], ['instrumentation', 143], ['product', 167], ['chairperson', 177], ['operating system', 182], ['site of astronomical discovery', 183], ['chromosome', 190], ['conferred by', 205], ['time of spacecraft launch', 210], ['sister', 211], ['license', 214], ['service entry', 216], ['manner of death', 217], ['religious order', 219], ['convicted of', 227], ['noble title', 229], ['standards body', 254], ['located next to body of water', 257], ['characters', 282], ['illustrator', 284], ['airline hub', 286], ['film editor', 302], ['canonization status', 362], ['drafted by', 381], ['noble family', 385], ['location of formation', 397], ['collection', 398], ['IUCN conservation status', 405], ['discoverer or inventor', 405], ['vessel class', 434], ['located on astronomical body', 439], ['place of burial', 458], ['league', 463], ['industry', 464], ['based on', 492], ['from fictional universe', 498], ['start time', 518], ['stock exchange', 535], ['found in taxon', 547], ['designer', 551], ['architect', 556], ['occupant', 562], ['parent company', 574], ['crosses', 583], ['residence', 604], ['programming language', 613], ['connecting line', 651], ['end time', 660], ['material used', 675], ['distributor', 684], ['production company', 833], ['point in time', 857], ['language of work or name', 905], ['cause of death', 924], ['lyrics by', 933], ['brother', 980], ['narrative location', 1006], ['present in work', 1207], ['mother', 1216], ['child', 1292], ['mouth of the watercourse', 1296], ['manufacturer', 1316], ['instrument', 1327], ['named after', 1488], ['founder', 1524], ['dissolved or abolished', 1564], ['spouse', 1880], ['licensed to broadcast to', 1921], ['voice type', 2365], ['conflict', 2436], ['military branch', 2522], ['award received', 2586], ['father', 2628], ['constellation', 2704], ['developer', 2849], ['creator', 2918], ['publisher', 2996], ['series', 3315], ['original network', 4129], ['country of origin', 4406], ['record label', 4459], ['educated at', 4731], ['employer', 4742], ['member of political party', 4842], ['continent', 4961], ['participant of', 5138], ['position held', 5182], ['native language', 5724], ['sex or gender', 5841], ['screenwriter', 5849], ['member of sports team', 11675], ['director', 11859], ['cast member', 11962], ['position played on team / speciality', 12932], ['publication date', 13464], ['headquarters location', 14389], ['author', 15812], ['sport', 16045], ['parent taxon', 20535], ['place of death', 21272], ['languages spoken or written', 21881], ['inception', 27742], ['performer', 30379], ['place of birth', 36940], ['country of citizenship', 44739], ['taxon rank', 64870], ['date of death', 67378], ['occupation', 70495], ['date of birth', 80006], ['country', 129801], ['located in the administrative territorial entity', 142330]]\n"
     ]
    }
   ],
   "source": [
    "included_ids = set()\n",
    "excluded_ids = set()\n",
    "\n",
    "for original_sentence_id in sentence_id['original']:\n",
    "    if (original_sentence_id in excluded_ids):\n",
    "        continue\n",
    "\n",
    "    if (len(repeated['sub_obj_masking'][sentence_id['sub_obj_masking'][original_sentence_id]]) > 1 or\n",
    "        len(repeated['NER_masking'][sentence_id['NER_masking'][original_sentence_id]]) > 1):\n",
    "        overlap_set = set(repeated['sub_obj_masking'][sentence_id['sub_obj_masking'][original_sentence_id]])\n",
    "        overlap_set = overlap_set | set(repeated['NER_masking'][sentence_id['NER_masking'][original_sentence_id]])\n",
    "\n",
    "        min_relation_name = None\n",
    "        min_relation_instances = np.inf\n",
    "        for overlap_id in overlap_set:\n",
    "            if (idx_to_relation_and_len[relation_to_idx_map[overlap_id[0]]][1] < min_relation_instances):\n",
    "                min_relation_name = overlap_id[0]\n",
    "                min_relation_instances = idx_to_relation_and_len[relation_to_idx_map[overlap_id[0]]][1]\n",
    "\n",
    "        if (original_sentence_id[0] == min_relation_name):\n",
    "            included_ids.add(original_sentence_id)\n",
    "            for _id in overlap_set:\n",
    "                if (_id != original_sentence_id):\n",
    "                    if (_id in included_ids):\n",
    "                        raise RuntimeError('Adding invalid id to the exclusion set.')\n",
    "                    if (_id not in excluded_ids):\n",
    "                        idx_to_relation_and_len[relation_to_idx_map[_id[0]]][1] -= 1\n",
    "                    excluded_ids.add(_id)\n",
    "        else:\n",
    "            if (original_sentence_id not in excluded_ids):\n",
    "                idx_to_relation_and_len[relation_to_idx_map[original_sentence_id[0]]][1] -= 1\n",
    "            excluded_ids.add(original_sentence_id)\n",
    "    else:\n",
    "        included_ids.add(original_sentence_id)\n",
    "\n",
    "\n",
    "print(\"Number of included sentences:\", len(included_ids))\n",
    "print(\"Number of excluded sentences:\", len(excluded_ids))\n",
    "\n",
    "print(idx_to_relation_and_len)\n",
    "print('\\n\\n')\n",
    "idx_to_relation_and_len.sort(key=lambda lst: lst[1])\n",
    "print(\"Total number of unique and exclusive sentences:\", sum(lst[1] for lst in idx_to_relation_and_len))\n",
    "print(\"Unseen:\", sum(counts for _, counts in idx_to_relation_and_len[:60]), \"; Seen:\", sum(counts for _, counts in idx_to_relation_and_len[60:]))\n",
    "print()\n",
    "print(idx_to_relation_and_len)\n",
    "\n",
    "\n",
    "# Save the unique sentences.\n",
    "max_sentence_length = 60\n",
    "path_dir_capped_exclusive_accounting_masking = join_path([path_ours, 'Levy_by_relation',\n",
    "                                                          'max_tokenized_sentence_len_' + str(max_sentence_length) + '_exclusive_and_unique_relation_masking'])\n",
    "\n",
    "paths = {}\n",
    "\n",
    "for masking_type in masking_types:\n",
    "    paths[masking_type] = {}\n",
    "\n",
    "    masking_path = join_path([path_dir_capped_exclusive_accounting_masking, masking_type]) + os.sep\n",
    "\n",
    "    directory = os.path.dirname(masking_path)\n",
    "    if (not os.path.exists(directory)):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    for relation in relation_list:\n",
    "        paths[masking_type][relation] = open(join_path([masking_path, re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt']), 'w', encoding='utf-8')\n",
    "\n",
    "try:\n",
    "    for relation in relation_list:\n",
    "        for masking_type in masking_types:\n",
    "            if (masking_type == 'original'):\n",
    "                path = join_path([path_dir_capped_exclusive, re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt'])\n",
    "            else:\n",
    "                path = join_path([path_dir_capped_exclusive, masking_type, re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt'])\n",
    "\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                for l_num, line in enumerate(f):\n",
    "                    if ((relation, l_num) in included_ids):\n",
    "                        paths[masking_type][relation].write(line.strip() + '\\n')\n",
    "        \n",
    "except:\n",
    "    for masking_type in masking_types:\n",
    "        for relation in relation_list:\n",
    "            paths[masking_type][relation].close()\n",
    "\n",
    "for masking_type in masking_types:\n",
    "    for relation in relation_list:\n",
    "        paths[masking_type][relation].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we actually create the datasets (with splits of train, validation and test), for the different settings and folds.\n",
    "\n",
    "The settings in question are:\n",
    "\n",
    "- Normal setting (N):\n",
    "    - Draw instances according to the data distribution, but guaranteeing at least one instance per relation.\n",
    "\n",
    "\n",
    "- Zero-Shot setting (ZS):\n",
    "    - Classes are disjoint across the different splits. For each split we follow the data distribution, with the particularity that for the train split we guarantee at least 20 instances of each of the training classes (to deem them as 'seen' classes instead of 'unseen' and make the splits comparable to the Few-Shot scenario).\n",
    "\n",
    "\n",
    "- Generalised Zero-Shot setting (GZS):\n",
    "    - Similar to Zero-Shot, except train classes are also present in the validation and test splits (since in a real life scenario one would test against all existing classes). For the eval splits (val and test), 50% of the training instances are assigned to the seen classes (and then we follow the data distribution of those classes, guaranteeing at least 5 instances per class) and the other 50% are assigned to the unseen classes (and then we follow the data distribution of those classes, guaranteeing at least 5 instances per class).\n",
    "\n",
    "\n",
    "- Few-Shot settings (with 1, 2, 5 and 10 shots) (FS-n): \n",
    "    - Similar to Zero-Shot, except now the train splits also have number-of-shots (1, 2, 5 or 10) instances of each class present in the eval splits. For the train splits we first guarantee the 'unseen' classes instances and, then, for the remaining missing number of instances we follow the data distribution of the 'seen' classes, with the particularity that we guarantee at least 20 instances of each of the training classes (to deem them as 'seen' classes instead of 'unseen').\n",
    "\n",
    "\n",
    "- Generalised Few-Shot settings (with 1, 2, 5 and 10 shots) (GFS-n): \n",
    "    - Similar to Few-Shot, except train classes are also present in the validation and test splits (since in a real life scenario one would test against all existing classes). For the eval splits (val and test), 50% of the training instances are assigned to the 'seen' classes (and then we follow the data distribution of those classes, guaranteeing at least 5 instances per class) and the other 50% are assigned to the 'unseen' classes (and then we follow the data distribution of those classes, guaranteeing at least 5 instances per class).\n",
    "\n",
    "\n",
    "---------------------\n",
    "---------------------\n",
    "---------------------\n",
    "\n",
    "The datasets to be created are (where we define 'seen' classes as the 60 most populated classes and 'unseen' classes as the remainder of the classes):\n",
    "\n",
    "- Final Evaluation ('final'):\n",
    "    - This is the dataset where we test the final performance of our model. Some characteristics are as following:\n",
    "        - This dataset type contains both 'seen' and 'unseen' classes (i.e. all classes. See the next dataset to understand why we make it explicit).\n",
    "        - Each setting of this dataset contains 10 folds (each with train, validation and test splits) (so, 10 folds for N, 10 folds for ZS, 10 folds for GZS, ...).\n",
    "        - There is no guarantee that the data is disjoint across distinct settings (see the next dataset to understand why we make it explicit).\n",
    "        - For each fold we guarantee that the train, validation and test splits are disjoint from each other.\n",
    "        - Across folds the train and validation splits are disjoint from one another.\n",
    "        - We also guarantee that the validation and test splits, of all folds and settings, are completely disjoint from any data contained in the next dataset type ('hyperparameter'), which is used for Hyper-Parameter Tuning.\n",
    "        - Finally, for each fold the splits are as follows, where BOTH 'seen' and 'unseen' classes are considered (each split is formed according to the specifications of the specific setting: read above):\n",
    "\n",
    "\n",
    "            For settings other than the Normal setting:\n",
    "              - 50 classes are sampled from the 'seen' set. These will be the train classes (disregarding specific setting configurations).\n",
    "              - For both the validation and test splits we sample 20 classes from the 'unseen' set (20 for each split and assuring the classes in\n",
    "                the validation split are disjoint from the classes in the test split). These will be the validation/test classes (disregarding\n",
    "                specific setting configurations).\n",
    "   \n",
    "   \n",
    "            - Train - 100.000 instances - Sampled according to the specifications of the specific setting: read above.\n",
    "            - Val   - 5.000   instances - Sampled according to the specifications of the specific setting: read above.\n",
    "            - test  - 25.000  instances - Sampled according to the specifications of the specific setting: read above.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Hyper-Parameter Tuning ('hyperparameter'):\n",
    "   - This is the dataset where we optimise the hyperparameters of the models. Some ablation experiments can also be performed here:\n",
    "        - This dataset type contains only 'seen' classes. This is essential as we want to make sure that for the final evaluation the models will have never seen the 'unseen' classes, thus rendering it a true (Generalised) Any-Shot setting.\n",
    "        - Each setting of this dataset contains 10 folds (each with train, validation and test splits) (so, 10 folds for N, 10 folds for ZS, 10 folds for GZS, ...).\n",
    "        - We guarantee that the data is disjoint across distinct settings. This allows to select the hyperparameters that perform the best regardless of the setting. For our case this makes particular sense, as the whole idea revolves around a Natural Language Understanding/Inference paradigm. Therefore, if you perform well in one setting you should also perform well in a different one.\n",
    "        - For each fold we guarantee that the train, validation and test splits are disjoint from each other.\n",
    "        - Across folds ??? all splits are disjoint from one another? the train and validation splits are disjoint from one another?\n",
    "        - Finally, for each fold the splits are as follows (each split is formed according to the specifications of the specific setting: read above):\n",
    "\n",
    "\n",
    "            For settings other than the Normal setting:\n",
    "              - 25 classes are sampled from the 'seen' set. These will be the train classes (disregarding specific setting configurations).\n",
    "              - For both the validation and test splits we sample 10 classes from the 'seen' set. These will be the validation/test classes (disregarding\n",
    "                specific setting configurations).\n",
    "              - We guarantee that the classes sampled for any split are disjoint from the classes selected for any other split.\n",
    "   \n",
    "   \n",
    "            - Train - 10.000 instances - Sampled according to the specifications of the specific setting: read above.\n",
    "            - Val   - 500    instances - Sampled according to the specifications of the specific setting: read above.\n",
    "            - test  - 2.500  instances - Sampled according to the specifications of the specific setting: read above.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Debugging ('DEBUG'):\n",
    "    - This is the dataset used to quickly debug models and training procedures:\n",
    "        - This dataset type contains only 'seen' classes.\n",
    "        - Regarding the (Generalised) Few-Shot settings, this dataset contains only 1-shot settings.\n",
    "        - Each setting of this dataset contains 2 folds (each with train, validation and test splits) (so, 2 folds for N, 2 folds for ZS, 2 folds for GZS, ...).\n",
    "        - We guarantee that the data is disjoint across distinct settings.\n",
    "        - For each fold we guarantee that the train, validation and test splits are disjoint from each other.\n",
    "        - Across folds all splits are disjoint from one another.\n",
    "        - Finally, for each fold the splits are as follows (each split is formed according to the specifications of the specific setting: read above):\n",
    "\n",
    "\n",
    "            For settings other than the Normal setting:\n",
    "              - 5 classes are sampled from the 'seen' set. These will be the train classes (disregarding specific setting configurations).\n",
    "              - For both the validation and test splits we sample 2 classes from the 'seen' set. These will be the validation/test classes (disregarding\n",
    "                specific setting configurations).\n",
    "              - We guarantee that the classes sampled for any split are disjoint from the classes selected for any other split.\n",
    "   \n",
    "   \n",
    "            - Train - 20 instances - Sampled according to the specifications of the specific setting: read above.\n",
    "            - Val   - 10 instances - Sampled according to the specifications of the specific setting: read above.\n",
    "            - test  - 10 instances - Sampled according to the specifications of the specific setting: read above.\n",
    "\n",
    "\n",
    "| Heading | Heading |\n",
    "| ----| ----|\n",
    "| text   | text |\n",
    "| text | text |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining a data sampler that guarantees a minimum number of instances.\n",
    "def get_num_samples_and_guarantee_min_count(probs, num_samples, min_count, balanced=False):\n",
    "    assert num_samples >= probs.shape[0]*min_count, \"It's impossible to guarantee a minimum sample count for this total amount of samples.\"\n",
    "    probs = probs.astype(float)\n",
    "\n",
    "#     if (balanced):\n",
    "#         counts = np.zeros(probs.shape) + num_samples // probs.shape[0]\n",
    "#         # In case 'num_samples % probs.shape[0] != 0' is True, that is, we cannot get the same number of samples on all classes, we\n",
    "#         # remove elements, at random, from the classes that have the most elements, making it as balanced as possible.\n",
    "#         while (counts.sum() != num_samples):\n",
    "#             counts[np.random.choice(numpy.where(counts == counts.max())[0])] -= 1\n",
    "\n",
    "#     else:\n",
    "    probs /= probs.sum()\n",
    "\n",
    "    counts = np.zeros(probs.shape)\n",
    "\n",
    "    num_missing_below_threshold = (min_count - counts[counts < min_count]).sum()\n",
    "    while (num_missing_below_threshold > 0 or counts.sum() != num_samples):\n",
    "        if (num_samples - counts.sum() - num_missing_below_threshold == 0):\n",
    "            counts[counts < min_count] = min_count\n",
    "        else:\n",
    "            counts += np.random.multinomial(num_samples - counts.sum() - num_missing_below_threshold, probs)\n",
    "        num_missing_below_threshold = (min_count - counts[counts < min_count]).sum()\n",
    "\n",
    "    # Assertions are redundant with while loop. Just a sanity check. REMOVE eventually?\n",
    "    assert (counts < min_count).sum() == 0, 'YO - 0'\n",
    "    assert counts.sum() == num_samples, 'YO - 1'\n",
    "    return counts.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings to be evaluated: {'DEBUG': ['N', 'ZS', 'GZS', 'FS-1', 'GFS-1'], 'HT': ['N', 'ZS', 'GZS', 'FS-1', 'GFS-1', 'FS-2', 'GFS-2', 'FS-5', 'GFS-5', 'FS-10', 'GFS-10'], 'final': ['N', 'ZS', 'GZS', 'FS-1', 'GFS-1', 'FS-2', 'GFS-2', 'FS-5', 'GFS-5', 'FS-10', 'GFS-10']}\n",
      "\n",
      "Total number of instances: 996361\n",
      "\n",
      "Distribution of data instances per class:\n",
      "\n",
      "'seen' classes (Total number of instances: 970866):\n",
      " [['located in the administrative territorial entity', 142330], ['country', 129801], ['date of birth', 80006], ['occupation', 70495], ['date of death', 67378], ['taxon rank', 64870], ['country of citizenship', 44739], ['place of birth', 36940], ['performer', 30379], ['inception', 27742], ['languages spoken or written', 21881], ['place of death', 21272], ['parent taxon', 20535], ['sport', 16045], ['author', 15812], ['headquarters location', 14389], ['publication date', 13464], ['position played on team / speciality', 12932], ['cast member', 11962], ['director', 11859], ['member of sports team', 11675], ['screenwriter', 5849], ['sex or gender', 5841], ['native language', 5724], ['position held', 5182], ['participant of', 5138], ['continent', 4961], ['member of political party', 4842], ['employer', 4742], ['educated at', 4731], ['record label', 4459], ['country of origin', 4406], ['original network', 4129], ['series', 3315], ['publisher', 2996], ['creator', 2918], ['developer', 2849], ['constellation', 2704], ['father', 2628], ['award received', 2586], ['military branch', 2522], ['conflict', 2436], ['voice type', 2365], ['licensed to broadcast to', 1921], ['spouse', 1880], ['dissolved or abolished', 1564], ['founder', 1524], ['named after', 1488], ['instrument', 1327], ['manufacturer', 1316], ['mouth of the watercourse', 1296], ['child', 1292], ['mother', 1216], ['present in work', 1207], ['narrative location', 1006]]\n",
      "\n",
      "\n",
      "'unseen' classes (Total number of instances: 25495):\n",
      " [['brother', 980], ['lyrics by', 933], ['cause of death', 924], ['language of work or name', 905], ['point in time', 857], ['production company', 833], ['distributor', 684], ['material used', 675], ['end time', 660], ['connecting line', 651], ['programming language', 613], ['residence', 604], ['crosses', 583], ['parent company', 574], ['occupant', 562], ['architect', 556], ['designer', 551], ['found in taxon', 547], ['stock exchange', 535], ['start time', 518], ['from fictional universe', 498], ['based on', 492], ['industry', 464], ['league', 463], ['place of burial', 458], ['located on astronomical body', 439], ['vessel class', 434], ['IUCN conservation status', 405], ['discoverer or inventor', 405], ['collection', 398], ['location of formation', 397], ['noble family', 385], ['drafted by', 381], ['canonization status', 362], ['film editor', 302], ['airline hub', 286], ['illustrator', 284], ['characters', 282], ['located next to body of water', 257], ['standards body', 254], ['noble title', 229], ['convicted of', 227], ['religious order', 219], ['manner of death', 217], ['service entry', 216], ['license', 214], ['sister', 211], ['time of spacecraft launch', 210], ['conferred by', 205], ['chromosome', 190], ['site of astronomical discovery', 183], ['operating system', 182], ['chairperson', 177], ['product', 167], ['instrumentation', 143], ['time of discovery', 128], ['date of official opening', 124], ['military rank', 123], ['head of government', 117], ['nominated for', 111], ['architectural style', 110], ['replaced by', 106], ['medical condition', 103], ['editor', 96], ['home venue', 96]]\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "# Define some dataset creation hyperparameters. #\n",
    "#################################################\n",
    "proposed_splits_meta_data = {}\n",
    "\n",
    "# Specify the maximum sentence length that is to be used.\n",
    "max_sentence_length = 60\n",
    "proposed_splits_meta_data['max_sentence_length'] = 60\n",
    "\n",
    "\n",
    "# Define the dataset types to be created.\n",
    "dataset_types = ['DEBUG', 'HT', 'final']\n",
    "proposed_splits_meta_data['dataset_types'] = dataset_types\n",
    "\n",
    "\n",
    "# Define the number of 'seen' classes. This will select the 'num_seen_classes' most populated classes as 'seen' classes.\n",
    "num_seen_classes = 55\n",
    "proposed_splits_meta_data['num_seen_classes'] = num_seen_classes\n",
    "\n",
    "\n",
    "# Define the number of classes per split, per dataset type. This is only used in settings other than the normal one.\n",
    "# These will be sampled from the entire set of corresponding classes (as per the setting).\n",
    "num_classes = {'DEBUG': {'train': 5,  'val': 2,  'test': 2},\n",
    "               'HT'   : {'train': 25, 'val': 10, 'test': 10},\n",
    "               'final': {'train': 50, 'val': 20, 'test': 20}}\n",
    "proposed_splits_meta_data['num_classes'] = num_classes\n",
    "\n",
    "\n",
    "# Define the settings to be created.\n",
    "settings = ['N', 'ZS', 'GZS']\n",
    "num_shots = [1, 2, 5, 10] # This indicates the number of shots for the specific dataset type.\n",
    "fs = lambda x: 'FS-' + str(x)\n",
    "gfs = lambda x: 'GFS-' + str(x)\n",
    "settings += [f(num_shot) for num_shot in num_shots for f in (fs, gfs)]\n",
    "settings = {'DEBUG': settings[:5], 'HT': settings, 'final': settings}\n",
    "proposed_splits_meta_data['settings'] = settings\n",
    "print(\"Settings to be evaluated:\", settings)\n",
    "\n",
    "\n",
    "# Define the number of folds, per dataset type.\n",
    "num_folds = {'DEBUG': 2, 'HT': 5, 'final': 10}\n",
    "proposed_splits_meta_data['num_folds'] = num_folds\n",
    "\n",
    "# Define the size of each split, per dataset type.\n",
    "# split_size = {'DEBUG': {'train': 20,    'val': 10,  'test': 10},\n",
    "#               'HT'   : {'train': 10000, 'val': 500, 'test': 750},\n",
    "#               'final': {'train': 50000, 'val': 750, 'test': 1000}}\n",
    "split_size = {'DEBUG': {'train': 20,    'val': 10,  'test': 10},\n",
    "              'HT'   : {'train': 10000, 'val': 500, 'test': 1000},\n",
    "              'final': {'train': 50000, 'val': 625, 'test': 1250}}\n",
    "proposed_splits_meta_data['split_size'] = split_size\n",
    "\n",
    "splits = {'train': 0, 'val': 1, 'test': 2}\n",
    "proposed_splits_meta_data['splits'] = splits\n",
    "\n",
    "\n",
    "test_only_in_other_tests = True\n",
    "HT_completely_disjoint = True\n",
    "proposed_splits_meta_data['test_only_in_other_tests'] = test_only_in_other_tests\n",
    "proposed_splits_meta_data['HT_completely_disjoint'] = HT_completely_disjoint\n",
    "\n",
    "uniform_class_max_diff = 1\n",
    "uniform_class_max_diff_HT = 1\n",
    "class_min_num_times_in_setting_split = 2\n",
    "class_min_num_times_in_setting_split_HT = 2\n",
    "proposed_splits_meta_data['uniform_class_max_diff'] = uniform_class_max_diff\n",
    "proposed_splits_meta_data['class_min_num_times_in_setting_split'] = class_min_num_times_in_setting_split\n",
    "proposed_splits_meta_data['uniform_class_max_diff_HT'] = uniform_class_max_diff_HT\n",
    "proposed_splits_meta_data['class_min_num_times_in_setting_split_HT'] = class_min_num_times_in_setting_split_HT\n",
    "\n",
    "min_count_train_DEBUG = 1\n",
    "min_count_eval_DEBUG = 1\n",
    "min_count_train_final_HT = 25\n",
    "min_count_eval_final_HT = 5\n",
    "proposed_splits_meta_data['min_count_train_DEBUG'] = min_count_train_DEBUG\n",
    "proposed_splits_meta_data['min_count_eval_DEBUG'] = min_count_eval_DEBUG\n",
    "proposed_splits_meta_data['min_count_train_final_HT'] = min_count_train_final_HT\n",
    "proposed_splits_meta_data['min_count_eval_final_HT'] = min_count_eval_final_HT\n",
    "\n",
    "\n",
    "\n",
    "###########################\n",
    "# Collect some statistics #\n",
    "###########################\n",
    "\n",
    "# Start by specifying the path to the data.\n",
    "path_dir_capped_exclusive_accounting_masking = join_path([path_ours, 'Levy_by_relation',\n",
    "                                                          'max_tokenized_sentence_len_' + str(max_sentence_length) + '_exclusive_and_unique_relation_masking',\n",
    "                                                          'original'])\n",
    "# path_data_capped_length_exclusive = path_ours + 'Levy_by_relation/max_tokenized_sentence_len_' + str(max_sentence_length) + '_exclusive_and_unique_relation/'\n",
    "\n",
    "# Then get the data distribution, sort from fewest number of instances to highest number of instances.\n",
    "idx_to_class_and_len = []\n",
    "\n",
    "for c_num, _class in enumerate(relation_list):\n",
    "#     path_to_class_data = path_data_capped_length_exclusive + re.sub('/', '-', re.sub(' ', '_', _class)) + '.txt'\n",
    "    path_to_class_data = join_path([path_dir_capped_exclusive_accounting_masking, re.sub('/', '-', re.sub(' ', '_', _class)) + '.txt'])\n",
    "    idx_to_class_and_len.append([_class, file_len(path_to_class_data)])\n",
    "\n",
    "\n",
    "idx_to_class_and_len.sort(key=lambda lst: lst[1], reverse=True)\n",
    "class_to_idx = {_class: idx for idx, (_class, _) in enumerate(idx_to_class_and_len)}\n",
    "proposed_splits_meta_data['idx_to_class_and_len'] = idx_to_class_and_len\n",
    "proposed_splits_meta_data['class_to_idx'] = class_to_idx\n",
    "\n",
    "classes_as_idx = list(range(len(class_to_idx)))\n",
    "classes_num_insts = np.array([num_insts for (_, num_insts) in idx_to_class_and_len])\n",
    "\n",
    "seen_classes = idx_to_class_and_len[:num_seen_classes]\n",
    "seen_classes_to_seen_idx = {_class: idx for idx, (_class, _) in enumerate(seen_classes)}\n",
    "seen_classes_as_idx = list(range(0, len(seen_classes_to_seen_idx)))\n",
    "\n",
    "unseen_classes = idx_to_class_and_len[num_seen_classes:]\n",
    "unseen_classes_to_unseen_idx = {_class: idx for idx, (_class, _) in enumerate(unseen_classes)}\n",
    "unseen_classes_as_idx = list(range(len(seen_classes_to_seen_idx), len(class_to_idx)))\n",
    "\n",
    "# We compute the probability of the unseen classes, as sampling for FS-10 from a uniform class distribution makes it extremely hard to achieve a solution.\n",
    "np_unseen_classes_lens = np.array([idx_to_class_and_len[idx][1] for idx in unseen_classes_as_idx], dtype=np.int64)\n",
    "\n",
    "unseen_classes_probs = np_unseen_classes_lens**(12/9)/np.sum(np_unseen_classes_lens**(12/9))\n",
    "\n",
    "\n",
    "\n",
    "print('\\nTotal number of instances:', sum(lst[1] for lst in idx_to_class_and_len))\n",
    "print('\\nDistribution of data instances per class:')\n",
    "print()\n",
    "print(\"'seen' classes (Total number of instances: \" + str(sum(lst[1] for lst in seen_classes)) + '):\\n', seen_classes)\n",
    "print(\"\\n\\n'unseen' classes (Total number of instances: \" + str(sum(lst[1] for lst in unseen_classes)) + '):\\n', unseen_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x792 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pltt\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for c_num, (_, count) in enumerate(idx_to_class_and_len):\n",
    "    x.append(c_num)\n",
    "    y.append(count)\n",
    "\n",
    "y.sort(reverse=True)\n",
    "fig = pltt.figure(figsize=(20,11))\n",
    "ax = fig.add_subplot(111)\n",
    "pltt.bar(range(1, len(x)+1), height=y)\n",
    "avg = np.mean(np.array(y))\n",
    "shader_mean = np.array([avg]*120)\n",
    "stdev = np.std(np.array(y))\n",
    "shader_stdev = np.array([stdev]*120)\n",
    "ax.annotate(r\"{:.0f} $\\pm$ {:.0f}\".format(avg, stdev), xy=(103, 10000), fontsize=20.0)\n",
    "pltt.hlines(avg, 0.5, 120.25, colors='r', linestyles='solid', label='avg', linewidth=4.0)\n",
    "pltt.fill_between(np.linspace(0.5, 120.25, 120), np.maximum(0, shader_mean-shader_stdev), shader_mean+shader_stdev, color=\"r\", alpha=0.2)\n",
    "\n",
    "x1,x2,y1,y2 = pltt.axis()\n",
    "pltt.axis((x1,x2,-1000,y2))\n",
    "\n",
    "#set parameters for tick labels\n",
    "pltt.tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=True, labelsize=23)\n",
    "pltt.tick_params(axis='y', which='major', labelsize=23)\n",
    "pltt.ylabel('Number of Instances', fontsize=30)\n",
    "pltt.xlabel('Class Id (Increasing Rarity)', fontsize=30)\n",
    "\n",
    "\n",
    "pltt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DISJOINTNESS TYPE: NOT in other folds' train/val splits.\n",
      "\n",
      "\n",
      "Found solution for (DEBUG) after 1 iterations.\n",
      "\n",
      "Dataset: HT | Maximum Verified Settings: ['N', 'ZS', 'GZS', 'FS-1', 'GFS-1', 'FS-2', 'GFS-2', 'FS-5', 'GFS-5', 'FS-10', 'GFS-10'] (11)\n",
      "Found solution for (HT) after 5 iterations.\n",
      "\n",
      "Found solution for (final, N) after 1134 iterations.\n",
      "Found solution for (final, ZS) after 56 iterations.\n",
      "Found solution for (final, GZS) after 26 iterations.\n",
      "Found solution for (final, FS-1) after 32 iterations.\n",
      "Found solution for (final, GFS-1) after 93 iterations.\n",
      "Found solution for (final, FS-2) after 4 iterations.\n",
      "Found solution for (final, GFS-2) after 43 iterations.\n",
      "Found solution for (final, FS-5) after 13 iterations.\n",
      "Found solution for (final, GFS-5) after 4 iterations.\n",
      "Found solution for (final, FS-10) after 8892 iterations.\n",
      "Found solution for (final, GFS-10) after 42 iterations.\n",
      "\n",
      "Found solution for (final) after 1 iterations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We create the matrices that will allows us to count the number of instances.\n",
    "num_instances_check = {dataset_type: {} for dataset_type in dataset_types}\n",
    "\n",
    "classes_in_split = {dataset_type: {setting: {fold: {} for fold in range(num_folds[dataset_type])}\n",
    "                                   for setting in settings[dataset_type]}\n",
    "                    for dataset_type in dataset_types}\n",
    "\n",
    "found_global_solution = False\n",
    "num_tries_global_solution = 1\n",
    "\n",
    "\n",
    "while (not found_global_solution):\n",
    "    try:\n",
    "        clear_output(True)\n",
    "        print(\"TEST DISJOINTNESS TYPE:\", \"NOT in other folds' train/val splits.\" if test_only_in_other_tests else \"in other folds' train/val splits.\")\n",
    "        print()\n",
    "\n",
    "        for dataset_type in dataset_types:\n",
    "            if (dataset_type == 'DEBUG'):\n",
    "                seen_classes_debug = random.sample(seen_classes_as_idx, sum(num_classes[dataset_type][split] for split in splits))\n",
    "                min_count_train = min_count_train_DEBUG\n",
    "                min_count_eval  = min_count_eval_DEBUG\n",
    "            else:\n",
    "                min_count_train = min_count_train_final_HT\n",
    "                min_count_eval  = min_count_eval_final_HT\n",
    "                \n",
    "            found_dataset_type_solution = False\n",
    "            num_tries_dataset_type_solution = 1\n",
    "            max_max_num_settings_verified = 0\n",
    "\n",
    "            while (not found_dataset_type_solution):\n",
    "                try:\n",
    "                    if (dataset_type == 'HT'):\n",
    "                        if (HT_completely_disjoint):\n",
    "                            disjoint_HT = np.zeros(len(idx_to_class_and_len), dtype=np.int64)\n",
    "                        else:\n",
    "                            disjoint_train_val_HT = np.zeros(len(idx_to_class_and_len), dtype=np.int64)\n",
    "                            if (test_only_in_other_tests):\n",
    "                                test_HT_maximums = np.zeros(len(idx_to_class_and_len), dtype=np.int64)\n",
    "                    maximum_number_of_settings_verified = 0\n",
    "    \n",
    "                    for setting_num, setting in enumerate(settings[dataset_type]):\n",
    "#                         print(\"SETTING AT FOR LEVEL:\", setting)\n",
    "                        found_setting_solution = False\n",
    "                        num_tries_setting_solution = 1\n",
    "                        num_seen_classes_fails = 0\n",
    "                        num_unseen_classes_fails = 0\n",
    "                        max_verified_classes_so_far = 0\n",
    "\n",
    "                        while (not found_setting_solution):\n",
    "                            try:\n",
    "                                if (dataset_type != 'DEBUG'):\n",
    "                                    if (dataset_type == 'HT' and HT_completely_disjoint):\n",
    "                                        disjoint_HT_setting = np.zeros(len(idx_to_class_and_len), dtype=np.int64)\n",
    "                                    else:\n",
    "                                        disjoint_train_setting = np.zeros(len(idx_to_class_and_len), dtype=np.int64)\n",
    "                                        disjoint_val_setting   = np.zeros(len(idx_to_class_and_len), dtype=np.int64)\n",
    "                                        if (test_only_in_other_tests):\n",
    "                                            test_setting_maximums = np.zeros(len(idx_to_class_and_len), dtype=np.int64)\n",
    "\n",
    "                                num_instances_check[dataset_type][setting] = np.zeros((num_folds[dataset_type], 3, len(idx_to_class_and_len)), dtype=np.int64)\n",
    "\n",
    "                                # Start by sampling the classes that will be present in this fold.\n",
    "                                if (dataset_type != 'DEBUG'):\n",
    "                                    train_samples = np.zeros(num_seen_classes, dtype=np.int64)\n",
    "                                    if (dataset_type == 'HT'):\n",
    "                                        val_samples   = np.zeros(num_seen_classes, dtype=np.int64)\n",
    "                                        test_samples  = np.zeros(num_seen_classes, dtype=np.int64)\n",
    "                                    else:\n",
    "                                        val_samples   = np.zeros(len(classes_as_idx) - num_seen_classes, dtype=np.int64)\n",
    "                                        test_samples  = np.zeros(len(classes_as_idx) - num_seen_classes, dtype=np.int64)\n",
    "\n",
    "                                for fold in range(num_folds[dataset_type]):\n",
    "                                    if (setting == 'N'):\n",
    "                                        for split in splits:\n",
    "                                            if (dataset_type == 'DEBUG'):\n",
    "                                                classes_in_split[dataset_type][setting][fold][split] = list(seen_classes_debug)\n",
    "                                            elif (dataset_type == 'HT'):\n",
    "                                                classes_in_split[dataset_type][setting][fold][split] = list(seen_classes_as_idx)\n",
    "                                            else:\n",
    "                                                classes_in_split[dataset_type][setting][fold][split] = list(classes_as_idx)\n",
    "                                    else:\n",
    "                                        if (dataset_type == 'DEBUG'):\n",
    "                                            sampled_classes = random.sample(seen_classes_debug, sum(num_classes[dataset_type][split] for split in splits))\n",
    "\n",
    "                                            train_classes = sampled_classes[:num_classes[dataset_type]['train']]\n",
    "                                            val_classes   = sampled_classes[num_classes[dataset_type]['train']:num_classes[dataset_type]['train']+num_classes[dataset_type]['val']]\n",
    "                                            test_classes  = sampled_classes[-num_classes[dataset_type]['test']:]\n",
    "                                            \n",
    "                                        elif (dataset_type == 'HT'):\n",
    "                                            train_sampled_classes = np.random.choice(np.array(seen_classes_as_idx), size=num_classes[dataset_type]['train'],\n",
    "                                                                                     replace=False, p=softmax(train_samples))\n",
    "                                            train_samples[train_sampled_classes] -= 50\n",
    "                                            train_classes = list(train_sampled_classes)\n",
    "\n",
    "                                            val_probs = softmax(np.array([val_sample for i, val_sample in enumerate(val_samples) if i not in train_sampled_classes]))\n",
    "                                            val_sampled_classes = np.random.choice(np.array([idx for idx in seen_classes_as_idx if idx not in train_sampled_classes]),\n",
    "                                                                                   size=num_classes[dataset_type]['val'], replace=False, p=val_probs)\n",
    "                                            val_samples[val_sampled_classes] -= 50\n",
    "                                            val_classes = list(val_sampled_classes)\n",
    "\n",
    "                                            test_probs = softmax(np.array([test_sample for i, test_sample in enumerate(test_samples) if i not in train_sampled_classes and\n",
    "                                                                                                                                        i not in val_sampled_classes]))\n",
    "                                            test_sampled_classes = np.random.choice(np.array([idx for idx in seen_classes_as_idx if idx not in train_sampled_classes and\n",
    "                                                                                                                                    idx not in val_sampled_classes]),\n",
    "                                                                                    size=num_classes[dataset_type]['test'], replace=False, p=test_probs)\n",
    "                                            test_samples[test_sampled_classes] -= 50\n",
    "                                            test_classes = list(test_sampled_classes)\n",
    "\n",
    "                                        else:\n",
    "                                            samples = np.random.choice(np.array(seen_classes_as_idx), size=num_classes[dataset_type]['train'],\n",
    "                                                                       replace=False, p=softmax(train_samples))\n",
    "                                            train_samples[samples] -= 25\n",
    "                                            train_classes = list(samples)\n",
    "\n",
    "                                            # We bias FS-10 class sampling slightly, while still ensuring the imposed constraints, just so\n",
    "                                            # it is feasible to easily sample a data distribution for the splits.\n",
    "                                            if (setting == 'FS-10'):\n",
    "                                                val_probs = val_samples + 8*fold*unseen_classes_probs\n",
    "                                                val_probs = softmax(val_probs)\n",
    "                                            else:\n",
    "                                                val_probs = softmax(val_samples)\n",
    "                                            samples = np.random.choice(np.array(unseen_classes_as_idx)-num_seen_classes, size=num_classes[dataset_type]['val'],\n",
    "                                                                       replace=False, p=val_probs)\n",
    "                                            val_samples[samples] -= 20\n",
    "                                            val_classes = list(samples + num_seen_classes)\n",
    "\n",
    "                                            # We bias FS-10 class sampling slightly, while still ensuring the imposed constraints, just so\n",
    "                                            # it is feasible to easily sample a data distribution for the splits.\n",
    "                                            if (setting == 'FS-10'):\n",
    "                                                test_probs = np.array([test_sample for i, test_sample in enumerate(test_samples + 8*fold*unseen_classes_probs) if i not in samples])\n",
    "                                                test_probs = softmax(test_probs)\n",
    "                                            else:\n",
    "                                                test_probs = softmax(np.array([test_sample for i, test_sample in enumerate(test_samples) if i not in samples]))\n",
    "                                            samples = np.random.choice(np.array([idx for idx in unseen_classes_as_idx if idx not in (samples + num_seen_classes)])-num_seen_classes,\n",
    "                                                                       size=num_classes[dataset_type]['test'],\n",
    "                                                                       replace=False,\n",
    "                                                                       p=test_probs)\n",
    "                                            test_samples[samples] -= 20\n",
    "                                            test_classes = list(samples + num_seen_classes)\n",
    "\n",
    "                                        classes_in_split[dataset_type][setting][fold]['train'] = train_classes\n",
    "                                        classes_in_split[dataset_type][setting][fold]['val']   = val_classes\n",
    "                                        classes_in_split[dataset_type][setting][fold]['test']  = test_classes\n",
    "\n",
    "\n",
    "                                # Check that the class distribution for this fold is approximately uniform.\n",
    "                                if (dataset_type == 'HT' and setting != 'N'):\n",
    "                                    classes_train_uniform_check = np.zeros(num_seen_classes, dtype=np.int8)\n",
    "                                    classes_val_uniform_check   = np.zeros(num_seen_classes, dtype=np.int8)\n",
    "                                    classes_test_uniform_check  = np.zeros(num_seen_classes, dtype=np.int8)\n",
    "\n",
    "                                    for fold in range(num_folds[dataset_type]):\n",
    "                                        classes_train_uniform_check[classes_in_split[dataset_type][setting][fold]['train']] += 1\n",
    "                                        classes_val_uniform_check[classes_in_split[dataset_type][setting][fold]['val']] += 1\n",
    "                                        classes_test_uniform_check[classes_in_split[dataset_type][setting][fold]['test']] += 1\n",
    "#                                     print('\\n\\n')\n",
    "#                                     print('TRAIN:', classes_train_uniform_check)\n",
    "#                                     print('VAL:', classes_val_uniform_check)\n",
    "#                                     print('TEST:', classes_test_uniform_check)\n",
    "\n",
    "                                    train_diff = np.max(classes_train_uniform_check) - np.min(classes_train_uniform_check)\n",
    "                                    err_msg = setting + \" does not have an approximate uniform class distribution. Train Diff: \" + str(train_diff)\n",
    "                                    err_msg += \" | (max: \" + str(max_verified_classes_so_far) + \")\"\n",
    "                                    assert train_diff <= uniform_class_max_diff, err_msg\n",
    "                                    err_msg = setting + \" does not have at least 2 times each class for train: \"\n",
    "                                    err_msg += str(num_seen_classes - np.sum(classes_train_uniform_check >= class_min_num_times_in_setting_split_HT))\n",
    "                                    err_msg += \" | (max: \" + str(max_verified_classes_so_far) + \")\"\n",
    "                                    assert (classes_train_uniform_check >= class_min_num_times_in_setting_split_HT).all(), err_msg\n",
    "        \n",
    "                                    val_diff = np.max(classes_val_uniform_check) - np.min(classes_val_uniform_check)\n",
    "                                    err_msg = setting + \" does not have an approximate uniform class distribution. Val Diff: \" + str(val_diff)\n",
    "                                    err_msg += \" | (max: \" + str(max_verified_classes_so_far) + \")\"\n",
    "                                    assert val_diff <= uniform_class_max_diff_HT, err_msg\n",
    "        \n",
    "                                    test_diff = np.max(classes_test_uniform_check) - np.min(classes_test_uniform_check)\n",
    "                                    err_msg = setting + \" does not have an approximate uniform class distribution. Test Diff: \" + str(test_diff)\n",
    "                                    err_msg += \" | (max: \" + str(max_verified_classes_so_far) + \")\"\n",
    "                                    assert test_diff <= uniform_class_max_diff_HT, err_msg\n",
    "\n",
    "                                    assert (classes_val_uniform_check + classes_test_uniform_check > 0).all() \n",
    "\n",
    "                                # Do the same in case this is the final_evaluation dataset.\n",
    "                                if (dataset_type == 'final' and setting != 'N'):\n",
    "                                    classes_train_uniform_check = np.zeros(num_seen_classes, dtype=np.int8)\n",
    "                                    classes_val_uniform_check   = np.zeros(len(idx_to_class_and_len), dtype=np.int8)\n",
    "                                    classes_test_uniform_check  = np.zeros(len(idx_to_class_and_len), dtype=np.int8)\n",
    "\n",
    "                                    for fold in range(num_folds[dataset_type]):\n",
    "                                        classes_train_uniform_check[classes_in_split[dataset_type][setting][fold]['train']] += 1\n",
    "                                        classes_val_uniform_check[classes_in_split[dataset_type][setting][fold]['val']] += 1\n",
    "                                        classes_test_uniform_check[classes_in_split[dataset_type][setting][fold]['test']] += 1\n",
    "\n",
    "                                    train_diff = np.max(classes_train_uniform_check) - np.min(classes_train_uniform_check)\n",
    "                                    err_msg = setting + \" does not have an approximate uniform class distribution. Train Diff: \" + str(train_diff)\n",
    "                                    err_msg += \" | (max: \" + str(max_verified_classes_so_far) + \")\"\n",
    "                                    assert train_diff <= uniform_class_max_diff, err_msg\n",
    "                                    err_msg = setting + \" does not have at least 2 times each class for train: \"\n",
    "                                    err_msg += str(num_seen_classes - np.sum(classes_train_uniform_check[:num_seen_classes] >= 2))\n",
    "                                    err_msg += \" | (max: \" + str(max_verified_classes_so_far) + \")\"\n",
    "                                    assert (classes_train_uniform_check[:num_seen_classes] >= class_min_num_times_in_setting_split).all(), err_msg\n",
    "        \n",
    "                                    val_diff = np.max(classes_val_uniform_check[num_seen_classes:]) - np.min(classes_val_uniform_check[num_seen_classes:])\n",
    "                                    err_msg = setting + \" does not have an approximate uniform class distribution. Val Diff: \" + str(val_diff)\n",
    "                                    err_msg += \" | (max: \" + str(max_verified_classes_so_far) + \")\"\n",
    "                                    assert val_diff <= uniform_class_max_diff, err_msg\n",
    "                                    err_msg = setting + \" does not have at least 2 times each class for val: \"\n",
    "                                    err_msg += str((len(idx_to_class_and_len) - num_seen_classes) - np.sum(classes_val_uniform_check[num_seen_classes:] >= 2))\n",
    "                                    err_msg += \" | (max: \" + str(max_verified_classes_so_far) + \")\"\n",
    "                                    assert (classes_val_uniform_check[num_seen_classes:] >= class_min_num_times_in_setting_split).all(), err_msg\n",
    "        \n",
    "                                    test_diff = np.max(classes_test_uniform_check[num_seen_classes:]) - np.min(classes_test_uniform_check[num_seen_classes:])\n",
    "                                    err_msg = setting + \" does not have an approximate uniform class distribution. Test Diff: \" + str(test_diff)\n",
    "                                    err_msg += \" | (max: \" + str(max_verified_classes_so_far) + \")\"\n",
    "                                    assert test_diff <= uniform_class_max_diff, err_msg\n",
    "                                    err_msg = setting + \" does not have at least 2 times each class for test: \"\n",
    "                                    err_msg += str((len(idx_to_class_and_len) - num_seen_classes) - np.sum(classes_test_uniform_check[num_seen_classes:] >= 2))\n",
    "                                    err_msg += \" | (max: \" + str(max_verified_classes_so_far) + \")\"\n",
    "                                    assert (classes_test_uniform_check[num_seen_classes:] >= class_min_num_times_in_setting_split).all(), err_msg\n",
    "                                \n",
    "\n",
    "                                # Now we sample the number of instances for this fold.\n",
    "                                for fold in range(num_folds[dataset_type]):\n",
    "#                                     print(setting)\n",
    "                                    if (setting == 'N' or setting == 'ZS'):\n",
    "                                        for split in splits:\n",
    "                                            if (split != 'train'):\n",
    "                                                min_count = min_count_eval\n",
    "                                            else:\n",
    "                                                min_count = 1 if setting == 'N' else min_count_train\n",
    "                                            samples = get_num_samples_and_guarantee_min_count(classes_num_insts[classes_in_split[dataset_type][setting][fold][split]],\n",
    "                                                                                              split_size[dataset_type][split], min_count=min_count)\n",
    "                                            num_instances_check[dataset_type][setting][fold, splits[split], classes_in_split[dataset_type][setting][fold][split]] = samples\n",
    "#                                             if (dataset_type == 'HT'):\n",
    "#                                                 print('\\n\\n\\n', dataset_type, setting, fold, split, '\\n', num_instances_check[dataset_type][setting].shape,\n",
    "#                                                       '\\n', num_instances_check[dataset_type][setting][fold, splits[split], :])\n",
    "\n",
    "                                    elif (setting == 'GZS'):\n",
    "                                        for split in splits:\n",
    "                                            min_count = min_count_train if split == 'train' else min_count_eval\n",
    "                                            if (split == 'train'):\n",
    "                                                samples = get_num_samples_and_guarantee_min_count(classes_num_insts[classes_in_split[dataset_type][setting][fold][split]],\n",
    "                                                                                                  split_size[dataset_type][split], min_count=min_count)\n",
    "                                                num_instances_check[dataset_type][setting][fold, splits[split], classes_in_split[dataset_type][setting][fold][split]] = samples\n",
    "                                            else:\n",
    "                                                t_samples = get_num_samples_and_guarantee_min_count(classes_num_insts[classes_in_split[dataset_type][setting][fold]['train']],\n",
    "                                                                                                    int(0.5*split_size[dataset_type][split]), min_count=min_count_eval)\n",
    "                                                e_samples = get_num_samples_and_guarantee_min_count(classes_num_insts[classes_in_split[dataset_type][setting][fold][split]],\n",
    "                                                                                                    split_size[dataset_type][split] - int(0.5*split_size[dataset_type][split]),\n",
    "                                                                                                    min_count=min_count_eval)\n",
    "                                                num_instances_check[dataset_type][setting][fold, splits[split], classes_in_split[dataset_type][setting][fold]['train']] = t_samples\n",
    "                                                num_instances_check[dataset_type][setting][fold, splits[split], classes_in_split[dataset_type][setting][fold][split]]   = e_samples\n",
    "\n",
    "                                    else: # (Generalised) Few-Shot Setting.\n",
    "                                        num_shots = int(setting.split('-')[1])\n",
    "                                        t_samples_from_val = get_num_samples_and_guarantee_min_count(classes_num_insts[classes_in_split[dataset_type][setting][fold]['val']],\n",
    "                                                                                                     num_shots*len(classes_in_split[dataset_type][setting][fold]['val']),\n",
    "                                                                                                     min_count=num_shots)\n",
    "                                        t_samples_from_tst = get_num_samples_and_guarantee_min_count(classes_num_insts[classes_in_split[dataset_type][setting][fold]['test']],\n",
    "                                                                                                     num_shots*len(classes_in_split[dataset_type][setting][fold]['test']),\n",
    "                                                                                                     min_count=num_shots)\n",
    "                                        eval_num_samples = np.sum(t_samples_from_val) + np.sum(t_samples_from_tst)\n",
    "                                        t_samples_from_trn = get_num_samples_and_guarantee_min_count(classes_num_insts[classes_in_split[dataset_type][setting][fold]['train']],\n",
    "                                                                                                     split_size[dataset_type]['train'] - eval_num_samples,\n",
    "                                                                                                     min_count=min_count_train)\n",
    "                                        num_instances_check[dataset_type][setting][fold, splits['train'], classes_in_split[dataset_type][setting][fold]['train']] = t_samples_from_trn\n",
    "                                        num_instances_check[dataset_type][setting][fold, splits['train'], classes_in_split[dataset_type][setting][fold]['val']]   = t_samples_from_val\n",
    "                                        num_instances_check[dataset_type][setting][fold, splits['train'], classes_in_split[dataset_type][setting][fold]['test']]  = t_samples_from_tst\n",
    "\n",
    "                                        for split in splits:\n",
    "                                            if (split == 'train'):\n",
    "                                                continue\n",
    "                                            else:\n",
    "                                                if (setting[0] == 'F'): # Non-Generalised Setting\n",
    "                                                    samples = get_num_samples_and_guarantee_min_count(classes_num_insts[classes_in_split[dataset_type][setting][fold][split]],\n",
    "                                                                                                      split_size[dataset_type][split], min_count=min_count_eval)\n",
    "                                                    num_instances_check[dataset_type][setting][fold, splits[split], classes_in_split[dataset_type][setting][fold][split]] = samples\n",
    "\n",
    "                                                else: # Generalised Setting\n",
    "                                                    t_samples = get_num_samples_and_guarantee_min_count(classes_num_insts[classes_in_split[dataset_type][setting][fold]['train']],\n",
    "                                                                                                        int(0.5*split_size[dataset_type][split]), min_count=min_count_eval)\n",
    "                                                    e_samples = get_num_samples_and_guarantee_min_count(classes_num_insts[classes_in_split[dataset_type][setting][fold][split]],\n",
    "                                                                                                        split_size[dataset_type][split] - int(0.5*split_size[dataset_type][split]),\n",
    "                                                                                                        min_count=min_count_eval)\n",
    "                                                    num_instances_check[dataset_type][setting][fold, splits[split], classes_in_split[dataset_type][setting][fold]['train']] = t_samples\n",
    "                                                    num_instances_check[dataset_type][setting][fold, splits[split], classes_in_split[dataset_type][setting][fold][split]]   = e_samples\n",
    "\n",
    "\n",
    "                                    # Verify that it is possible to build this fold.\n",
    "                                    if (dataset_type != 'DEBUG'):\n",
    "                                        if (dataset_type == 'HT'):\n",
    "                                            fold_num_insts = np.sum(num_instances_check[dataset_type][setting][fold], axis=0)\n",
    "                                        elif (dataset_type == 'final'):\n",
    "                                            if (HT_completely_disjoint):\n",
    "                                                fold_train_num_insts = np.maximum(disjoint_HT, num_instances_check[dataset_type][setting][fold][splits['train']])\n",
    "                                            else:\n",
    "                                                fold_train_num_insts = np.maximum(disjoint_train_val_HT + test_HT_maximums, num_instances_check[dataset_type][setting][fold][splits['train']])\n",
    "                                            fold_num_insts = fold_train_num_insts + np.sum(num_instances_check[dataset_type][setting][fold][1:], axis=0)\n",
    "\n",
    "                                        verify_fold_less_equal = np.less_equal(fold_num_insts, classes_num_insts)\n",
    "                                        err_msg = dataset_type + \", \" + setting + \", \" + str(fold) + \": \" + str(np.sum(verify_fold_less_equal))\n",
    "                                        if (dataset_type == 'final'):\n",
    "                                            if (not all(verify_fold_less_equal[:len(seen_classes_as_idx)])):\n",
    "                                                num_seen_classes_fails += 1\n",
    "                                            if (not all(verify_fold_less_equal[len(seen_classes_as_idx):])):\n",
    "                                                num_unseen_classes_fails += 1\n",
    "                                        assert verify_fold_less_equal.all(), err_msg\n",
    "\n",
    "                                        # Increment counts to test for setting disjointness.\n",
    "                                        if (dataset_type == 'HT' and HT_completely_disjoint):\n",
    "                                            disjoint_HT_setting += fold_num_insts\n",
    "                                        else:\n",
    "                                            disjoint_train_setting += num_instances_check[dataset_type][setting][fold][splits['train']]\n",
    "                                            disjoint_val_setting   += num_instances_check[dataset_type][setting][fold][splits['val']]\n",
    "                                            if (test_only_in_other_tests):\n",
    "                                                test_setting_maximums = np.maximum(test_setting_maximums, num_instances_check[dataset_type][setting][fold][splits['test']])\n",
    "\n",
    "                                # Now verify that this entire setting is possible by itself.\n",
    "                                if (dataset_type != 'DEBUG'):\n",
    "                                    if (test_only_in_other_tests):\n",
    "                                        if (dataset_type == 'HT'):\n",
    "                                            if (HT_completely_disjoint):\n",
    "                                                total_setting_num_instances = disjoint_HT_setting \n",
    "                                            else:\n",
    "                                                total_setting_num_instances = disjoint_train_setting + disjoint_val_setting + test_setting_maximums\n",
    "                                        elif (dataset_type == 'final'):\n",
    "                                            # This accounts for the train set being allowed to reuse instances from the 'HT' dataset.\n",
    "                                            if (HT_completely_disjoint):\n",
    "                                                setting_train_num_insts = np.maximum(disjoint_HT, disjoint_train_setting)\n",
    "                                            else:\n",
    "                                                setting_train_num_insts = np.maximum(disjoint_train_val_HT + test_HT_maximums, disjoint_train_setting)\n",
    "                                            total_setting_num_instances = setting_train_num_insts + disjoint_val_setting + test_setting_maximums\n",
    "                                    verify_setting_less_equal = np.less_equal(total_setting_num_instances, classes_num_insts)\n",
    "\n",
    "                                    if (dataset_type == 'final'):\n",
    "                                        if (not all(verify_setting_less_equal[:len(seen_classes_as_idx)])):\n",
    "                                            num_seen_classes_fails += 1\n",
    "                                        if (not all(verify_setting_less_equal[len(seen_classes_as_idx):])):\n",
    "                                            num_unseen_classes_fails += 1\n",
    "\n",
    "                                    num_verified_classes = np.sum(verify_setting_less_equal)\n",
    "                                    if (max_verified_classes_so_far < num_verified_classes):\n",
    "                                        max_verified_classes_so_far = num_verified_classes\n",
    "\n",
    "\n",
    "                                    err_msg = \"(\" + dataset_type + \", \" + setting + \") verified number of relations: \" + str(np.sum(verify_setting_less_equal))\n",
    "                                    err_msg += \" (max: \" + str(max_verified_classes_so_far) + \")\"\n",
    "                                    assert verify_setting_less_equal.all(), err_msg\n",
    "\n",
    "                                found_setting_solution = True\n",
    "\n",
    "                            except AssertionError as err:\n",
    "                                if (not found_setting_solution):\n",
    "                                    if (dataset_type == 'HT'):\n",
    "                                        print('\\x1b[2K\\r', err, end=\"\", flush=True)\n",
    "                                        if (num_tries_setting_solution == 5000):\n",
    "                                            raise AssertionError(err)\n",
    "                                    elif (dataset_type == 'final'):\n",
    "                                        if (num_tries_setting_solution >= 2000 and num_seen_classes_fails/num_tries_setting_solution > 0.2):\n",
    "                                            raise AssertionError(err)\n",
    "                                    if (dataset_type == 'final' and (num_tries_setting_solution - 1) % 250 == 0):\n",
    "                                        print('\\r' + str(err) + \" | Number of attempts: \" + str(num_tries_setting_solution) +\n",
    "                                              \" ('seen' fails: \" + str(num_seen_classes_fails) +\n",
    "                                              ' ({:.2f}%)'.format(100*num_seen_classes_fails/num_tries_setting_solution) + \")\" +\n",
    "                                              \" ('unseen' fails: \" + str(num_unseen_classes_fails) +\n",
    "                                              ' ({:.2f}%)'.format(100*num_unseen_classes_fails/num_tries_setting_solution) + \")\",\n",
    "                                              end=\"\", flush=True)\n",
    "                                    num_tries_setting_solution += 1\n",
    "\n",
    "                        # Increment counts to test for 'HT' disjointness and check for disjointness.\n",
    "                        if (dataset_type == 'HT'):\n",
    "                            if (HT_completely_disjoint):\n",
    "                                disjoint_HT += disjoint_HT_setting\n",
    "                                verify_disjoint_HT = disjoint_HT\n",
    "                            else:\n",
    "                                disjoint_train_val_HT += disjoint_train_setting + disjoint_val_setting\n",
    "                                if (test_only_in_other_tests):\n",
    "                                    test_HT_maximums = np.maximum(test_HT_maximums, test_setting_maximums)\n",
    "                                verify_disjoint_HT = disjoint_train_val_HT + test_HT_maximums\n",
    "\n",
    "                            # Here we check if it is still possible (with this new setting) to create the dataset_type 'HT'.\n",
    "                            verify_HT_settings_less_equal = np.less_equal(verify_disjoint_HT, classes_num_insts)\n",
    "                            err_msg = dataset_type + \" (verified number of settings before failure): \" + str(setting_num) + \"/\"\n",
    "                            err_msg += str(len(settings[dataset_type])) + \".\" + \" | \" + str(maximum_number_of_settings_verified)\n",
    "                            assert verify_HT_settings_less_equal.all(), err_msg\n",
    "\n",
    "                        if (dataset_type != 'DEBUG'):\n",
    "                            maximum_number_of_settings_verified += 1\n",
    "\n",
    "                            if (max_max_num_settings_verified < maximum_number_of_settings_verified):\n",
    "                                max_max_num_settings_verified = maximum_number_of_settings_verified\n",
    "\n",
    "                            if (dataset_type == 'HT'):\n",
    "                                print(\"\\rDataset: \" + dataset_type + \" | Maximum Verified Settings: \" +\n",
    "                                      str(settings[dataset_type][:max_max_num_settings_verified]) +\n",
    "                                      \" (\" + str(maximum_number_of_settings_verified) + \")\", end=\"\", flush=True)\n",
    "                            if (dataset_type == 'final'):\n",
    "                                print(\"\\rFound solution for (\" + dataset_type + \", \" + setting + \") after \" + str(num_tries_setting_solution) + \" iterations.\")\n",
    "\n",
    "                            if (maximum_number_of_settings_verified == len(settings[dataset_type])):\n",
    "                                found_dataset_type_solution = True\n",
    "                        else:\n",
    "                            found_dataset_type_solution = True\n",
    "\n",
    "                except AssertionError as err:\n",
    "                    if (not found_dataset_type_solution):\n",
    "                        if (dataset_type == 'final'):\n",
    "                            raise RuntimeError(err)\n",
    "                        num_tries_dataset_type_solution += 1\n",
    "\n",
    "\n",
    "            print(\"\\nFound solution for (\" + dataset_type + \") after \" + str(num_tries_dataset_type_solution) + \" iterations.\\n\")\n",
    "            if (dataset_type == 'final'):\n",
    "                found_global_solution = True\n",
    "\n",
    "    except RuntimeError as err:\n",
    "        if (not found_global_solution):\n",
    "            clear_output()\n",
    "            num_tries_global_solution += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(join_path([path_proposed_splits_original_masking, 'proposed_splits_data_distribution.dat']), 'wb') as f:\n",
    "    pickle.dump(num_instances_check, f)\n",
    "    pickle.dump(classes_in_split, f)\n",
    "\n",
    "\n",
    "with open(join_path([path_proposed_splits_original_masking, 'proposed_splits_meta_data.txt']), 'w', encoding='utf-8') as f:\n",
    "    pretty_dict_json_dump(proposed_splits_meta_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join_path([path_proposed_splits_original, 'proposed_splits_data_distribution.dat']), 'rb') as f:\n",
    "    num_instances_check = pickle.load(f)\n",
    "    classes_in_split = pickle.load(f)\n",
    "\n",
    "disjoint_HT = sum(np.sum(num_instances_check['HT'][setting], axis=(0, 1)) for setting in num_instances_check['HT'])\n",
    "\n",
    "\n",
    "with open(join_path([path_proposed_splits_original, 'proposed_splits_meta_data.txt']), 'r', encoding='utf-8') as f:\n",
    "    proposed_splits_meta_data = pretty_dict_json_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d435b00be31f480698bb9de5a1ef707c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset_type', options=('DEBUG', 'HT', 'final'), value='DEBUG'), O…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.set_printoptions(formatter={'all':lambda x: '{:5d}'.format(x)})\n",
    "def specific_dataset_print(dataset_type, setting, fold, split, mask_train, mask_class_dist):\n",
    "    # The interact module automatically retrieves the dictionary values, for the corresponding keys.\n",
    "\n",
    "    print(\"\\n\"*5)\n",
    "    if (setting != 'N'):\n",
    "        train_mask = (num_instances_check[dataset_type][setting][fold][0] == 0).astype(np.int8)\n",
    "        if (setting[:3] == 'GFS' or setting[:3] == 'FS-'):\n",
    "            train_mask += (num_instances_check[dataset_type][setting][fold][0] == int(setting.split('-')[1])).astype(np.int8)\n",
    "    else:\n",
    "        train_mask = np.ones_like(num_instances_check[dataset_type][setting][fold][0], dtype=np.int8)\n",
    "\n",
    "    if (setting[:3] == 'GFS' or setting[:3] == 'FS-'):\n",
    "        anti_train_mask = (~train_mask.astype(np.bool)).astype(np.int8)\n",
    "    else:\n",
    "        anti_train_mask = np.ones_like(num_instances_check[dataset_type][setting][fold][0], dtype=np.int8)\n",
    "        \n",
    "\n",
    "    masked = np.multiply(train_mask, num_instances_check[dataset_type][setting][fold][split])\n",
    "    anti_masked = np.multiply(anti_train_mask, num_instances_check[dataset_type][setting][fold][split])\n",
    "\n",
    "    \n",
    "    print('Fold num insts:', sum(np.sum(num_instances_check[dataset_type][setting][fold][split_num])\n",
    "                                 for split_num in proposed_splits_meta_data['splits'].values()))\n",
    "    print('Split num insts:', np.sum(num_instances_check[dataset_type][setting][fold][split]))\n",
    "    print(\"'Exclusive' split num insts:\", np.sum(masked if split != 0 else anti_masked))\n",
    "\n",
    "    if (setting != 'N' and not mask_class_dist):\n",
    "        print(\"'seen' classes distribution train:\")\n",
    "        if (dataset_type == 'HT' or dataset_type == 'DEBUG'):\n",
    "            train = np.zeros(proposed_splits_meta_data['num_seen_classes'], dtype=np.int8)\n",
    "            val = np.zeros(proposed_splits_meta_data['num_seen_classes'], dtype=np.int8)\n",
    "            test = np.zeros(proposed_splits_meta_data['num_seen_classes'], dtype=np.int8)\n",
    "        else:\n",
    "            total_num_classes = num_instances_check[dataset_type][setting][fold][split].shape[0]\n",
    "            train = np.zeros(proposed_splits_meta_data['num_seen_classes'], dtype=np.int8)\n",
    "            val = np.zeros(total_num_classes - proposed_splits_meta_data['num_seen_classes'], dtype=np.int8)\n",
    "            test = np.zeros(total_num_classes - proposed_splits_meta_data['num_seen_classes'], dtype=np.int8)\n",
    "\n",
    "        for fold_num in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "            train[classes_in_split[dataset_type][setting][fold_num]['train']] += 1\n",
    "            val_classes = [class_idx - (proposed_splits_meta_data['num_seen_classes'] if dataset_type == 'final' else 0)\n",
    "                           for class_idx in classes_in_split[dataset_type][setting][fold_num]['val']]\n",
    "            val[val_classes] += 1\n",
    "            test_classes = [class_idx - (proposed_splits_meta_data['num_seen_classes'] if dataset_type == 'final' else 0)\n",
    "                            for class_idx in classes_in_split[dataset_type][setting][fold_num]['test']]\n",
    "            test[test_classes] += 1\n",
    "\n",
    "    #     dist_seen = np.sum(num_instances_check[dataset_type][setting][fold_num][0][:end] != 0\n",
    "    #                        for fold_num in range(proposed_splits_meta_data['num_folds'][dataset_type]))\n",
    "        print(\"Diff max-min:\", np.max(train) - np.min(train))\n",
    "        print(train)\n",
    "        print(\"'unseen' classes distribution val:\")\n",
    "    #     dist_unseen_val = np.sum(num_instances_check[dataset_type][setting][fold_num][1][start:-end_eval] != 0\n",
    "    #                              for fold_num in range(proposed_splits_meta_data['num_folds'][dataset_type]))\n",
    "        print(\"Diff max-min:\", np.max(val) - np.min(val))\n",
    "        print(val)\n",
    "        print(\"'unseen' classes distribution test:\")\n",
    "    #     dist_unseen_test = np.sum(num_instances_check[dataset_type][setting][fold_num][2][start:-end_eval] != 0\n",
    "    #                               for fold_num in range(proposed_splits_meta_data['num_folds'][dataset_type]))\n",
    "        print(\"Diff max-min:\", np.max(test) - np.min(test))\n",
    "        print(test)\n",
    "        print()\n",
    "    print(masked if mask_train else num_instances_check[dataset_type][setting][fold][split])\n",
    "\n",
    "def select_dataset_type_print(dataset_type):\n",
    "    global settings\n",
    "    global num_folds\n",
    "    global splits\n",
    "    interact(specific_dataset_print, dataset_type=fixed(dataset_type), setting=list(num_instances_check[dataset_type].keys()),\n",
    "             fold=list(range(proposed_splits_meta_data['num_folds'][dataset_type])), split=proposed_splits_meta_data['splits'],\n",
    "             mask_train=False, mask_class_dist=False)\n",
    "\n",
    "interact(select_dataset_type_print, dataset_type=proposed_splits_meta_data['dataset_types']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New - getting instances into files, already separating into different masking types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We use this specific one below to get instances into files when we are already accouting for instance uniqueness with masking when creating the splits distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120   voice type\n",
      "\n",
      "\n",
      "The following settings are fully disjoint (also disjoint from the HT dataset):\n",
      "\n",
      "\n",
      "The following settings have disjoint test splits (However, some of the train instances will also exist in the HT dataset):\n",
      "\n",
      "\n",
      "The following settings do NOT have disjoint test splits (and some of the train instances will also exist in the HT dataset):\n",
      "N\n",
      "ZS\n",
      "GZS\n",
      "FS-1\n",
      "GFS-1\n",
      "FS-2\n",
      "GFS-2\n",
      "FS-5\n",
      "GFS-5\n",
      "FS-10\n",
      "GFS-10\n"
     ]
    }
   ],
   "source": [
    "# We define the fold names of each dataset_type.\n",
    "dataset_type_names = {'HT': 'hyperparameter_tuning', 'final': 'final_evaluation', 'DEBUG': 'DEBUG'}\n",
    "masking_types = ['original', 'unmasked', 'sub_obj_masking', 'NER_masking']\n",
    "\n",
    "# We get the path to the data files separated by class.\n",
    "max_sentence_length = proposed_splits_meta_data['max_sentence_length']\n",
    "max_sentence_length_exclusive_unique_masking = 'max_tokenized_sentence_len_' + str(max_sentence_length) + '_exclusive_and_unique_relation_masking'\n",
    "path_dir_capped_exclusive_unique_masking = join_path([path_ours, 'Levy_by_relation', max_sentence_length_exclusive_unique_masking])\n",
    "\n",
    "\n",
    "# We open the destination files, start keeping track of labels and relations exclusive to each split.\n",
    "paths = {}\n",
    "labels = {}\n",
    "relations_exclusive_to_split = {}\n",
    "for masking_type in masking_types:\n",
    "    paths[masking_type] = {}\n",
    "    labels[masking_type] = {}\n",
    "    relations_exclusive_to_split[masking_type] = {}\n",
    "\n",
    "    path = join_path([path_proposed_splits_masking, masking_type]) + os.sep\n",
    "    directory = os.path.dirname(path)\n",
    "    if (not os.path.exists(directory)):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "        paths[masking_type][dataset_type] = {}\n",
    "        labels[masking_type][dataset_type] = {}\n",
    "        relations_exclusive_to_split[masking_type][dataset_type] = {}\n",
    "\n",
    "        path = join_path([path_proposed_splits_masking, masking_type,\n",
    "                          dataset_type_names[dataset_type]]) + os.sep\n",
    "        directory = os.path.dirname(path)\n",
    "        if (not os.path.exists(directory)):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "            paths[masking_type][dataset_type][setting] = {}\n",
    "            labels[masking_type][dataset_type][setting] = {}\n",
    "            relations_exclusive_to_split[masking_type][dataset_type][setting] = {}\n",
    "\n",
    "            path = join_path([path_proposed_splits_masking, masking_type,\n",
    "                              dataset_type_names[dataset_type], setting]) + os.sep\n",
    "            directory = os.path.dirname(path)\n",
    "            if (not os.path.exists(directory)):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "                paths[masking_type][dataset_type][setting][fold] = {}\n",
    "                labels[masking_type][dataset_type][setting][fold] = {}\n",
    "                relations_exclusive_to_split[masking_type][dataset_type][setting][fold] = {}\n",
    "\n",
    "                path = join_path([path_proposed_splits_masking, masking_type,\n",
    "                                  dataset_type_names[dataset_type], setting, str(fold)]) + os.sep\n",
    "\n",
    "                directory = os.path.dirname(path)\n",
    "                if (not os.path.exists(directory)):\n",
    "                    os.makedirs(directory)\n",
    "\n",
    "                for split in proposed_splits_meta_data['splits']:\n",
    "                    path = join_path([path_proposed_splits_masking, masking_type,\n",
    "                                  dataset_type_names[dataset_type], setting, str(fold), split + \".txt\"])\n",
    "                    paths[masking_type][dataset_type][setting][fold][split] = open(path, 'w', encoding='utf-8')\n",
    "                    labels[masking_type][dataset_type][setting][fold][split] = []\n",
    "                    relations_exclusive_to_split[masking_type][dataset_type][setting][fold][split] = []\n",
    "\n",
    "# We get the total number of instances to be processed. Mostly for printing purposes and estimating remaining time.\n",
    "num_instances_per_relation = {}\n",
    "for relation in relation_list:\n",
    "    num_instances_per_relation[relation] = file_len(join_path([path_dir_capped_exclusive_unique_masking, 'original',\n",
    "                                                               re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt']))\n",
    "total_num_instances = sum(num_instances_per_relation[relation] for relation in num_instances_per_relation)\n",
    "\n",
    "\n",
    "# We start the process of creating the splits.\n",
    "instance_num = 0\n",
    "try:\n",
    "    for r_num, relation in enumerate(relation_list):\n",
    "        print('\\x1b[2K\\r' + str(r_num + 1) + '/' + str(len(relation_list)) + '   ' + relation, end=\"\", flush=True)\n",
    "        for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "            for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "                for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "                    for split in proposed_splits_meta_data['splits']:\n",
    "                        for masking_type in masking_types:\n",
    "                            n = num_instances_check[dataset_type][setting][fold][proposed_splits_meta_data['splits'][split]][proposed_splits_meta_data['class_to_idx'][relation]]\n",
    "                            labels[masking_type][dataset_type][setting][fold][split] += [relation] * n\n",
    "                            if (proposed_splits_meta_data['class_to_idx'][relation] in classes_in_split[dataset_type][setting][fold][split]):\n",
    "                                relations_exclusive_to_split[masking_type][dataset_type][setting][fold][split].append(relation)\n",
    "\n",
    "        data_to_write = {masking_type: [] for masking_type in masking_types}\n",
    "\n",
    "        # Load all the data pertaining this relation.\n",
    "        for masking_type in masking_types:\n",
    "            if (masking_type != 'unmasked'):\n",
    "                path_relation = join_path([path_dir_capped_exclusive_unique_masking, masking_type, re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt'])\n",
    "                with open(path_relation, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        line_temp = line.strip()\n",
    "                        if (masking_type == 'original'):\n",
    "                            data_to_write['original'].append(relation + '\\t' + line_temp)\n",
    "                            data_to_write['unmasked'].append(''.join(space_word_tokenize_string(line_temp.split('\\t')[0], tokenizer)))\n",
    "                        else:\n",
    "                            data_to_write[masking_type].append(line_temp)\n",
    "                            \n",
    "\n",
    "\n",
    "        err_msg = \"The number of instances is different from what was expected.\"\n",
    "        n = proposed_splits_meta_data['idx_to_class_and_len'][proposed_splits_meta_data['class_to_idx'][relation]][1]\n",
    "        assert all(len(data_to_write[masking_type]) == n for masking_type in masking_types), err_msg\n",
    "\n",
    "        # Shuffle data\n",
    "        zipped_data_to_write = list(zip(*tuple(data_to_write[masking_type] for masking_type in masking_types)))\n",
    "        random.shuffle(zipped_data_to_write)\n",
    "        for masking_type, masking_type_train_data in list(zip(data_to_write.keys(), list(zip(*zipped_data_to_write)))):\n",
    "            data_to_write[masking_type] = masking_type_train_data\n",
    "\n",
    "\n",
    "        num_instances_disjoint_HT = disjoint_HT[proposed_splits_meta_data['class_to_idx'][relation]]\n",
    "        HT_instances = {masking_type: data_to_write[masking_type][:num_instances_disjoint_HT] for masking_type in data_to_write}\n",
    "        non_HT_instances = {masking_type: data_to_write[masking_type][num_instances_disjoint_HT:] for masking_type in data_to_write}\n",
    "\n",
    "        # We start by writing out the 'DEBUG' and 'HT' datasets.\n",
    "        for masking_type in masking_types:\n",
    "            for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "                if (dataset_type != 'final'):\n",
    "                    current_slice_start = 0\n",
    "                    for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "                        for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "                            for split in proposed_splits_meta_data['splits']:\n",
    "                                current_slice_end = current_slice_start\n",
    "                                current_slice_end += num_instances_check[dataset_type][setting][fold][proposed_splits_meta_data['splits'][split]][proposed_splits_meta_data['class_to_idx'][relation]]\n",
    "                                # DEBUG classes also belong to the seen classes.\n",
    "                                for line in HT_instances[masking_type][current_slice_start:current_slice_end]:\n",
    "                                    paths[masking_type][dataset_type][setting][fold][split].write(line + '\\n')\n",
    "                                current_slice_start += num_instances_check[dataset_type][setting][fold][proposed_splits_meta_data['splits'][split]][proposed_splits_meta_data['class_to_idx'][relation]]\n",
    "\n",
    "        # Here we take care of the 'final' dataset\n",
    "        fully_disjoint_from_HT_settings = {}\n",
    "        disjoint_test_splits_but_no_disjointness_from_HT_settings = {}\n",
    "        test_in_other_tests_settings = {}\n",
    "        for setting in proposed_splits_meta_data['settings']['final']:\n",
    "            if (setting not in fully_disjoint_from_HT_settings):\n",
    "                fully_disjoint_from_HT_settings[setting] = []\n",
    "                disjoint_test_splits_but_no_disjointness_from_HT_settings[setting] = []\n",
    "                test_in_other_tests_settings[setting] = []\n",
    "\n",
    "            fold_sums = np.sum(num_instances_check['final'][setting], axis=0)\n",
    "            num_instances_disjoint_train = fold_sums[0][proposed_splits_meta_data['class_to_idx'][relation]]\n",
    "            num_instances_disjoint_val = fold_sums[1][proposed_splits_meta_data['class_to_idx'][relation]]\n",
    "            num_instances_disjoint_test = fold_sums[2][proposed_splits_meta_data['class_to_idx'][relation]]\n",
    "            num_max_instances_test = np.max(num_instances_check['final'][setting], axis=0)[2][proposed_splits_meta_data['class_to_idx'][relation]]\n",
    "\n",
    "            train_final_bigger_disjoint_HT = num_instances_disjoint_train > num_instances_disjoint_HT\n",
    "\n",
    "            fully_disjoint = num_instances_disjoint_train + num_instances_disjoint_val + num_instances_disjoint_test < len(non_HT_instances['original'])\n",
    "            train_in_non_HT_instances = num_instances_disjoint_train - num_instances_disjoint_HT if train_final_bigger_disjoint_HT else 0\n",
    "            disjoint_test = train_in_non_HT_instances + num_instances_disjoint_val + num_instances_disjoint_test < len(non_HT_instances['original'])\n",
    "\n",
    "            final_train_instances = {}\n",
    "            final_val_instances = {}\n",
    "            final_test_instances = {}\n",
    "\n",
    "            if (fully_disjoint):\n",
    "                fully_disjoint_from_HT_settings[setting].append(relation)\n",
    "                for masking_type in masking_types:\n",
    "                    final_train_instances[masking_type] = non_HT_instances[masking_type][:num_instances_disjoint_train]\n",
    "                    final_val_instances[masking_type] = non_HT_instances[masking_type][num_instances_disjoint_train:num_instances_disjoint_train + num_instances_disjoint_val]\n",
    "                    final_test_instances[masking_type] = non_HT_instances[masking_type][-num_instances_disjoint_test:]\n",
    "            else:\n",
    "                if (disjoint_test):\n",
    "                    disjoint_test_splits_but_no_disjointness_from_HT_settings[setting].append(relation)\n",
    "                else:\n",
    "                    test_in_other_tests_settings[setting].append(relation)\n",
    "\n",
    "                for masking_type in masking_types:\n",
    "                    if (disjoint_test):\n",
    "                        final_train_instances[masking_type] = HT_instances[masking_type] + non_HT_instances[masking_type][:-(num_instances_disjoint_test + num_instances_disjoint_val)]\n",
    "                        final_val_instances[masking_type] = non_HT_instances[masking_type][-(num_instances_disjoint_test + num_instances_disjoint_val):-num_instances_disjoint_test]\n",
    "                        final_test_instances[masking_type] = non_HT_instances[masking_type][-num_instances_disjoint_test:]\n",
    "                    else:\n",
    "                        # This way we can have a bigger pool of possible samples for the test splits.\n",
    "                        final_train_instances[masking_type] = HT_instances[masking_type] + non_HT_instances[masking_type][:train_in_non_HT_instances]\n",
    "                        final_val_instances[masking_type] = non_HT_instances[masking_type][train_in_non_HT_instances:train_in_non_HT_instances + num_instances_disjoint_val]\n",
    "                        assert len(non_HT_instances[masking_type]) - train_in_non_HT_instances + num_instances_disjoint_val >= num_max_instances_test, 'TEST IN TEST ERROR'\n",
    "                        final_test_instances[masking_type] = non_HT_instances[masking_type][train_in_non_HT_instances + num_instances_disjoint_val:]\n",
    "\n",
    "                # We shuffle the training instances to mix HT instances with potential non-HT instances.\n",
    "                zipped_train = list(zip(*tuple(final_train_instances[masking_type] for masking_type in masking_types)))\n",
    "                random.shuffle(zipped_train)\n",
    "                for masking_type, masking_type_train_data in list(zip(final_train_instances.keys(), list(zip(*zipped_train)))):\n",
    "                    final_train_instances[masking_type] = masking_type_train_data\n",
    "\n",
    "            # Now we write out the this specific setting of the 'final' dataset.\n",
    "            current_slice_start = {'train': 0, 'val': 0, 'test': 0}\n",
    "            for fold in range(proposed_splits_meta_data['num_folds']['final']):\n",
    "                for split in proposed_splits_meta_data['splits']:\n",
    "                    current_slice_end = current_slice_start[split]\n",
    "                    current_slice_end += num_instances_check['final'][setting][fold][proposed_splits_meta_data['splits'][split]][proposed_splits_meta_data['class_to_idx'][relation]]\n",
    "                    current_data = {}\n",
    "                    if (split == 'train'):\n",
    "                        for masking_type in masking_types:\n",
    "                            current_data[masking_type] = final_train_instances[masking_type]\n",
    "                    elif (split == 'val'):\n",
    "                        for masking_type in masking_types:\n",
    "                            current_data[masking_type] = final_val_instances[masking_type]\n",
    "                    else:\n",
    "                        if (fully_disjoint or disjoint_test):\n",
    "                            for masking_type in masking_types:\n",
    "                                current_data[masking_type] = final_test_instances[masking_type]\n",
    "                        else:\n",
    "                            zipped_test = list(zip(*tuple(final_test_instances[masking_type]\n",
    "                                                          for masking_type in masking_types)))\n",
    "                            random.shuffle(zipped_test)\n",
    "                            for masking_type, masking_type_test_data in list(zip(final_test_instances.keys(), list(zip(*zipped_test)))):\n",
    "                                final_test_instances[masking_type] = masking_type_test_data\n",
    "                            for masking_type in masking_types:\n",
    "                                current_data[masking_type] = final_test_instances[masking_type]\n",
    "\n",
    "                    for masking_type in masking_types:\n",
    "                        for line in current_data[masking_type][current_slice_start[split]:current_slice_end]:\n",
    "                            paths[masking_type]['final'][setting][fold][split].write(line + '\\n')\n",
    "\n",
    "                    if (split == 'test' and not fully_disjoint and not disjoint_test):\n",
    "                        pass\n",
    "                    else:\n",
    "                        current_slice_start[split] += num_instances_check['final'][setting][fold][proposed_splits_meta_data['splits'][split]][proposed_splits_meta_data['class_to_idx'][relation]]\n",
    "\n",
    "except:\n",
    "    # Close file_pointers in case of error.\n",
    "    for masking_type in masking_types:\n",
    "        for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "            for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "                for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "                    for split in proposed_splits_meta_data['splits']:\n",
    "                        paths[masking_type][dataset_type][setting][fold][split].close()\n",
    "    raise\n",
    "\n",
    "print('\\n\\n\\nThe following settings are fully disjoint (also disjoint from the HT dataset):')\n",
    "for setting in fully_disjoint_from_HT_settings:\n",
    "    if (all(relation in fully_disjoint_from_HT_settings[setting] for relation in relation_list)):\n",
    "        print(setting)\n",
    "\n",
    "print(\n",
    "    '\\n\\nThe following settings have disjoint test splits (However, some of the train instances will also exist in the HT dataset):')\n",
    "for setting in disjoint_test_splits_but_no_disjointness_from_HT_settings:\n",
    "    if (all(relation in fully_disjoint_from_HT_settings[setting] or relation in\n",
    "            disjoint_test_splits_but_no_disjointness_from_HT_settings[setting] for relation in relation_list) and\n",
    "            any(relation not in fully_disjoint_from_HT_settings[setting] for relation in relation_list)):\n",
    "        print(setting)\n",
    "\n",
    "print(\n",
    "    '\\n\\nThe following settings do NOT have disjoint test splits (and some of the train instances will also exist in the HT dataset):')\n",
    "for setting in test_in_other_tests_settings:\n",
    "    if (any(relation not in fully_disjoint_from_HT_settings[setting] and relation not in\n",
    "            disjoint_test_splits_but_no_disjointness_from_HT_settings[setting] for relation in relation_list)):\n",
    "        print(setting)\n",
    "\n",
    "# Close file_pointers.\n",
    "for masking_type in masking_types:\n",
    "    for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "        for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "            for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "                for split in proposed_splits_meta_data['splits']:\n",
    "                    split_cls = relations_exclusive_to_split[masking_type][dataset_type][setting][fold][split]\n",
    "                    with open(paths[masking_type][dataset_type][setting][fold][split].name[:-4] + '_relations.txt', 'w', encoding='utf-8') as f:\n",
    "                        for relation in split_cls:\n",
    "                            f.write(relation + '\\n')\n",
    "                    if (masking_type != 'original'):\n",
    "                        with open(paths[masking_type][dataset_type][setting][fold][split].name[:-4] + '.lbs', 'wb') as f:\n",
    "                            pickle.dump(labels[masking_type][dataset_type][setting][fold][split], f)\n",
    "                    paths[masking_type][dataset_type][setting][fold][split].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have found a data distribution (for each dataset_type, setting and fold) that respects the imposed constraints, we actually get the instances into each separate file. Wecreate different experiment settings, concerning the masking of entities in the sentences. Specifically, one where we simply use the plain sentences (disregarding any information concerning the entities involved in the relation we are attempting to produce), one where we mask out the entities of interest, namely with SUBJECT_ENTITY and OBJECT_ENTITY (since we assume we have binary relations) and a last one where we simply mask all entities recognized by a NER algorithm (where we hope it behaves the same as the unmasked one, but without allowing the model to overfit on specific entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run this part on gpu or otherwise it would take forever.\n",
    "# create_splits_for_masking_types(masking_types=['original', 'unmasked', 'sub_obj_masking', 'NER_masking'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As a sanity check we verify that the disjointness guarantees are actually verified. We also check that the classes present in each split are the correct ones and also check the minimum number of instances per split is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class assignments are correct.\n",
      "HT is completely disjoint.\n",
      "Number of distinct instances in HT: 632500\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Setting N has no repeated entries.\n",
      "Setting N is properly disjoint.\n",
      "\n",
      "Setting ZS has no repeated entries.\n",
      "Setting ZS is properly disjoint.\n",
      "\n",
      "Setting GZS has no repeated entries.\n",
      "Setting GZS is properly disjoint.\n",
      "\n",
      "Setting FS-1 has no repeated entries.\n",
      "Setting FS-1 is properly disjoint.\n",
      "\n",
      "Setting GFS-1 has no repeated entries.\n",
      "Setting GFS-1 is properly disjoint.\n",
      "\n",
      "Setting FS-2 has no repeated entries.\n",
      "Setting FS-2 is properly disjoint.\n",
      "\n",
      "Setting GFS-2 has no repeated entries.\n",
      "Setting GFS-2 is properly disjoint.\n",
      "\n",
      "Setting FS-5 has no repeated entries.\n",
      "Setting FS-5 is properly disjoint.\n",
      "\n",
      "Setting GFS-5 has no repeated entries.\n",
      "Setting GFS-5 is properly disjoint.\n",
      "\n",
      "Setting FS-10 has no repeated entries.\n",
      "Setting FS-10 is properly disjoint.\n",
      "\n",
      "Setting GFS-10 has no repeated entries.\n",
      "Setting GFS-10 is properly disjoint.\n"
     ]
    }
   ],
   "source": [
    "dataset_types = proposed_splits_meta_data['dataset_types']\n",
    "dataset_type_names = {'HT': 'hyperparameter_tuning', 'final': 'final_evaluation', 'DEBUG': 'DEBUG'}\n",
    "masking_types = ['original', 'unmasked', 'sub_obj_masking', 'NER_masking']\n",
    "\n",
    "HT = {}\n",
    "\n",
    "paths  = {}\n",
    "labels = {}\n",
    "for masking_type in masking_types:\n",
    "    paths[masking_type] = {}\n",
    "    labels[masking_type] = {}\n",
    "    for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "        paths[masking_type][dataset_type] = {}\n",
    "        labels[masking_type][dataset_type] = {}\n",
    "        for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "            paths[masking_type][dataset_type][setting] = {}\n",
    "            labels[masking_type][dataset_type][setting] = {}\n",
    "            for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "                paths[masking_type][dataset_type][setting][fold] = {}\n",
    "                labels[masking_type][dataset_type][setting][fold] = {}\n",
    "                for split in proposed_splits_meta_data['splits']:\n",
    "                    path = join_path([path_proposed_splits_masking, masking_type, dataset_type_names[dataset_type], setting, str(fold), split])\n",
    "                    paths[masking_type][dataset_type][setting][fold][split] = path\n",
    "                    labels[masking_type][dataset_type][setting][fold][split] = []\n",
    "\n",
    "\n",
    "                    \n",
    "# We start by verifying that all labels are correctly assigned (for the different masking types) and that the correct classes are in each split.\n",
    "# We start by the classes in each split (which we test only on the original. Since we compara each masking type's labels with the original, by\n",
    "# association we are also testing for the correct classes).\n",
    "for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "    for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "        for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "            for split in proposed_splits_meta_data['splits']:\n",
    "                with open(paths['original'][dataset_type][setting][fold][split] + '.txt', 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        _class = line.strip().split('\\t')[0]\n",
    "                        labels['original'][dataset_type][setting][fold][split].append(_class)\n",
    "\n",
    "                        # Check for class having been correctly assigned to fold and split\n",
    "                        if (setting == 'N' or setting == 'ZS'):\n",
    "                            assert proposed_splits_meta_data['class_to_idx'][_class] in classes_in_split[dataset_type][setting][fold][split], setting\n",
    "                        else:\n",
    "                            if (setting[0] == 'G' and split != 'train'):\n",
    "                                assert any(proposed_splits_meta_data['class_to_idx'][_class] in classes_in_split[dataset_type][setting][fold][_split]\n",
    "                                           for _split in ['train', split])\n",
    "                            elif ((setting[:2] == 'FS' or setting[:2] == 'GF') and split == 'train'):\n",
    "                                assert any(proposed_splits_meta_data['class_to_idx'][_class] in classes_in_split[dataset_type][setting][fold][_split]\n",
    "                                           for _split in proposed_splits_meta_data['splits'])\n",
    "                            else:\n",
    "                                assert proposed_splits_meta_data['class_to_idx'][_class] in classes_in_split[dataset_type][setting][fold][split], dataset_type + ' ' + setting + ' ' + str(fold) + ' ' + split\n",
    "\n",
    "\n",
    "for masking_type in masking_types:\n",
    "    if (masking_type != 'original'):\n",
    "        for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "            for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "                for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "                    for split in proposed_splits_meta_data['splits']:\n",
    "                        with open(paths[masking_type][dataset_type][setting][fold][split] + '.lbs', 'rb') as f:\n",
    "                            labels[masking_type][dataset_type][setting][fold][split] = pickle.load(f)\n",
    "                            assert labels[masking_type][dataset_type][setting][fold][split] == labels['original'][dataset_type][setting][fold][split]\n",
    "\n",
    "print('Class assignments are correct.')\n",
    "\n",
    "\n",
    "\n",
    "# We start by verifying that HT is completely disjoint\n",
    "for setting in paths['original']['HT']:\n",
    "    for fold in paths['original']['HT'][setting]:\n",
    "        for split in paths['original']['HT'][setting][fold]:\n",
    "            with open(paths['original']['HT'][setting][fold][split] + '.txt', 'r', encoding='utf-8') as f:\n",
    "                for l_num, line in enumerate(f):\n",
    "                    line_temp = line.strip().split('\\t')                      \n",
    "                    \n",
    "                    if (line_temp[1] not in HT):\n",
    "                        HT[line_temp[1]] = 1\n",
    "                    else:\n",
    "                        print('ERROR!')\n",
    "                        HT[line_temp[1]] += 1\n",
    "\n",
    "print(\"HT is\" + (\"\" if all(HT[instance] == 1 for instance in HT) else \" NOT\") + \" completely disjoint.\")\n",
    "print(\"Number of distinct instances in HT:\", sum(HT[instance] for instance in HT))\n",
    "print('\\n\\n')\n",
    "\n",
    "# Now we check the final evaluation dataset's disjointness assumptions.\n",
    "for setting in paths['original']['final']:\n",
    "    setting_instances = {}\n",
    "    for fold in paths['original']['final'][setting]:\n",
    "        setting_instances[fold] = {}\n",
    "        for split in paths['original']['final'][setting][fold]:\n",
    "            setting_instances[fold][split] = {}\n",
    "            with open(paths['original']['final'][setting][fold][split] + '.txt', 'r', encoding='utf-8') as f:\n",
    "                for l_num, line in enumerate(f):\n",
    "                    line_temp = line.strip().split('\\t')                        \n",
    "\n",
    "                    if (line_temp[1] not in setting_instances[fold][split]):\n",
    "                        setting_instances[fold][split][line_temp[1]] = 1\n",
    "                    else:\n",
    "                        print('ERROR!')\n",
    "                        setting_instances[fold][split][line_temp[1]] += 1\n",
    "\n",
    "    no_repeated_entries = all(setting_instances[fold][split][instance] == 1 for fold in paths['original']['final'][setting]\n",
    "                                                                                for split in paths['original']['final'][setting][fold]\n",
    "                                                                                    for instance in setting_instances[fold][split])\n",
    "    print(\"\\nSetting \" + setting + \" has\" + (\" no\" if no_repeated_entries else \"\") + \" repeated entries.\")\n",
    "\n",
    "    for fold in paths['original']['final'][setting]:\n",
    "        for split in paths['original']['final'][setting][fold]:\n",
    "            if (split != 'test'):\n",
    "                for instance in setting_instances[fold][split]:\n",
    "                    assert all(instance not in setting_instances[other_fold][other_split] for other_fold in paths['original']['final'][setting]\n",
    "                                                                                              for other_split in paths['original']['final'][setting][other_fold]\n",
    "                                                                                                  if fold != other_fold or split != other_split)\n",
    "                    if (split == 'val'):\n",
    "                        assert instance not in HT\n",
    "            else:\n",
    "                for instance in setting_instances[fold][split]:\n",
    "                    assert all(instance not in setting_instances[other_fold][other_split] for other_fold in paths['original']['final'][setting]\n",
    "                                                                                              for other_split in paths['original']['final'][setting][other_fold]\n",
    "                                                                                                  if split != other_split)\n",
    "                    assert instance not in HT\n",
    "\n",
    "    print(\"Setting \" + setting + \" is properly disjoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From experience we know that producing separate ELMo embeddings' .hfd5 files for each individual (dataset_type, setting, fold, split) takes too much memory (over 1TB, which is the space we have on the LISA cluster). As such, we compute 1 unique file (per masking type), where we then reference the other files to ELMo embeddings of this big unique file. Hopefully, the hdf5 compression techniques will allow us to save a lot of space. (Spoiler: They do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unmasked:\n",
      "HT: 632500 unique sentences (0 repetitions).\n",
      "\n",
      "\n",
      "\n",
      "final (N): 518036 unique sentences (train_sentences_in_HT:321735, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:980).\n",
      "final (ZS): 518750 unique sentences (train_sentences_in_HT:500000, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:0).\n",
      "final (GZS): 518665 unique sentences (train_sentences_in_HT:329292, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:125).\n",
      "final (FS-1): 518560 unique sentences (train_sentences_in_HT:499600, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:348).\n",
      "final (GFS-1): 518669 unique sentences (train_sentences_in_HT:329100, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:123).\n",
      "final (FS-2): 518708 unique sentences (train_sentences_in_HT:499200, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:77).\n",
      "final (GFS-2): 518661 unique sentences (train_sentences_in_HT:328906, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:130).\n",
      "final (FS-5): 518585 unique sentences (train_sentences_in_HT:498000, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:294).\n",
      "final (GFS-5): 518658 unique sentences (train_sentences_in_HT:328181, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:132).\n",
      "final (FS-10): 517989 unique sentences (train_sentences_in_HT:496000, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:1359).\n",
      "final (GFS-10): 518631 unique sentences (train_sentences_in_HT:326253, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:191).\n",
      "\n",
      "\n",
      "\n",
      "sub_obj_masking:\n",
      "HT: 632500 unique sentences (0 repetitions).\n",
      "\n",
      "\n",
      "\n",
      "final (N): 518036 unique sentences (train_sentences_in_HT:321735, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:980).\n",
      "final (ZS): 518750 unique sentences (train_sentences_in_HT:500000, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:0).\n",
      "final (GZS): 518665 unique sentences (train_sentences_in_HT:329292, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:125).\n",
      "final (FS-1): 518560 unique sentences (train_sentences_in_HT:499600, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:348).\n",
      "final (GFS-1): 518669 unique sentences (train_sentences_in_HT:329100, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:123).\n",
      "final (FS-2): 518708 unique sentences (train_sentences_in_HT:499200, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:77).\n",
      "final (GFS-2): 518661 unique sentences (train_sentences_in_HT:328906, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:130).\n",
      "final (FS-5): 518585 unique sentences (train_sentences_in_HT:498000, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:294).\n",
      "final (GFS-5): 518658 unique sentences (train_sentences_in_HT:328181, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:132).\n",
      "final (FS-10): 517989 unique sentences (train_sentences_in_HT:496000, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:1359).\n",
      "final (GFS-10): 518631 unique sentences (train_sentences_in_HT:326253, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:191).\n",
      "\n",
      "\n",
      "\n",
      "NER_masking:\n",
      "HT: 632500 unique sentences (0 repetitions).\n",
      "\n",
      "\n",
      "\n",
      "final (N): 518036 unique sentences (train_sentences_in_HT:321735, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:980).\n",
      "final (ZS): 518750 unique sentences (train_sentences_in_HT:500000, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:0).\n",
      "final (GZS): 518665 unique sentences (train_sentences_in_HT:329292, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:125).\n",
      "final (FS-1): 518560 unique sentences (train_sentences_in_HT:499600, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:348).\n",
      "final (GFS-1): 518669 unique sentences (train_sentences_in_HT:329100, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:123).\n",
      "final (FS-2): 518708 unique sentences (train_sentences_in_HT:499200, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:77).\n",
      "final (GFS-2): 518661 unique sentences (train_sentences_in_HT:328906, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:130).\n",
      "final (FS-5): 518585 unique sentences (train_sentences_in_HT:498000, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:294).\n",
      "final (GFS-5): 518658 unique sentences (train_sentences_in_HT:328181, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:132).\n",
      "final (FS-10): 517989 unique sentences (train_sentences_in_HT:496000, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:1359).\n",
      "final (GFS-10): 518631 unique sentences (train_sentences_in_HT:326253, val_sentences_in_HT:0, test_sentences_in_HT:0, test_in_other_tests:191).\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_type_names = {'DEBUG': 'DEBUG', 'HT': 'hyperparameter_tuning', 'final': 'final_evaluation'}\n",
    "masking_types = ['unmasked', 'sub_obj_masking', 'NER_masking']\n",
    "\n",
    "paths = {}\n",
    "for masking_type in masking_types:\n",
    "    paths[masking_type] = {}\n",
    "    for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "        if (dataset_type == 'DEBUG'):\n",
    "            continue\n",
    "        paths[masking_type][dataset_type] = {}\n",
    "        for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "            paths[masking_type][dataset_type][setting] = {}\n",
    "            for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "                paths[masking_type][dataset_type][setting][fold] = {}\n",
    "                for split in proposed_splits_meta_data['splits']:\n",
    "                    path = join_path([path_proposed_splits_masking, masking_type, dataset_type_names[dataset_type], setting, str(fold), split + '.txt'])\n",
    "                    paths[masking_type][dataset_type][setting][fold][split] = path\n",
    "\n",
    "HT_sentences = {masking_type: {} for masking_type in masking_types}\n",
    "final_setting_sentences = {masking_type: {setting: {} for setting in proposed_splits_meta_data['settings'][dataset_type]} for masking_type in masking_types}\n",
    "\n",
    "for masking_type in masking_types:\n",
    "    print(masking_type + ':')\n",
    "    for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "        if (dataset_type == 'DEBUG'):\n",
    "            continue\n",
    "        for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "            if (dataset_type == 'final'):\n",
    "                train_sentences_in_HT = 0\n",
    "                val_sentences_in_HT   = 0\n",
    "                test_sentences_in_HT  = 0\n",
    "                test_in_other_tests = 0\n",
    "\n",
    "            for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "                for split in proposed_splits_meta_data['splits']:\n",
    "                    with open(paths[masking_type][dataset_type][setting][fold][split], 'r', encoding='utf-8') as f:\n",
    "                        for line in f:\n",
    "                            sentence = line.strip()\n",
    "                            if (dataset_type == 'HT'):\n",
    "                                if (sentence not in HT_sentences[masking_type]):\n",
    "                                    HT_sentences[masking_type][sentence] = [(setting, fold, split)]\n",
    "                                else:\n",
    "                                    HT_sentences[masking_type][sentence].append((setting, fold, split))\n",
    "                            else:\n",
    "                                if (sentence in HT_sentences[masking_type]):\n",
    "                                    if (split == 'train'):\n",
    "                                        train_sentences_in_HT += 1\n",
    "                                    elif (split == 'val'):\n",
    "                                        val_sentences_in_HT += 1\n",
    "                                    else:\n",
    "                                        test_sentences_in_HT += 1\n",
    "                                if (sentence not in final_setting_sentences[masking_type][setting]):\n",
    "                                    final_setting_sentences[masking_type][setting][sentence] = [(fold, split)]\n",
    "                                else:\n",
    "                                    final_setting_sentences[masking_type][setting][sentence].append((fold, split))\n",
    "\n",
    "\n",
    "            for sentence in final_setting_sentences[masking_type][setting]:\n",
    "                if (len(final_setting_sentences[masking_type][setting][sentence]) > 1):\n",
    "                    if (any(_split == 'train' for _fold, _split in final_setting_sentences[masking_type][setting][sentence]) and\n",
    "                        any(_split == 'val' for _fold, _split in final_setting_sentences[masking_type][setting][sentence])):\n",
    "                        raise RuntimeError(masking_type + ', ' + dataset_type + ', ' + setting + ': sentence in both train and val! (' + sentence + ').')\n",
    "\n",
    "                    if ((any(_split == 'train' for _fold, _split in final_setting_sentences[masking_type][setting][sentence]) or\n",
    "                         any(_split == 'val' for _fold, _split in final_setting_sentences[masking_type][setting][sentence])) and\n",
    "                        any(_split == 'test' for _fold, _split in final_setting_sentences[masking_type][setting][sentence])):\n",
    "                        raise RuntimeError(masking_type + ', ' + dataset_type + ', ' + setting + ': sentence in both (train or val) and test! (' + sentence + ').')\n",
    "\n",
    "                    if (len(set(final_setting_sentences[masking_type][setting][sentence])) != len(final_setting_sentences[masking_type][setting][sentence])):\n",
    "                        print(sentence, final_setting_sentences[masking_type][setting][sentence])\n",
    "                        raise RuntimeError('Repeated same sentence in same (fold, split).')\n",
    "\n",
    "                    if (any(_split == 'train' for _fold, _split in final_setting_sentences[masking_type][setting][sentence])):\n",
    "                        raise RuntimeError(masking_type + ', ' + dataset_type + ', ' + setting + ': sentence in multiple train splits! (' + sentence + ').')\n",
    "\n",
    "                    if (any(_split == 'val' for _fold, _split in final_setting_sentences[masking_type][setting][sentence])):\n",
    "                        raise RuntimeError(masking_type + ', ' + dataset_type + ', ' + setting + ': sentence in multiple val splits! (' + sentence + ').')\n",
    "                            \n",
    "\n",
    "                    test_in_other_tests += len(final_setting_sentences[masking_type][setting][sentence])\n",
    "                        \n",
    "\n",
    "            \n",
    "            if (dataset_type == 'final'):\n",
    "                print('final (' + setting + '): ' + str(len(final_setting_sentences[masking_type][setting])) +\n",
    "                      ' unique sentences (train_sentences_in_HT:' + str(train_sentences_in_HT) + ', val_sentences_in_HT:' + str(val_sentences_in_HT) +\n",
    "                       ', test_sentences_in_HT:' + str(test_sentences_in_HT) + ', test_in_other_tests:' + str(test_in_other_tests) + ').')\n",
    "\n",
    "        if (dataset_type == 'HT'):\n",
    "            print('HT: ' + str(len(HT_sentences[masking_type])) + ' unique sentences (' + str(sum(len(sentences[masking_type][sentence]) - 1\n",
    "                                                                                                    for sentence in HT_sentences[masking_type]\n",
    "                                                                                                        if len(HT_sentences[masking_type][sentence]) > 1)) +\n",
    "                  ' repetitions).')\n",
    "\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unmasked\n",
      " 33.333%\n",
      "Number unique sentences in experiment type (excluding 'DEBUG'): 992189\n",
      " 33.333%\n",
      "Writing sentences_to_ELMo.txt\n",
      "\n",
      "Writing DBUG sentences_to_ELMo.txt\n",
      "\n",
      "\n",
      "\n",
      "sub_obj_masking\n",
      " 66.665%\n",
      "Number unique sentences in experiment type (excluding 'DEBUG'): 992189\n",
      " 66.665%\n",
      "Writing sentences_to_ELMo.txt\n",
      "\n",
      "Writing DBUG sentences_to_ELMo.txt\n",
      "\n",
      "\n",
      "\n",
      "NER_masking\n",
      "100.000%\n",
      "Number unique sentences in experiment type (excluding 'DEBUG'): 992189\n",
      "100.000%\n",
      "Writing sentences_to_ELMo.txt\n",
      "\n",
      "Writing DBUG sentences_to_ELMo.txt\n",
      "\n",
      "\n",
      "\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "dataset_type_names = {'DEBUG': 'DEBUG', 'HT': 'hyperparameter_tuning', 'final': 'final_evaluation'}\n",
    "masking_types = ['unmasked', 'sub_obj_masking', 'NER_masking']\n",
    "\n",
    "paths = {}\n",
    "for masking_type in masking_types:\n",
    "    paths[masking_type] = {}\n",
    "    for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "        paths[masking_type][dataset_type] = {}\n",
    "        for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "            paths[masking_type][dataset_type][setting] = {}\n",
    "            for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "                paths[masking_type][dataset_type][setting][fold] = {}\n",
    "                for split in proposed_splits_meta_data['splits']:\n",
    "                    path = join_path([path_proposed_splits_masking, masking_type, dataset_type_names[dataset_type], setting, str(fold), split + '.txt'])\n",
    "                    paths[masking_type][dataset_type][setting][fold][split] = path\n",
    "\n",
    "\n",
    "num_instances = 0\n",
    "for masking_type in masking_types:\n",
    "    for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "        for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "            for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "                for split in proposed_splits_meta_data['splits']:\n",
    "                    num_instances += file_len(paths[masking_type][dataset_type][setting][fold][split])\n",
    "\n",
    "\n",
    "\n",
    "l_num = 0\n",
    "l_num_writing = 0\n",
    "for masking_type in masking_types:\n",
    "    print(masking_type)\n",
    "\n",
    "    # We start by reading in all the sentences.\n",
    "    unique_hdf5_sentence_to_idx = {}\n",
    "    unique_hdf5_sentence_to_idx_debug = {}\n",
    "    for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "        for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "            for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "                for split in proposed_splits_meta_data['splits']:\n",
    "                    with open(paths[masking_type][dataset_type][setting][fold][split], 'r', encoding='utf-8') as f:\n",
    "                        for line in f:\n",
    "                            if (l_num % 500 == 0 or l_num + 1 == num_instances):\n",
    "                                print('\\r' + '{:7.3f}%'.format(100*(l_num+1)/num_instances), end=\"\", flush=True)\n",
    "                            l_num += 1\n",
    "\n",
    "                            sentence = line.strip()\n",
    "                            # Note that we make use of the fact that python 3.6 uses ordered dictionaries.\n",
    "                            if (dataset_type == 'DEBUG'):\n",
    "                                if (sentence not in unique_hdf5_sentence_to_idx_debug):\n",
    "                                    unique_hdf5_sentence_to_idx_debug[sentence] = len(unique_hdf5_sentence_to_idx_debug)\n",
    "                            else:\n",
    "                                if (sentence not in unique_hdf5_sentence_to_idx):\n",
    "                                    unique_hdf5_sentence_to_idx[sentence] = len(unique_hdf5_sentence_to_idx)\n",
    "\n",
    "    print()\n",
    "    print(\"Number unique sentences in experiment type (excluding 'DEBUG'):\", len(unique_hdf5_sentence_to_idx))\n",
    "\n",
    "\n",
    "    # Now we write all the indices to file.\n",
    "    for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "        for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "            for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "                for split in proposed_splits_meta_data['splits']:\n",
    "                    indices = []\n",
    "                    with open(paths[masking_type][dataset_type][setting][fold][split], 'r', encoding='utf-8') as f:\n",
    "                        for line in f:\n",
    "                            if (l_num_writing % 500 == 0 or l_num_writing + 1 == num_instances):\n",
    "                                print('\\x1b[2K\\r' + '{:7.3f}%'.format(100*(l_num_writing+1)/num_instances), end=\"\", flush=True)\n",
    "                            l_num_writing += 1\n",
    "\n",
    "                            sentence = line.strip()\n",
    "                            indices.append(unique_hdf5_sentence_to_idx_debug[sentence] if dataset_type == 'DEBUG' else unique_hdf5_sentence_to_idx[sentence])\n",
    "                    with open(paths[masking_type][dataset_type][setting][fold][split][:-4] + '.idxs', 'wb') as f:\n",
    "                        pickle.dump(indices, f)\n",
    "\n",
    "    with open(join_path([path_proposed_splits_masking, masking_type, 'sentences_to_ELMo.txt']), 'w', encoding='utf-8') as f:\n",
    "        print()\n",
    "        print('Writing sentences_to_ELMo.txt')\n",
    "        for s_num, sentence in enumerate(unique_hdf5_sentence_to_idx):\n",
    "            assert s_num == unique_hdf5_sentence_to_idx[sentence] # Sanity check. Seems quite redundant....\n",
    "            f.write(sentence + '\\n')\n",
    "\n",
    "    with open(join_path([path_proposed_splits_masking, masking_type, 'DEBUG_sentences_to_ELMo.txt']), 'w', encoding='utf-8') as f:\n",
    "        print('Writing DBUG sentences_to_ELMo.txt')\n",
    "        for s_num, sentence in enumerate(unique_hdf5_sentence_to_idx_debug):\n",
    "            assert s_num == unique_hdf5_sentence_to_idx_debug[sentence] # Sanity check. Seems quite redundant....\n",
    "            f.write(sentence + '\\n')\n",
    "\n",
    "    print('\\n\\n')\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we get the unsupervised labels (i.e., just the word identifiers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.000%"
     ]
    }
   ],
   "source": [
    "dataset_type_names = {'DEBUG': 'DEBUG', 'HT': 'hyperparameter_tuning', 'final': 'final_evaluation'}\n",
    "masking_types = ['unmasked', 'sub_obj_masking']#, 'NER_masking']\n",
    "\n",
    "paths = {}\n",
    "for masking_type in masking_types:\n",
    "    paths[masking_type] = {}\n",
    "    for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "        paths[masking_type][dataset_type] = {}\n",
    "        for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "            paths[masking_type][dataset_type][setting] = {}\n",
    "            for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "                path = path_proposed_splits + masking_type + '/' + dataset_type_names[dataset_type] + '/' + setting + '/' + str(fold) + '/train.txt'\n",
    "                paths[masking_type][dataset_type][setting][fold] = path\n",
    "\n",
    "\n",
    "num_instances = 0\n",
    "for masking_type in masking_types:\n",
    "    for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "        for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "            for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "                num_instances += file_len(paths[masking_type][dataset_type][setting][fold])\n",
    "\n",
    "\n",
    "\n",
    "l_num = 0\n",
    "for masking_type in masking_types:\n",
    "    for dataset_type in proposed_splits_meta_data['dataset_types']:\n",
    "        for setting in proposed_splits_meta_data['settings'][dataset_type]:\n",
    "            for fold in range(proposed_splits_meta_data['num_folds'][dataset_type]):\n",
    "                idx_to_token_map = []\n",
    "                token_to_idx_map = {}\n",
    "                data = []\n",
    "\n",
    "                with open(paths[masking_type][dataset_type][setting][fold], 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        if (l_num % 250 == 0 or l_num + 1 == num_instances):\n",
    "                            print('\\r' + '{:7.3f}%'.format(100*(l_num+1)/num_instances), end=\"\", flush=True)\n",
    "                        l_num += 1\n",
    "\n",
    "                        data.append(line.strip().split(' '))\n",
    "                        for token in data[-1]:\n",
    "                            if (token not in token_to_idx_map):                 # Read from bottom up - Maybe fixed that issue, since I had to implement my own loss.\n",
    "                                idx_to_token_map.append(token)                  # The comment below is dumb as then we need to shift predictions in the model\n",
    "                                token_to_idx_map[token] = len(idx_to_token_map) # By not subtracting 1 we can reserve the 0 index for the <PAD> token\n",
    "\n",
    "                    # Transform the tokenized sentences to indexed sentences\n",
    "                    data_as_idxs = [[token_to_idx_map[token] for token in data_instance] for data_instance in data]\n",
    "\n",
    "                    # Save indexed sentences, and corresponding maps, to memory. 'ulbs' stands for 'unsupervised labels'\n",
    "                    with open(paths[masking_type][dataset_type][setting][fold][:-3] + 'ulbs', \"wb+\") as f:\n",
    "                        pickle.dump(data_as_idxs, f)\n",
    "                        pickle.dump(idx_to_token_map, f)\n",
    "                        pickle.dump(token_to_idx_map, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLACHO'S RELATION DESCRIPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_names = []\n",
    "#path_relation_descriptions = join_path([path_ours, \"relations_descriptions\"])# + each relation's name\n",
    "with open(path_relation_names, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        relation_names.append(line.strip())\n",
    "\n",
    "relation_descriptions_count = {relation: file_len(join_path([path_relation_descriptions, re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt']))\n",
    "                               for relation in relation_names}\n",
    "\n",
    "relation_to_lowercased = {relation: relation.lower() for relation in relation_names}\n",
    "lowercased_to_relation = {relation.lower(): relation for relation in relation_names}\n",
    "\n",
    "relation_lowercased_descriptions_count = {relation_to_lowercased[relation]: relation_descriptions_count[relation] for relation in relation_names}\n",
    "ordered_relation_descs_count = {lowercased_to_relation[relation]: relation_lowercased_descriptions_count[relation]\n",
    "                                for relation in dict(sorted(relation_lowercased_descriptions_count.items()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pltt\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for relation in ordered_relation_descs_count:\n",
    "    x.append(relation)\n",
    "    y.append(ordered_relation_descs_count[relation])\n",
    "\n",
    "y.sort(reverse=True)\n",
    "fig = pltt.figure(figsize=(12,7))\n",
    "ax = fig.add_subplot(111)\n",
    "pltt.bar(range(1, len(x)+1), height=y)\n",
    "avg = np.mean(np.array(list(ordered_relation_descs_count.values())))\n",
    "shader_mean = np.array([avg]*120)\n",
    "stdev = np.std(np.array(list(ordered_relation_descs_count.values())))\n",
    "shader_stdev = np.array([stdev]*120)\n",
    "ax.annotate(r\"{:.2f} $\\pm$ {:.2f}\".format(avg, stdev), xy=(95, 10), fontsize=20.0)\n",
    "pltt.hlines(avg, 0.5, 120.25, colors='r', linestyles='solid', label='avg', linewidth=4.0)\n",
    "pltt.fill_between(np.linspace(0.5, 120.25, 120), shader_mean-shader_stdev, shader_mean+shader_stdev, color=\"r\", alpha=0.2)\n",
    "#set parameters for tick labels\n",
    "pltt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "pltt.tick_params(axis='y', which='major', labelsize=23)\n",
    "pltt.ylabel('Number of Descriptions', fontsize=30)\n",
    "\n",
    "pltt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1098\n",
      "OBJECT_ENTITY is where the home games of SUBJECT_ENTITY are played\n",
      "10\n",
      "[OBJECT_ENTITY, is, where, the, home, games, of, SUBJECT_ENTITY, are, played]\n",
      "7.373406193078324 1.8785781791722675\n"
     ]
    }
   ],
   "source": [
    "descriptions = set()\n",
    "\n",
    "for relation in relation_names:\n",
    "    with open(join_path([path_relation_descriptions, re.sub('/', '-', re.sub(' ', '_', relation)) + '.txt']), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            descriptions.add(line.strip())\n",
    "\n",
    "descriptions = list(descriptions)\n",
    "print(len(descriptions))\n",
    "\n",
    "print(descriptions[0])\n",
    "print(len(tokenizer.tokenize(descriptions[0])))\n",
    "print(tokenizer.tokenize(descriptions[0]))\n",
    "descriptions_lengths = np.array([len(tokenizer.tokenize(description)) for description in descriptions])\n",
    "\n",
    "print(np.mean(descriptions_lengths), np.std(descriptions_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airline hub': 9, 'architect': 29, 'architectural style': 3, 'author': 27, 'award received': 5, 'based on': 2, 'brother': 2, 'canonization status': 1, 'cast member': 10, 'cause of death': 23, 'chairperson': 1, 'characters': 1, 'child': 2, 'chromosome': 10, 'collection': 1, 'conferred by': 1, 'conflict': 20, 'connecting line': 11, 'constellation': 21, 'continent': 18, 'convicted of': 3, 'country': 22, 'country of citizenship': 16, 'country of origin': 6, 'creator': 7, 'crosses': 4, 'date of birth': 9, 'date of death': 20, 'date of official opening': 14, 'designer': 9, 'developer': 9, 'director': 23, 'discoverer or inventor': 15, 'dissolved or abolished': 9, 'distributor': 19, 'drafted by': 6, 'editor': 4, 'educated at': 8, 'employer': 1, 'end time': 1, 'father': 14, 'film editor': 17, 'found in taxon': 3, 'founder': 9, 'from fictional universe': 11, 'head of government': 4, 'headquarters location': 6, 'home venue': 29, 'illustrator': 4, 'inception': 26, 'industry': 1, 'instrument': 24, 'instrumentation': 15, 'IUCN conservation status': 8, 'language of work or name': 8, 'languages spoken or written': 8, 'league': 8, 'license': 1, 'licensed to broadcast to': 3, 'located in the administrative territorial entity': 3, 'located next to body of water': 3, 'located on astronomical body': 9, 'location of formation': 6, 'lyrics by': 12, 'manner of death': 4, 'manufacturer': 15, 'material used': 2, 'medical condition': 18, 'member of political party': 7, 'member of sports team': 15, 'military branch': 26, 'military rank': 2, 'mother': 9, 'mouth of the watercourse': 9, 'named after': 4, 'narrative location': 2, 'native language': 10, 'noble family': 3, 'noble title': 2, 'nominated for': 2, 'occupant': 1, 'occupation': 3, 'operating system': 1, 'original network': 34, 'parent company': 3, 'parent taxon': 5, 'participant of': 9, 'performer': 29, 'place of birth': 14, 'place of burial': 11, 'place of death': 5, 'point in time': 12, 'position held': 3, 'position played on team / speciality': 28, 'present in work': 10, 'product': 6, 'production company': 5, 'programming language': 3, 'publication date': 28, 'publisher': 7, 'record label': 4, 'religious order': 2, 'replaced by': 3, 'residence': 2, 'screenwriter': 14, 'series': 14, 'service entry': 10, 'sex or gender': 20, 'sister': 2, 'site of astronomical discovery': 9, 'sport': 17, 'spouse': 1, 'standards body': 1, 'start time': 1, 'stock exchange': 9, 'taxon rank': 2, 'time of discovery': 19, 'time of spacecraft launch': 12, 'vessel class': 4, 'voice type': 8}\n"
     ]
    }
   ],
   "source": [
    "print(ordered_relation_descs_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airline hub', 'architect', 'architectural style', 'author', 'award received', 'based on', 'brother', 'canonization status', 'cast member', 'cause of death', 'chairperson', 'characters', 'child', 'chromosome', 'collection', 'conferred by', 'conflict', 'connecting line', 'constellation', 'continent', 'convicted of', 'country', 'country of citizenship', 'country of origin', 'creator', 'crosses', 'date of birth', 'date of death', 'date of official opening', 'designer', 'developer', 'director', 'discoverer or inventor', 'dissolved or abolished', 'distributor', 'drafted by', 'editor', 'educated at', 'employer', 'end time', 'father', 'film editor', 'found in taxon', 'founder', 'from fictional universe', 'head of government', 'headquarters location', 'home venue', 'illustrator', 'inception', 'industry', 'instrument', 'instrumentation', 'IUCN conservation status', 'language of work or name', 'languages spoken or written', 'league', 'license', 'licensed to broadcast to', 'located in the administrative territorial entity', 'located next to body of water', 'located on astronomical body', 'location of formation', 'lyrics by', 'manner of death', 'manufacturer', 'material used', 'medical condition', 'member of political party', 'member of sports team', 'military branch', 'military rank', 'mother', 'mouth of the watercourse', 'named after', 'narrative location', 'native language', 'noble family', 'noble title', 'nominated for', 'occupant', 'occupation', 'operating system', 'original network', 'parent company', 'parent taxon', 'participant of', 'performer', 'place of birth', 'place of burial', 'place of death', 'point in time', 'position held', 'position played on team / speciality', 'present in work', 'product', 'production company', 'programming language', 'publication date', 'publisher', 'record label', 'religious order', 'replaced by', 'residence', 'screenwriter', 'series', 'service entry', 'sex or gender', 'sister', 'site of astronomical discovery', 'sport', 'spouse', 'standards body', 'start time', 'stock exchange', 'taxon rank', 'time of discovery', 'time of spacecraft launch', 'vessel class', 'voice type']\n"
     ]
    }
   ],
   "source": [
    "from   tkinter import Tk\n",
    "relations_list = list(ordered_relation_descs_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{c|c}\n",
      "Relation & Number of Descriptions     \\\\ \\hline\n",
      "airline hub & 9 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "architect & 29 \\\\architectural style & 3 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "author & 27 \\\\award received & 5 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "based on & 2 \\\\brother & 2 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "canonization status & 1 \\\\cast member & 10 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "cause of death & 23 \\\\chairperson & 1 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "characters & 1 \\\\child & 2 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "chromosome & 10 \\\\collection & 1 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "conferred by & 1 \\\\conflict & 20 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "connecting line & 11 \\\\constellation & 21 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "continent & 18 \\\\convicted of & 3 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "country & 22 \\\\country of citizenship & 16 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "country of origin & 6 \\\\creator & 7 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "crosses & 4 \\\\date of birth & 9 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "date of death & 20 \\\\date of official opening & 14 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "designer & 9 \\\\developer & 9 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "director & 23 \\\\discoverer or inventor & 15 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "dissolved or abolished & 9 \\\\distributor & 19 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "drafted by & 6 \\\\editor & 4 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "educated at & 8 \\\\employer & 1 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "end time & 1 \\\\father & 14 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "film editor & 17 \\\\found in taxon & 3 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "founder & 9 \\\\from fictional universe & 11 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "head of government & 4 \\\\headquarters location & 6 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "home venue & 29 \\\\illustrator & 4 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "inception & 26 \\\\industry & 1 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "instrument & 24 \\\\instrumentation & 15 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "IUCN conservation status & 8 \\\\language of work or name & 8 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "languages spoken or written & 8 \\\\league & 8 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "license & 1 \\\\licensed to broadcast to & 3 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "located in the administrative territorial entity & 3 \\\\\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First half of relations\n",
    "\n",
    "latex_header = r\"\\begin{tabular}{c|c}\" + \"\\n\"\n",
    "latex_header += r\"Relation & Number of Descriptions     \\\\ \\hline\" + \"\\n\"\n",
    "\n",
    "\n",
    "latex_body = r\"\"\n",
    "\n",
    "for l_num, relation in enumerate(relations_list[:60]):\n",
    "    if ((l_num + 1) % 2 == 0):\n",
    "        latex_body += r\"\\rowcolor[HTML]{EFEFEF} \" + \"\\n\"\n",
    "    latex_body += r\"{} & {} \\\\\".format(relation, ordered_relation_descs_count[relation])\n",
    "\n",
    "latex_footer = r\"\\end{tabular}\" + \"\\n\"\n",
    "\n",
    "latex_table = latex_header + latex_body + latex_footer\n",
    "\n",
    "r = Tk()\n",
    "r.withdraw()\n",
    "r.clipboard_clear()\n",
    "r.clipboard_append(latex_table)\n",
    "r.update() # now it stays on the clipboard after the window is closed\n",
    "r.destroy()\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{c|c}\n",
      "Relation & Number of Descriptions     \\\\ \\hline\n",
      "located next to body of water & 3 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "located on astronomical body & 9 \\\\location of formation & 6 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "lyrics by & 12 \\\\manner of death & 4 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "manufacturer & 15 \\\\material used & 2 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "medical condition & 18 \\\\member of political party & 7 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "member of sports team & 15 \\\\military branch & 26 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "military rank & 2 \\\\mother & 9 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "mouth of the watercourse & 9 \\\\named after & 4 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "narrative location & 2 \\\\native language & 10 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "noble family & 3 \\\\noble title & 2 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "nominated for & 2 \\\\occupant & 1 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "occupation & 3 \\\\operating system & 1 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "original network & 34 \\\\parent company & 3 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "parent taxon & 5 \\\\participant of & 9 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "performer & 29 \\\\place of birth & 14 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "place of burial & 11 \\\\place of death & 5 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "point in time & 12 \\\\position held & 3 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "position played on team / speciality & 28 \\\\present in work & 10 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "product & 6 \\\\production company & 5 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "programming language & 3 \\\\publication date & 28 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "publisher & 7 \\\\record label & 4 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "religious order & 2 \\\\replaced by & 3 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "residence & 2 \\\\screenwriter & 14 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "series & 14 \\\\service entry & 10 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "sex or gender & 20 \\\\sister & 2 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "site of astronomical discovery & 9 \\\\sport & 17 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "spouse & 1 \\\\standards body & 1 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "start time & 1 \\\\stock exchange & 9 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "taxon rank & 2 \\\\time of discovery & 19 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "time of spacecraft launch & 12 \\\\vessel class & 4 \\\\\\rowcolor[HTML]{EFEFEF} \n",
      "voice type & 8 \\\\\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Second half of relations\n",
    "\n",
    "latex_header = r\"\\begin{tabular}{c|c}\" + \"\\n\"\n",
    "latex_header += r\"Relation & Number of Descriptions     \\\\ \\hline\" + \"\\n\"\n",
    "\n",
    "\n",
    "latex_body = r\"\"\n",
    "\n",
    "for l_num, relation in enumerate(relations_list[60:]):\n",
    "    if ((l_num + 1) % 2 == 0):\n",
    "        latex_body += r\"\\rowcolor[HTML]{EFEFEF} \" + \"\\n\"\n",
    "    latex_body += r\"{} & {} \\\\\".format(relation, ordered_relation_descs_count[relation])\n",
    "\n",
    "latex_footer = r\"\\end{tabular}\" + \"\\n\"\n",
    "\n",
    "latex_table = latex_header + latex_body + latex_footer\n",
    "\n",
    "r = Tk()\n",
    "r.withdraw()\n",
    "r.clipboard_clear()\n",
    "r.clipboard_append(latex_table)\n",
    "r.update() # now it stays on the clipboard after the window is closed\n",
    "r.destroy()\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
